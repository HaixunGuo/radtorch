{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RADTorch The Radiology Machine Learning Tool Kit About RADTorch provides a package of higher level functions and classes that significantly decrease the amount of time needed for implementation of different machine and deep learning algorithms on DICOM medical images. RADTorch was developed and is currently maintained by Mohamed Elbanan, MD: a Radiology Resident at Yale New Haven Health System, Clinical Research Affiliate at Yale School of Medicine and a Machine-learning enthusiast. Getting Started Running a state-of-the-art DICOM image classifier can be run using the Image Classification Pipeline using the commands: from radtorch import pipeline classifier = pipeline.Image_Classification(data_directory='path to data') classifier.train() The above 3 lines of code will run an image classifier using VGG16 with pre-trained weights. Playground RADTorch playground for testing is provided on Google Colab . Contributing RadTorch is on GitHub . Bug reports and pull requests are welcome.","title":"Home"},{"location":"#radtorch-the-radiology-machine-learning-tool-kit","text":"","title":"RADTorch   The Radiology Machine Learning Tool Kit "},{"location":"#about","text":"RADTorch provides a package of higher level functions and classes that significantly decrease the amount of time needed for implementation of different machine and deep learning algorithms on DICOM medical images. RADTorch was developed and is currently maintained by Mohamed Elbanan, MD: a Radiology Resident at Yale New Haven Health System, Clinical Research Affiliate at Yale School of Medicine and a Machine-learning enthusiast.","title":"About"},{"location":"#getting-started","text":"Running a state-of-the-art DICOM image classifier can be run using the Image Classification Pipeline using the commands: from radtorch import pipeline classifier = pipeline.Image_Classification(data_directory='path to data') classifier.train() The above 3 lines of code will run an image classifier using VGG16 with pre-trained weights.","title":"Getting Started"},{"location":"#playground","text":"RADTorch playground for testing is provided on Google Colab .","title":"Playground"},{"location":"#contributing","text":"RadTorch is on GitHub . Bug reports and pull requests are welcome.","title":"Contributing"},{"location":"copyright/","text":"Copyrights Copyrights are reserved to authors of all used open source packages and snippets of code. PyTorch https://github.com/pytorch/pytorch/blob/master/LICENSE From PyTorch Copyright \u00a9 2016- Facebook, Inc (Adam Paszke) Copyright \u00a9 2014- Facebook, Inc (Soumith Chintala) Copyright \u00a9 2011-2014 Idiap Research Institute (Ronan Collobert) Copyright \u00a9 2012-2014 Deepmind Technologies (Koray Kavukcuoglu) Copyright \u00a9 2011-2012 NEC Laboratories America (Koray Kavukcuoglu) Copyright \u00a9 2011-2013 NYU (Clement Farabet) Copyright \u00a9 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston) Copyright \u00a9 2006 Idiap Research Institute (Samy Bengio) Copyright \u00a9 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz) From Caffe2: Copyright \u00a9 2016-present, Facebook Inc. All rights reserved. All contributions by Facebook: Copyright \u00a9 2016 Facebook Inc. All contributions by Google: Copyright \u00a9 2015 Google Inc. All rights reserved. All contributions by Yangqing Jia: Copyright \u00a9 2015 Yangqing Jia All rights reserved. All contributions from Caffe: Copyright\u00a9 2013, 2014, 2015, the respective contributors All rights reserved. All other contributions: Copyright\u00a9 2015, 2016 the respective contributors All rights reserved. Caffe2 uses a copyright model similar to Caffe: each contributor holds copyright over their contributions to Caffe2. The project versioning records all such contribution and copyright details. If a contributor wants to further mark their specific copyright on a particular contribution, they should indicate their copyright solely in the commit message of the change when it is committed. Sklearn https://scikit-learn.org/stable/about.html#citing-scikit-learn Buitinck, L., Louppe, G., Blondel, M., Pedregosa, F., Mueller, A., Grisel, O., Niculae, V., Prettenhofer, P., Gramfort, A., Grobler, J. and Layton, R., 2013. API design for machine learning software: experiences from the scikit-learn project. arXiv preprint arXiv:1309.0238. Pydicom https://github.com/pydicom/pydicom/blob/master/LICENSE License file for pydicom, a pure-python DICOM library Copyright \u00a9 2008-2020 Darcy Mason and pydicom contributors Except for portions outlined below, pydicom is released under an MIT license: Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. Portions of pydicom (private dictionary file(s)) were generated from the private dictionary of the GDCM library, released under the following license: Program: GDCM (Grassroots DICOM). A DICOM library Module: http://gdcm.sourceforge.net/Copyright.html Copyright \u00a9 2006-2010 Mathieu Malaterre Copyright \u00a9 1993-2005 CREATIS (CREATIS = Centre de Recherche et d'Applications en Traitement de l'Image) All rights reserved. Matplotlib https://matplotlib.org/users/license.html Matplotlib only uses BSD compatible code, and its license is based on the PSF license. See the Open Source Initiative licenses page for details on individual licenses. Non-BSD compatible licenses (e.g., LGPL) are acceptable in matplotlib toolkits. For a discussion of the motivations behind the licensing choice, see Licenses. Confusion Matrix Matplotlib Code snippet adapted with modification from : https://www.kaggle.com/grfiv4/plot-a-confusion-matrix","title":"Copyrights"},{"location":"copyright/#copyrights","text":"Copyrights are reserved to authors of all used open source packages and snippets of code.","title":"Copyrights"},{"location":"copyright/#pytorch","text":"https://github.com/pytorch/pytorch/blob/master/LICENSE From PyTorch Copyright \u00a9 2016- Facebook, Inc (Adam Paszke) Copyright \u00a9 2014- Facebook, Inc (Soumith Chintala) Copyright \u00a9 2011-2014 Idiap Research Institute (Ronan Collobert) Copyright \u00a9 2012-2014 Deepmind Technologies (Koray Kavukcuoglu) Copyright \u00a9 2011-2012 NEC Laboratories America (Koray Kavukcuoglu) Copyright \u00a9 2011-2013 NYU (Clement Farabet) Copyright \u00a9 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston) Copyright \u00a9 2006 Idiap Research Institute (Samy Bengio) Copyright \u00a9 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz) From Caffe2: Copyright \u00a9 2016-present, Facebook Inc. All rights reserved. All contributions by Facebook: Copyright \u00a9 2016 Facebook Inc. All contributions by Google: Copyright \u00a9 2015 Google Inc. All rights reserved. All contributions by Yangqing Jia: Copyright \u00a9 2015 Yangqing Jia All rights reserved. All contributions from Caffe: Copyright\u00a9 2013, 2014, 2015, the respective contributors All rights reserved. All other contributions: Copyright\u00a9 2015, 2016 the respective contributors All rights reserved. Caffe2 uses a copyright model similar to Caffe: each contributor holds copyright over their contributions to Caffe2. The project versioning records all such contribution and copyright details. If a contributor wants to further mark their specific copyright on a particular contribution, they should indicate their copyright solely in the commit message of the change when it is committed.","title":"PyTorch"},{"location":"copyright/#sklearn","text":"https://scikit-learn.org/stable/about.html#citing-scikit-learn Buitinck, L., Louppe, G., Blondel, M., Pedregosa, F., Mueller, A., Grisel, O., Niculae, V., Prettenhofer, P., Gramfort, A., Grobler, J. and Layton, R., 2013. API design for machine learning software: experiences from the scikit-learn project. arXiv preprint arXiv:1309.0238.","title":"Sklearn"},{"location":"copyright/#pydicom","text":"https://github.com/pydicom/pydicom/blob/master/LICENSE License file for pydicom, a pure-python DICOM library Copyright \u00a9 2008-2020 Darcy Mason and pydicom contributors Except for portions outlined below, pydicom is released under an MIT license: Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. Portions of pydicom (private dictionary file(s)) were generated from the private dictionary of the GDCM library, released under the following license: Program: GDCM (Grassroots DICOM). A DICOM library Module: http://gdcm.sourceforge.net/Copyright.html Copyright \u00a9 2006-2010 Mathieu Malaterre Copyright \u00a9 1993-2005 CREATIS (CREATIS = Centre de Recherche et d'Applications en Traitement de l'Image) All rights reserved.","title":"Pydicom"},{"location":"copyright/#matplotlib","text":"https://matplotlib.org/users/license.html Matplotlib only uses BSD compatible code, and its license is based on the PSF license. See the Open Source Initiative licenses page for details on individual licenses. Non-BSD compatible licenses (e.g., LGPL) are acceptable in matplotlib toolkits. For a discussion of the motivations behind the licensing choice, see Licenses.","title":"Matplotlib"},{"location":"copyright/#confusion-matrix-matplotlib","text":"Code snippet adapted with modification from : https://www.kaggle.com/grfiv4/plot-a-confusion-matrix","title":"Confusion Matrix Matplotlib"},{"location":"datautils/","text":"Data Module radtorch.datautils list_of_files datautils.list_of_files(root) Create a list of file paths from a root folder and its sub directories. Arguments root: (str) path of target folder. Output list of file paths. Example root_path = 'root/' list_of_files(root_path) ['root/folder1/0000.dcm', 'root/folder1/0001.dcm', 'root/folder2/0000.dcm', ...] path_to_class datautils.path_to_class(filepath) Creates a class name from the immediate parent folder of a target file. Arguments filepath: (str) path to target file. Output (str) folder name / class name. Example file_path = 'root/folder1/folder2/0000.dcm' path_to_class(file_path) 'folder2' root_to_class datautils.root_to_class(root) Creates list of classes and dictionary of classes and idx in a given data root. All first level subfolders within the root are converted into classes and given class id. Arguments root: (str) path of target root. Output (tuple) of classes: (list) of generated classes, class_to_idx: (dictionary) of classes and class id numbers Example This example assumes that root folder contains 3 folders (folder1, folder2 and folder3) each contains images of 1 class. root_folder = 'root/' root_to_class(root_folder) ['folder1', 'folder2', 'folder3'], {'folder1':0, 'folder2':1, 'folder3':2} class_to_idx datautils.class_to_idx(classes) Creates a dictionary of classes to classes idx from provided list of classes Arguments classes: (list) list of classes Output Output: (dictionary) dictionary of classes to class idx Example class_list = ['class1','class4', 'class2', 'class3'] class_to_idx(class_list) {'class1':0, 'class2':1, 'class3':2, 'class4':3} dataset_from_folder datautils.dataset_from_folder(data_directory, is_dicom=True, mode='RAW', wl=None, trans=Compose(ToTensor())) Creates a dataset from a root directory using subdirectories as classes/labels. Parameters data_director: (str) target data root directory. is_dicom: (boolean) True for DICOM images, False for regular images.(default=True) mode: (str) output mode for DICOM images only. options: RAW= Raw pixels, HU= Image converted to Hounsefield Units, WIN= 'window' image windowed to certain W and L, MWIN = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together]. wl: (list) list of lists of combinations of window level and widths to be used with WIN and MWIN. In the form of : [[Level,Width], [Level,Width],\u2026]. Only 3 combinations are allowed for MWIN (for now). (default=None) trans: (pytorch transforms) pytroch transforms to be performed on the dataset. (default=Convert to tensor) Methods class_to_idx Returns dictionary of dataset classes and corresponding class id. classes Returns list of dataset classes info Returns detailed information of the dataset. dataset_from_table datautils.dataset_from_table(data_directory, is_csv=True, is_dicom=True, input_source=None, img_path_column='IMAGE_PATH', img_label_column='IMAGE_LABEL', mode='RAW', wl=None, trans=Compose(ToTensor())) Creates a dataset using labels and filepaths from a table which can be either a excel sheet or pandas dataframe. Parameters data_directory: (str) target data directory. is_csv: (boolean) True for csv, False for pandas dataframe. (default=True) is_dicom: (boolean) True for DICOM images, False for regular images.(default=True) input_source: (str or pandas dataframe object) source for labelling data. This is path to csv file or name of pandas dataframe if pandas to be used. img_path_column: (str) name of the image path column in data input. (default = \"IMAGE_PATH\") img_label_column: (str) name of label column in the data input (default = \"IMAGE_LABEL\") mode: (str) output mode for DICOM images only. options: RAW= Raw pixels, HU= Image converted to Hounsefield Units, WIN= 'window' image windowed to certain W and L, MWIN = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together]. wl: (list) list of lists of combinations of window level and widths to be used with WIN and MWIN.In the form of : [[Level,Width], [Level,Width],\u2026]. Only 3 combinations are allowed for MWIN (for now). (default=None) transforms: (pytorch transforms) pytroch transforms to be performed on the dataset. (default=Convert to tensor) Methods class_to_idx Returns dictionary of dataset classes and corresponding class id. classes Returns list of dataset classes info Returns detailed information of the dataset.","title":"Datautils Module"},{"location":"datautils/#data-module-radtorchdatautils","text":"","title":"Data Module  radtorch.datautils "},{"location":"datautils/#list_of_files","text":"datautils.list_of_files(root) Create a list of file paths from a root folder and its sub directories. Arguments root: (str) path of target folder. Output list of file paths. Example root_path = 'root/' list_of_files(root_path) ['root/folder1/0000.dcm', 'root/folder1/0001.dcm', 'root/folder2/0000.dcm', ...]","title":"list_of_files"},{"location":"datautils/#path_to_class","text":"datautils.path_to_class(filepath) Creates a class name from the immediate parent folder of a target file. Arguments filepath: (str) path to target file. Output (str) folder name / class name. Example file_path = 'root/folder1/folder2/0000.dcm' path_to_class(file_path) 'folder2'","title":"path_to_class"},{"location":"datautils/#root_to_class","text":"datautils.root_to_class(root) Creates list of classes and dictionary of classes and idx in a given data root. All first level subfolders within the root are converted into classes and given class id. Arguments root: (str) path of target root. Output (tuple) of classes: (list) of generated classes, class_to_idx: (dictionary) of classes and class id numbers Example This example assumes that root folder contains 3 folders (folder1, folder2 and folder3) each contains images of 1 class. root_folder = 'root/' root_to_class(root_folder) ['folder1', 'folder2', 'folder3'], {'folder1':0, 'folder2':1, 'folder3':2}","title":"root_to_class"},{"location":"datautils/#class_to_idx","text":"datautils.class_to_idx(classes) Creates a dictionary of classes to classes idx from provided list of classes Arguments classes: (list) list of classes Output Output: (dictionary) dictionary of classes to class idx Example class_list = ['class1','class4', 'class2', 'class3'] class_to_idx(class_list) {'class1':0, 'class2':1, 'class3':2, 'class4':3}","title":"class_to_idx"},{"location":"datautils/#dataset_from_folder","text":"datautils.dataset_from_folder(data_directory, is_dicom=True, mode='RAW', wl=None, trans=Compose(ToTensor())) Creates a dataset from a root directory using subdirectories as classes/labels. Parameters data_director: (str) target data root directory. is_dicom: (boolean) True for DICOM images, False for regular images.(default=True) mode: (str) output mode for DICOM images only. options: RAW= Raw pixels, HU= Image converted to Hounsefield Units, WIN= 'window' image windowed to certain W and L, MWIN = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together]. wl: (list) list of lists of combinations of window level and widths to be used with WIN and MWIN. In the form of : [[Level,Width], [Level,Width],\u2026]. Only 3 combinations are allowed for MWIN (for now). (default=None) trans: (pytorch transforms) pytroch transforms to be performed on the dataset. (default=Convert to tensor) Methods class_to_idx Returns dictionary of dataset classes and corresponding class id. classes Returns list of dataset classes info Returns detailed information of the dataset.","title":"dataset_from_folder"},{"location":"datautils/#dataset_from_table","text":"datautils.dataset_from_table(data_directory, is_csv=True, is_dicom=True, input_source=None, img_path_column='IMAGE_PATH', img_label_column='IMAGE_LABEL', mode='RAW', wl=None, trans=Compose(ToTensor())) Creates a dataset using labels and filepaths from a table which can be either a excel sheet or pandas dataframe. Parameters data_directory: (str) target data directory. is_csv: (boolean) True for csv, False for pandas dataframe. (default=True) is_dicom: (boolean) True for DICOM images, False for regular images.(default=True) input_source: (str or pandas dataframe object) source for labelling data. This is path to csv file or name of pandas dataframe if pandas to be used. img_path_column: (str) name of the image path column in data input. (default = \"IMAGE_PATH\") img_label_column: (str) name of label column in the data input (default = \"IMAGE_LABEL\") mode: (str) output mode for DICOM images only. options: RAW= Raw pixels, HU= Image converted to Hounsefield Units, WIN= 'window' image windowed to certain W and L, MWIN = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together]. wl: (list) list of lists of combinations of window level and widths to be used with WIN and MWIN.In the form of : [[Level,Width], [Level,Width],\u2026]. Only 3 combinations are allowed for MWIN (for now). (default=None) transforms: (pytorch transforms) pytroch transforms to be performed on the dataset. (default=Convert to tensor) Methods class_to_idx Returns dictionary of dataset classes and corresponding class id. classes Returns list of dataset classes info Returns detailed information of the dataset.","title":"dataset_from_table"},{"location":"dicomutils/","text":"DICOM Module radtorch.dicomutils Tools and Functions for DICOM images handling and extraction of pixel information. from radtorch import dicomutils window_dicom dicomutils.window_dicom(filepath, level, width) Converts DICOM image to numpy array with certain width and level. Arguments filepath: (str) input DICOM image path. level: (int) target window level. width: (int) target window width. Output Output: (array) windowed image as numpy array. dicom_to_narray dicomutils.dicom_to_narray(filepath, mode='RAW', wl=None) Converts DICOM image to a numpy array with target changes as below. Arguments filepath: (str) input DICOM image path. mode: (str) output mode. (default='RAW') Options: 'RAW' = Raw pixels, 'HU' = Image converted to Hounsefield Units. 'WIN' = 'window' image windowed to certain W and L, 'MWIN' = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together]. wl: (list) list of lists of combinations of window level and widths to be used with WIN and MWIN. (default=None) In the form of : [[Level,Width], [Level,Width],...]. Only 3 combinations are allowed for MWIN (for now). Output Output: (array) array of same shape as input DICOM image with 1 channel. In case of MWIN mode, output has same size by 3 channels. dicom_to_pil dicomutils.dicom_to_pil(filepath) Converts DICOM image to PIL image object. Arguments filepath: (str) input DICOM image path. Output Output: (pillow image object) .","title":"Dicomutils Module"},{"location":"dicomutils/#dicom-module-radtorchdicomutils","text":"Tools and Functions for DICOM images handling and extraction of pixel information. from radtorch import dicomutils","title":"DICOM Module  radtorch.dicomutils "},{"location":"dicomutils/#window_dicom","text":"dicomutils.window_dicom(filepath, level, width) Converts DICOM image to numpy array with certain width and level. Arguments filepath: (str) input DICOM image path. level: (int) target window level. width: (int) target window width. Output Output: (array) windowed image as numpy array.","title":"window_dicom"},{"location":"dicomutils/#dicom_to_narray","text":"dicomutils.dicom_to_narray(filepath, mode='RAW', wl=None) Converts DICOM image to a numpy array with target changes as below. Arguments filepath: (str) input DICOM image path. mode: (str) output mode. (default='RAW') Options: 'RAW' = Raw pixels, 'HU' = Image converted to Hounsefield Units. 'WIN' = 'window' image windowed to certain W and L, 'MWIN' = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together]. wl: (list) list of lists of combinations of window level and widths to be used with WIN and MWIN. (default=None) In the form of : [[Level,Width], [Level,Width],...]. Only 3 combinations are allowed for MWIN (for now). Output Output: (array) array of same shape as input DICOM image with 1 channel. In case of MWIN mode, output has same size by 3 channels.","title":"dicom_to_narray"},{"location":"dicomutils/#dicom_to_pil","text":"dicomutils.dicom_to_pil(filepath) Converts DICOM image to PIL image object. Arguments filepath: (str) input DICOM image path. Output Output: (pillow image object) .","title":"dicom_to_pil"},{"location":"install/","text":"Installation RADTorch tool kit and its dependencies can be installed using the following terminal commands: git clone github.com/radtorch/radtorch.git pip3 install radtorch/. To uninstall simply use: pip3 uninstall radtorch Requirements Internet connection is required for installation and model weight download. Python 3.5 or later. Dependencies RADTorch depends on the following packages which are installed automatically during the installation process. torch 1.4.0 torchvision 0.5.0 numpy 1.17.4 pandas 0.25.3 pydicom 1.3.0 matplotlib 3.1.3 pillow 7.0.0 tqdm 4.38.0 sklearn 0.22.1 pathlib","title":"Installation"},{"location":"install/#installation","text":"RADTorch tool kit and its dependencies can be installed using the following terminal commands: git clone github.com/radtorch/radtorch.git pip3 install radtorch/. To uninstall simply use: pip3 uninstall radtorch","title":"Installation"},{"location":"install/#requirements","text":"Internet connection is required for installation and model weight download. Python 3.5 or later.","title":"Requirements"},{"location":"install/#dependencies","text":"RADTorch depends on the following packages which are installed automatically during the installation process. torch 1.4.0 torchvision 0.5.0 numpy 1.17.4 pandas 0.25.3 pydicom 1.3.0 matplotlib 3.1.3 pillow 7.0.0 tqdm 4.38.0 sklearn 0.22.1 pathlib","title":"Dependencies"},{"location":"license/","text":"MIT License Copyright \u00a9 2020 RADTorch, Mohamed Elbanan M.D. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#mit-license","text":"Copyright \u00a9 2020 RADTorch, Mohamed Elbanan M.D. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"MIT License"},{"location":"modelsutils/","text":"Models Module radtorch.modelsutils create_model modelsutils.create_model(model_arch, output_classes, mode, pre_trained=True, unfreeze_weights=True) Creates a PyTorch training neural network model with specified network architecture. Input channels and output classes can be specified. Arguments model_arch: (str) The architecture of the model neural network. Examples include 'vgg16', 'resnet50', and 'resnet152'. pre_trained: (boolen) Load the pretrained weights of the neural network. (default=True) unfreeze_weights: (boolen) Unfreeze model weights for training.(default=True) output_classes: (int) Number of output classes for image classification problems. mode: (str) 'train' for training model. 'feature_extraction' for feature extraction model Output Output: (PyTorch neural network object) Example my_model = modelsutils.create_model(model_arch='resnet50', output_classes=2) create_loss_function modelsutils.create_loss_function(type) Creates a PyTorch training loss function object. Arguments type: (str) type of the loss functions required. Output Output: (PyTorch loss function object) Example loss = modelsutils.create_loss_function(type='NLLLoss') create_optimizer modelsutils.create_optimizer(traning_model, optimizer_type, learning_rate) Creates a PyTorch optimizer object. Arguments training_model: (pytorch Model object) target training model. optimizer_type: (str) type of optimizer e.g.'Adam' or 'SGD'. learning_rate: (float) learning rate. Output Output: (PyTorch optimizer object) train_model train_model(model, train_data_loader, valid_data_loader, train_data_set, valid_data_set, loss_criterion, optimizer, epochs, device, verbose) Training loop for pytorch model object. Arguments model: (PyTorch neural network object) Model to be trained. train_data_loader: (PyTorch dataloader object) training data dataloader. valid_data_loader: (PyTorch dataloader object) validation data dataloader. train_data_loader: (PyTorch dataset object) training data dataset. valid_data_loader: (PyTorch dataset object) validation data dataset. loss_criterion: (PyTorch nn object) Loss function to be used during training. optimizer: (PyTorch optimizer object) Optimizer to be used during training. epochs: (int) training epochs. device: (str) device to be used for training. This can be 'cpu' or 'cuda'. verbose: (boolen) True to display training messages. Output model: (PyTorch neural network object) trained model. train_metrics: (list) list of np arrays of training loss and accuracy. Example trained_model, training_metrics = modelsutils.train_model(model=my_model, train_data_loader=train_dl, valid_data_loader=valid_dl, train_data_set=train_ds, valid_data_set=valid_ds, loss_criterion=my_loss, optimizer=my_optim, epochs=100, device='cuda', verbose=True) model_inference modelsutils.model_inference(model, input_image_path, inference_transformations=transforms.Compose([transforms.ToTensor()])) Performs Inference/Predictions on a target image using a trained model. Arguments model: (PyTorch Model) Trained neural network. input_image_path: (str) path to target image inference_transformations: (pytorch transforms list) pytroch transforms to be performed on the dataset. Output Output: (tupe) tuple of prediction class id and prediction accuracy percentage. supported modelsutils.supported() Returns a list of the currently supported network architectures and loss functions.","title":"Modelsutils Module"},{"location":"modelsutils/#models-module-radtorchmodelsutils","text":"","title":"Models Module  radtorch.modelsutils "},{"location":"modelsutils/#create_model","text":"modelsutils.create_model(model_arch, output_classes, mode, pre_trained=True, unfreeze_weights=True) Creates a PyTorch training neural network model with specified network architecture. Input channels and output classes can be specified. Arguments model_arch: (str) The architecture of the model neural network. Examples include 'vgg16', 'resnet50', and 'resnet152'. pre_trained: (boolen) Load the pretrained weights of the neural network. (default=True) unfreeze_weights: (boolen) Unfreeze model weights for training.(default=True) output_classes: (int) Number of output classes for image classification problems. mode: (str) 'train' for training model. 'feature_extraction' for feature extraction model Output Output: (PyTorch neural network object) Example my_model = modelsutils.create_model(model_arch='resnet50', output_classes=2)","title":"create_model"},{"location":"modelsutils/#create_loss_function","text":"modelsutils.create_loss_function(type) Creates a PyTorch training loss function object. Arguments type: (str) type of the loss functions required. Output Output: (PyTorch loss function object) Example loss = modelsutils.create_loss_function(type='NLLLoss')","title":"create_loss_function"},{"location":"modelsutils/#create_optimizer","text":"modelsutils.create_optimizer(traning_model, optimizer_type, learning_rate) Creates a PyTorch optimizer object. Arguments training_model: (pytorch Model object) target training model. optimizer_type: (str) type of optimizer e.g.'Adam' or 'SGD'. learning_rate: (float) learning rate. Output Output: (PyTorch optimizer object)","title":"create_optimizer"},{"location":"modelsutils/#train_model","text":"train_model(model, train_data_loader, valid_data_loader, train_data_set, valid_data_set, loss_criterion, optimizer, epochs, device, verbose) Training loop for pytorch model object. Arguments model: (PyTorch neural network object) Model to be trained. train_data_loader: (PyTorch dataloader object) training data dataloader. valid_data_loader: (PyTorch dataloader object) validation data dataloader. train_data_loader: (PyTorch dataset object) training data dataset. valid_data_loader: (PyTorch dataset object) validation data dataset. loss_criterion: (PyTorch nn object) Loss function to be used during training. optimizer: (PyTorch optimizer object) Optimizer to be used during training. epochs: (int) training epochs. device: (str) device to be used for training. This can be 'cpu' or 'cuda'. verbose: (boolen) True to display training messages. Output model: (PyTorch neural network object) trained model. train_metrics: (list) list of np arrays of training loss and accuracy. Example trained_model, training_metrics = modelsutils.train_model(model=my_model, train_data_loader=train_dl, valid_data_loader=valid_dl, train_data_set=train_ds, valid_data_set=valid_ds, loss_criterion=my_loss, optimizer=my_optim, epochs=100, device='cuda', verbose=True)","title":"train_model"},{"location":"modelsutils/#model_inference","text":"modelsutils.model_inference(model, input_image_path, inference_transformations=transforms.Compose([transforms.ToTensor()])) Performs Inference/Predictions on a target image using a trained model. Arguments model: (PyTorch Model) Trained neural network. input_image_path: (str) path to target image inference_transformations: (pytorch transforms list) pytroch transforms to be performed on the dataset. Output Output: (tupe) tuple of prediction class id and prediction accuracy percentage.","title":"model_inference"},{"location":"modelsutils/#supported","text":"modelsutils.supported() Returns a list of the currently supported network architectures and loss functions.","title":"supported"},{"location":"pipeline/","text":"Pipeline Module radtorch.pipeline Pipelines are probably the most exciting feature of RADTorch tool kit. With few lines of code, the pipeline module allows you to run state-of-the-art image classification algorithms and much more. Image_Classification pipeline.Image_Classification(data_directory, transformations='default', custom_resize='default', device='default', optimizer='Adam', is_dicom=True, label_from_table=False, is_csv=None, table_source=None, path_col='IMAGE_PATH', label_col='IMAGE_LABEL', mode='RAW', wl=None, batch_size=16, test_percent=0.2, valid_percent=0.2, model_arch='vgg16', pre_trained=True, unfreeze_weights=True, train_epochs=20, learning_rate=0.0001, loss_function='CrossEntropyLoss') The Image Classification pipeline simplifies the process of binary and multi-class image classification into a single line of code. Under the hood, the following happens: The pipeline creates a master dataset from the provided data directory and source of labels/classes either from folder structre or pandas/csv table. Master dataset is subdivided into train, valid and test subsets using the percentages defined by user. The following transformations are applied on the dataset images: Resize to the default image size allowed by the model architecture. Window/Level adjustment according to values specified by user. Single channel grayscale DICOM images are converted into 3 channel grayscale images to fit into the model. Selected Model architecture, optimizer, and loss function are downloaded/created. Model is trained. Training metrics are saved as training progresses and can be displayed after training is done. Confusion Matrix and ROC (for binary classification) can be displayed as well (by default, the test subset is used to calculate the confusion matrix and the ROC) Trained model can be exported to outside file for future use. Parameters data_directory: (str) target data directory. (Required) is_dicom: (boolean) True for DICOM images, False for regular images.(default=True) label_from_table: (boolean) True if labels are to extracted from table, False if labels are to be extracted from subfolders. (default=False) is_csv: (boolean) True for csv, False for pandas dataframe. table_source: (str or pandas dataframe object) source for labelling data.This is path to csv file or name of pandas dataframe if pandas to be used. (default=None). path_col: (str) name of the column with the image path. (default='IMAGE_PATH') label_col: (str) name of the label/class column. (default='IMAGE_LABEL') mode: (str) output mode for DICOM images only where RAW= Raw pixels, HU= Image converted to Hounsefield Units, WIN= 'window' image windowed to certain W and L, MWIN = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together. (default='RAW') wl: (list) list of lists of combinations of window level and widths to be used with WIN and MWIN.In the form of : [[Level,Width], [Level,Width],...]. Only 3 combinations are allowed for MWIN (for now). (default=None) transformations: (pytorch transforms list) pytroch transforms to be performed on the dataset. (default=Convert to tensor) custom_resize: (int) by default, a radtorch pipeline will resize the input images into the default training model input image size as demosntrated in the table shown in radtorch home page. This default size can be changed here if needed. batch_size: (int) batch size of the dataset (default=16) test_percent: (float) percentage of dataset to use for testing. Float value between 0 and 1.0. (default=0.2) valid_percent: (float) percentage of dataset to use for validation. Float value between 0 and 1.0. (default=0.2) model_arch: (str) PyTorch neural network architecture (default='vgg16') pre_trained: (boolean) Load the pretrained weights of the neural network. (default=True) unfreeze_weights: (boolean) if True, all model weights will be retrained. (default=True) train_epochs: (int) Number of training epochs. (default=20) learning_rate: (str) training learning rate. (default = 0.0001) loss_function: (str) training loss function. (default='CrossEntropyLoss') optimizer: (str) Optimizer to be used during training. (default='Adam') device: (str) device to be used for training. This can be adjusted to 'cpu' or 'cuda'. If nothing is selected, the pipeline automatically detects if cuda is available and trains on it. Methods info Display Parameters of the Image Classification Pipeline. dataset_info Display Dataset Information. sample Display sample of the training dataset. Arguments: num_of_images_per_row: (int) number of images per column. (default=5) fig_size: (tuple) figure size. (default=(10,10)) show_labels: (boolean) show the image label idx. (default=True) train Train the image classification pipeline. Arguments: verbose: (boolean) Show display progress after each epoch. (default=True) metrics Display the training metrics. export_model Export the trained model into a target file. Arguments: output_path: (str) path to output file. For example 'foler/folder/model.pth' set_trained_model Loads a previously trained model into pipeline Arguments: model_path: (str) path to target model mode: (str) either 'train' or 'infer'.'train' will load the model to be trained. 'infer' will load the model for inference. Examples Importing the pipeline and setting up data directory from radtorch import pipeline data_root = '/content/data' Create the image classifier pipeline The below example will create the pipeline using resnet50 model architecture with trained weights loaded and will train for 30 epochs. clf = pipeline.Image_Classification(data_directory=data_root, mode='HU', model_arch='resnet50', train_epochs=30) Show dataset Information clf.dataset_info() Number of intances = 77 Number of classes = 2 Class IDX = {'axr': 0, 'cxr': 1} Class Frequency: Class Number of instances 1 39 0 38 None Train Dataset Size 47 Valid Dataset Size 15 Test Dataset Size 15 Display sample of the dataset clf.sample() Train the classifier clf.train() Starting training at 2020-02-27 17:47:54.918173 Epoch : 000/30 : [Training: Loss: 0.7937, Accuracy: 46.8085%] [Validation : Loss : 0.7436, Accuracy: 40.0000%] [Time: 1.4200s] Epoch : 001/30 : [Training: Loss: 0.6054, Accuracy: 63.8298%] [Validation : Loss : 0.7793, Accuracy: 40.0000%] [Time: 1.3436s] Epoch : 002/30 : [Training: Loss: 0.5504, Accuracy: 80.8511%] [Validation : Loss : 1.2314, Accuracy: 40.0000%] [Time: 1.3500s] ... ... ... Epoch : 026/30 : [Training: Loss: 0.0499, Accuracy: 97.8723%] [Validation : Loss : 0.4143, Accuracy: 93.3333%] [Time: 1.3612s] Epoch : 027/30 : [Training: Loss: 0.0235, Accuracy: 97.8723%] [Validation : Loss : 0.0321, Accuracy: 100.000%] [Time: 1.3540s] Epoch : 028/30 : [Training: Loss: 0.0142, Accuracy: 100.000%] [Validation : Loss : 0.2476, Accuracy: 93.3333%] [Time: 1.3584s] Epoch : 029/30 : [Training: Loss: 0.0067, Accuracy: 100.000%] [Validation : Loss : 0.4216, Accuracy: 93.3333%] [Time: 1.3728s] Total training time = 0:00:40.758802 Display training metrics clf.metrics() Display Confusion Matrix clf.confusion_matrix() Display ROC clf.roc() Export Trained Model clf.export_model('/folder/model.pth') Feature_Extraction pipeline.Feature_Extraction(data_directory, transformations='default', custom_resize = 'default', is_dicom=True,label_from_table=False, is_csv=None,table_source=None, device='default', path_col = 'IMAGE_PATH', label_col = 'IMAGE_LABEL', mode='RAW', wl=None, model_arch='vgg16', pre_trained=True, unfreeze_weights=False) The feature extraction pipeline utilizes a pre-trained model to extract a set of features that can be used in another machine learning algorithms e.g. XGBoost. The trained model by default can one of the supported model architectures trained with default weights trained on the ImageNet dataset or a model that has been trained and exported using the image classification pipeline. The output is a pandas dataframe that has feature columns, label column and file path column. Under the hood, the pipeline removes the last FC layer of the pretrained models to output the features. The number of extracted features depends on the model architecture selected: Model Architecture Default Input Image Size Output Features VGG16 244 x 244 4096 VGG19 244 x 244 4096 resnet50 244 x 244 2048 resnet152 244 x 244 2048 resnet101 244 x 244 2048 wide_resnet50_2 244 x 244 2048 wide_resnet101_2 244 x 244 2048 inception_v3 299 x 299 2048 Parameters is_dicom: (boolean) True for DICOM images, False for regular images.(default=True) label_from_table: [boolean] True if labels are to extracted from table, False if labels are to be extracted from subfolders. (default=False) is_csv: (boolean) True for csv, False for pandas dataframe. table_source: (str or pandas dataframe object) source for labelling data. (default=None) This is path to csv file or name of pandas dataframe if pandas to be used. path_col: (str) name of the column with the image path. (default='IMAGE_PATH') label_col: (str) name of the label/class column. (default='IMAGE_LABEL') mode: (str) output mode for DICOM images only. .Options: RAW= Raw pixels, HU= Image converted to Hounsefield Units, WIN= 'window' image windowed to certain W and L, MWIN = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together]. (default='RAW') wl: (list) list of lists of combinations of window level and widths to be used with WIN and MWIN. In the form of : [[Level,Width], [Level,Width],...]. Only 3 combinations are allowed for MWIN (for now).(default=None) transformations: (pytorch transforms) pytroch transforms to be performed on the dataset. (default=Convert to tensor) custom_resize: (int) by default, a radtorch pipeline will resize the input images into the default training model input image size as demosntrated in the table shown in radtorch home page. This default size can be changed here if needed. model_arch: [str] PyTorch neural network architecture (default='vgg16') pre_trained: (boolean) Load the pretrained weights of the neural network. If False, the last layer is only retrained = Transfer Learning. (default=True) unfreeze_weights: (boolean) if True, all model weights, not just final layer, will be retrained. (default=False) device: (str) device to be used for training. This can be adjusted to 'cpu' or 'cuda'. If nothing is selected, the pipeline automatically detects if cuda is available and trains on it. Methods info Displays Feature Extraction Pipeline Parameters. dataset_info Display Dataset Information. sample Display sample of the training dataset. num_features Displays number of features to be extracted. run Extracts features from dataset. Arguments: verbose: (boolean) Show the feature table. (default=True) export_features Exports the features to csv. Arguments: csv_path: (str) Path to output csv file. set_trained_model Loads a previously trained model into pipeline Arguments: model_path: (str) path to target model mode: (str) either 'train' or 'infer'.'train' will load the model to be trained. 'infer' will load the model for inference. Examples Importing the pipeline and setting up data directory from radtorch import pipeline data_root = '/content/data' Create the feature extractor pipeline The below example will create the pipeline using resnet152 model architecture with trained weights loaded. extractor = pipeline.Feature_Extraction(data_directory=data_root, mode='HU', model_arch='resnet152') Display number of Features to be extracted extractor.num_features() 2048 Show Dataset information extractor.dataset_info() Number of intances = 77 Number of classes = 2 Class IDX = {'axr': 0, 'cxr': 1} Class Frequency: Class Number of instances 0 38 1 39 Display sample of the dataset extractor.sample() Run pipeline to extract features extractor.run() | | img_path | label_idx | f_0 | f_1 | f_2 | f_3 | ........ | |---:|:---------------|-----------:|----------:|---------:|------------:|---------:|---------:| | 0 | /content/dat...| 0 | 0.135294 | 0.368051 | 0.000352088 | 0.378677 | ........ | | 1 | /content/dat...| 0 | 0.0721618 | 0.930238 | 0.0286931 | 0.732228 | ........ | | 2 | /content/dat...| 0 | 0.0780637 | 0.432966 | 0.0175741 | 0.685681 | ........ | | 3 | /content/dat...| 0 | 0.560777 | 0.449213 | 0.0432512 | 0.432942 | ........ | | 4 | /content/dat...| 0 | 0.176524 | 0.669066 | 0.0396659 | 0.273474 | ........ | Show feature names list extractor.feature_names ['f_0','f_1','f_2','f_3','f_4', 'f_5', ... ]","title":"Pipeline Module"},{"location":"pipeline/#pipeline-module-radtorchpipeline","text":"Pipelines are probably the most exciting feature of RADTorch tool kit. With few lines of code, the pipeline module allows you to run state-of-the-art image classification algorithms and much more.","title":"Pipeline Module  radtorch.pipeline "},{"location":"pipeline/#image_classification","text":"pipeline.Image_Classification(data_directory, transformations='default', custom_resize='default', device='default', optimizer='Adam', is_dicom=True, label_from_table=False, is_csv=None, table_source=None, path_col='IMAGE_PATH', label_col='IMAGE_LABEL', mode='RAW', wl=None, batch_size=16, test_percent=0.2, valid_percent=0.2, model_arch='vgg16', pre_trained=True, unfreeze_weights=True, train_epochs=20, learning_rate=0.0001, loss_function='CrossEntropyLoss') The Image Classification pipeline simplifies the process of binary and multi-class image classification into a single line of code. Under the hood, the following happens: The pipeline creates a master dataset from the provided data directory and source of labels/classes either from folder structre or pandas/csv table. Master dataset is subdivided into train, valid and test subsets using the percentages defined by user. The following transformations are applied on the dataset images: Resize to the default image size allowed by the model architecture. Window/Level adjustment according to values specified by user. Single channel grayscale DICOM images are converted into 3 channel grayscale images to fit into the model. Selected Model architecture, optimizer, and loss function are downloaded/created. Model is trained. Training metrics are saved as training progresses and can be displayed after training is done. Confusion Matrix and ROC (for binary classification) can be displayed as well (by default, the test subset is used to calculate the confusion matrix and the ROC) Trained model can be exported to outside file for future use.","title":"Image_Classification"},{"location":"pipeline/#parameters","text":"data_directory: (str) target data directory. (Required) is_dicom: (boolean) True for DICOM images, False for regular images.(default=True) label_from_table: (boolean) True if labels are to extracted from table, False if labels are to be extracted from subfolders. (default=False) is_csv: (boolean) True for csv, False for pandas dataframe. table_source: (str or pandas dataframe object) source for labelling data.This is path to csv file or name of pandas dataframe if pandas to be used. (default=None). path_col: (str) name of the column with the image path. (default='IMAGE_PATH') label_col: (str) name of the label/class column. (default='IMAGE_LABEL') mode: (str) output mode for DICOM images only where RAW= Raw pixels, HU= Image converted to Hounsefield Units, WIN= 'window' image windowed to certain W and L, MWIN = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together. (default='RAW') wl: (list) list of lists of combinations of window level and widths to be used with WIN and MWIN.In the form of : [[Level,Width], [Level,Width],...]. Only 3 combinations are allowed for MWIN (for now). (default=None) transformations: (pytorch transforms list) pytroch transforms to be performed on the dataset. (default=Convert to tensor) custom_resize: (int) by default, a radtorch pipeline will resize the input images into the default training model input image size as demosntrated in the table shown in radtorch home page. This default size can be changed here if needed. batch_size: (int) batch size of the dataset (default=16) test_percent: (float) percentage of dataset to use for testing. Float value between 0 and 1.0. (default=0.2) valid_percent: (float) percentage of dataset to use for validation. Float value between 0 and 1.0. (default=0.2) model_arch: (str) PyTorch neural network architecture (default='vgg16') pre_trained: (boolean) Load the pretrained weights of the neural network. (default=True) unfreeze_weights: (boolean) if True, all model weights will be retrained. (default=True) train_epochs: (int) Number of training epochs. (default=20) learning_rate: (str) training learning rate. (default = 0.0001) loss_function: (str) training loss function. (default='CrossEntropyLoss') optimizer: (str) Optimizer to be used during training. (default='Adam') device: (str) device to be used for training. This can be adjusted to 'cpu' or 'cuda'. If nothing is selected, the pipeline automatically detects if cuda is available and trains on it.","title":"Parameters"},{"location":"pipeline/#methods","text":"info Display Parameters of the Image Classification Pipeline. dataset_info Display Dataset Information. sample Display sample of the training dataset. Arguments: num_of_images_per_row: (int) number of images per column. (default=5) fig_size: (tuple) figure size. (default=(10,10)) show_labels: (boolean) show the image label idx. (default=True) train Train the image classification pipeline. Arguments: verbose: (boolean) Show display progress after each epoch. (default=True) metrics Display the training metrics. export_model Export the trained model into a target file. Arguments: output_path: (str) path to output file. For example 'foler/folder/model.pth' set_trained_model Loads a previously trained model into pipeline Arguments: model_path: (str) path to target model mode: (str) either 'train' or 'infer'.'train' will load the model to be trained. 'infer' will load the model for inference.","title":"Methods"},{"location":"pipeline/#examples","text":"Importing the pipeline and setting up data directory from radtorch import pipeline data_root = '/content/data' Create the image classifier pipeline The below example will create the pipeline using resnet50 model architecture with trained weights loaded and will train for 30 epochs. clf = pipeline.Image_Classification(data_directory=data_root, mode='HU', model_arch='resnet50', train_epochs=30) Show dataset Information clf.dataset_info() Number of intances = 77 Number of classes = 2 Class IDX = {'axr': 0, 'cxr': 1} Class Frequency: Class Number of instances 1 39 0 38 None Train Dataset Size 47 Valid Dataset Size 15 Test Dataset Size 15 Display sample of the dataset clf.sample() Train the classifier clf.train() Starting training at 2020-02-27 17:47:54.918173 Epoch : 000/30 : [Training: Loss: 0.7937, Accuracy: 46.8085%] [Validation : Loss : 0.7436, Accuracy: 40.0000%] [Time: 1.4200s] Epoch : 001/30 : [Training: Loss: 0.6054, Accuracy: 63.8298%] [Validation : Loss : 0.7793, Accuracy: 40.0000%] [Time: 1.3436s] Epoch : 002/30 : [Training: Loss: 0.5504, Accuracy: 80.8511%] [Validation : Loss : 1.2314, Accuracy: 40.0000%] [Time: 1.3500s] ... ... ... Epoch : 026/30 : [Training: Loss: 0.0499, Accuracy: 97.8723%] [Validation : Loss : 0.4143, Accuracy: 93.3333%] [Time: 1.3612s] Epoch : 027/30 : [Training: Loss: 0.0235, Accuracy: 97.8723%] [Validation : Loss : 0.0321, Accuracy: 100.000%] [Time: 1.3540s] Epoch : 028/30 : [Training: Loss: 0.0142, Accuracy: 100.000%] [Validation : Loss : 0.2476, Accuracy: 93.3333%] [Time: 1.3584s] Epoch : 029/30 : [Training: Loss: 0.0067, Accuracy: 100.000%] [Validation : Loss : 0.4216, Accuracy: 93.3333%] [Time: 1.3728s] Total training time = 0:00:40.758802 Display training metrics clf.metrics() Display Confusion Matrix clf.confusion_matrix() Display ROC clf.roc() Export Trained Model clf.export_model('/folder/model.pth')","title":"Examples"},{"location":"pipeline/#feature_extraction","text":"pipeline.Feature_Extraction(data_directory, transformations='default', custom_resize = 'default', is_dicom=True,label_from_table=False, is_csv=None,table_source=None, device='default', path_col = 'IMAGE_PATH', label_col = 'IMAGE_LABEL', mode='RAW', wl=None, model_arch='vgg16', pre_trained=True, unfreeze_weights=False) The feature extraction pipeline utilizes a pre-trained model to extract a set of features that can be used in another machine learning algorithms e.g. XGBoost. The trained model by default can one of the supported model architectures trained with default weights trained on the ImageNet dataset or a model that has been trained and exported using the image classification pipeline. The output is a pandas dataframe that has feature columns, label column and file path column. Under the hood, the pipeline removes the last FC layer of the pretrained models to output the features. The number of extracted features depends on the model architecture selected: Model Architecture Default Input Image Size Output Features VGG16 244 x 244 4096 VGG19 244 x 244 4096 resnet50 244 x 244 2048 resnet152 244 x 244 2048 resnet101 244 x 244 2048 wide_resnet50_2 244 x 244 2048 wide_resnet101_2 244 x 244 2048 inception_v3 299 x 299 2048","title":"Feature_Extraction"},{"location":"pipeline/#parameters_1","text":"is_dicom: (boolean) True for DICOM images, False for regular images.(default=True) label_from_table: [boolean] True if labels are to extracted from table, False if labels are to be extracted from subfolders. (default=False) is_csv: (boolean) True for csv, False for pandas dataframe. table_source: (str or pandas dataframe object) source for labelling data. (default=None) This is path to csv file or name of pandas dataframe if pandas to be used. path_col: (str) name of the column with the image path. (default='IMAGE_PATH') label_col: (str) name of the label/class column. (default='IMAGE_LABEL') mode: (str) output mode for DICOM images only. .Options: RAW= Raw pixels, HU= Image converted to Hounsefield Units, WIN= 'window' image windowed to certain W and L, MWIN = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together]. (default='RAW') wl: (list) list of lists of combinations of window level and widths to be used with WIN and MWIN. In the form of : [[Level,Width], [Level,Width],...]. Only 3 combinations are allowed for MWIN (for now).(default=None) transformations: (pytorch transforms) pytroch transforms to be performed on the dataset. (default=Convert to tensor) custom_resize: (int) by default, a radtorch pipeline will resize the input images into the default training model input image size as demosntrated in the table shown in radtorch home page. This default size can be changed here if needed. model_arch: [str] PyTorch neural network architecture (default='vgg16') pre_trained: (boolean) Load the pretrained weights of the neural network. If False, the last layer is only retrained = Transfer Learning. (default=True) unfreeze_weights: (boolean) if True, all model weights, not just final layer, will be retrained. (default=False) device: (str) device to be used for training. This can be adjusted to 'cpu' or 'cuda'. If nothing is selected, the pipeline automatically detects if cuda is available and trains on it.","title":"Parameters"},{"location":"pipeline/#methods_1","text":"info Displays Feature Extraction Pipeline Parameters. dataset_info Display Dataset Information. sample Display sample of the training dataset. num_features Displays number of features to be extracted. run Extracts features from dataset. Arguments: verbose: (boolean) Show the feature table. (default=True) export_features Exports the features to csv. Arguments: csv_path: (str) Path to output csv file. set_trained_model Loads a previously trained model into pipeline Arguments: model_path: (str) path to target model mode: (str) either 'train' or 'infer'.'train' will load the model to be trained. 'infer' will load the model for inference.","title":"Methods"},{"location":"pipeline/#examples_1","text":"Importing the pipeline and setting up data directory from radtorch import pipeline data_root = '/content/data' Create the feature extractor pipeline The below example will create the pipeline using resnet152 model architecture with trained weights loaded. extractor = pipeline.Feature_Extraction(data_directory=data_root, mode='HU', model_arch='resnet152') Display number of Features to be extracted extractor.num_features() 2048 Show Dataset information extractor.dataset_info() Number of intances = 77 Number of classes = 2 Class IDX = {'axr': 0, 'cxr': 1} Class Frequency: Class Number of instances 0 38 1 39 Display sample of the dataset extractor.sample() Run pipeline to extract features extractor.run() | | img_path | label_idx | f_0 | f_1 | f_2 | f_3 | ........ | |---:|:---------------|-----------:|----------:|---------:|------------:|---------:|---------:| | 0 | /content/dat...| 0 | 0.135294 | 0.368051 | 0.000352088 | 0.378677 | ........ | | 1 | /content/dat...| 0 | 0.0721618 | 0.930238 | 0.0286931 | 0.732228 | ........ | | 2 | /content/dat...| 0 | 0.0780637 | 0.432966 | 0.0175741 | 0.685681 | ........ | | 3 | /content/dat...| 0 | 0.560777 | 0.449213 | 0.0432512 | 0.432942 | ........ | | 4 | /content/dat...| 0 | 0.176524 | 0.669066 | 0.0396659 | 0.273474 | ........ | Show feature names list extractor.feature_names ['f_0','f_1','f_2','f_3','f_4', 'f_5', ... ]","title":"Examples"},{"location":"version/","text":"Version Log v.0.1.0 _ 3-1-2020 Features Module Function Module Functions PipeLine Dicomutils Image_Classification window_dicom Feature_Extraction dicom_to_narray Datautils dicom_to_pil list_of_files Visutils path_to_class show_dataloader_sample root_to_class show_dataset_info class_to_idx show_metrics dataset_from_folder show_dicom_sample dataset_from_table show_roc Modelsutils show_nn_roc create_model show_confusion_matrix create_loss_function plot_confusion_matrix create_optimizer train_model model_inference supported","title":"Version log"},{"location":"version/#version-log","text":"","title":"Version Log"},{"location":"version/#v010-_-3-1-2020","text":"Features Module Function Module Functions PipeLine Dicomutils Image_Classification window_dicom Feature_Extraction dicom_to_narray Datautils dicom_to_pil list_of_files Visutils path_to_class show_dataloader_sample root_to_class show_dataset_info class_to_idx show_metrics dataset_from_folder show_dicom_sample dataset_from_table show_roc Modelsutils show_nn_roc create_model show_confusion_matrix create_loss_function plot_confusion_matrix create_optimizer train_model model_inference supported","title":"v.0.1.0 _ 3-1-2020"},{"location":"visutils/","text":"Visualization Module radtorch.visutils Different tools and utilities for data visualization. Based upon Matplotlib. from radtorch import visutils show_dataloader_sample visutils.show_dataloader_sample(dataloader, num_of_images_per_row=10, figsize=(10,10), show_labels=True) Displays sample of certain dataloader with corresponding class idx Arguments dataloader: (dataloader object) selected pytorch dataloader. num_of_images_per_row: (int) number of images per row. (default=10) figsize: (tuple) size of displayed figure. (default = (10,10)) show_labels: (boolen) display class idx of the sample displayed .(default=True) Output Output: (figure) show_dataset_info visutils.show_dataset_info(dataset) Displays a summary of the pytorch dataset information. Arguments dataset: (pytorch dataset object) target dataset to inspect. Output Output: (str) Dataset information including: Number of instances Number of classes Dictionary of class and class_id Class frequency breakdown. show_metrics visutils.show_metrics(source, fig_size=(15,5)) Displays metrics created by the training loop. Arguments source: (list) the metrics generated during the training process as by modelsutils.train_model() fig_size: (tuple) size of the displayed figure. (default=15,5) Output Output: (figure) Matplotlib graphs of accuracy and error for training and validation. show_dicom_sample visutils.how_dicom_sample(dataloader, figsize=(30,10)) Displays a sample image from a DICOM dataloader. Returns a single image in case of one window and 3 images in case of multiple window. Arguments dataloader: (dataloader object) selected pytorch dataloader. figsize: (tuple) size of the displayed figure. (default=30,10) Output Output: (figure) single image in case of one window and 3 images in case of multiple window. show_roc visutils.show_roc(true_labels, predictions, auc=True, figure_size=(10,10), title='ROC Curve') Displays ROC curve and AUC using true and predicted label lists. Arguments true_labels: (list) list of true labels. predictions: (list) list of predicted labels. auc: (boolen) True to display AUC. (default=True) figure_size: (tuple) size of the displayed figure. (default=10,10) title: (str) title displayed on top of the output figure. (default='ROC Curve') Output Output: (figure) show_nn_roc visutils.show_nn_roc(model, target_data_set, auc=True, figure_size=(10,10)) Displays the ROC and AUC of a certain trained model on a target(for example test) dataset. Arguments model: (pytorch model object) target model. target_data_set: (pytorch dataset object) target dataset. auc: (boolen) True to display AUC. (default=True) figure_size: (tuple) size of the displayed figure. (default=10,10) Output Output: (figure) plot_confusion_matrix visutils.plot_confusion_matrix(cm,target_names, title='Confusion Matrix', cmap=None,normalize=False,figure_size=(8,6)) Given a sklearn confusion matrix (cm), make a nice plot. Code adapted from : https://www.kaggle.com/grfiv4/plot-a-confusion-matrix . Arguments cm: (numpy array) confusion matrix from sklearn.metrics.confusion_matrix. target_names: (list) list of class names. title: (str) title displayed on top of the output figure. (default='Confusion Matrix') cmap: (str) The gradient of the values displayed from matplotlib.pyplot.cm . See http://matplotlib.org/examples/color/colormaps_reference.html . (default=None which is plt.get_cmap('jet') or plt.cm.Blues) normalize: (boolean) If False, plot the raw numbers. If True, plot the proportions. (default=False) figure_size: (tuple) size of the displayed figure. (default=8,6) Output Output: (figure) show_confusion_matrix visutils.show_confusion_matrix(model, target_data_set, target_classes, figure_size=(8,6), cmap=None) Displays Confusion Matrix for Image Classifier Model. Arguments model: (pytorch model object) target model. target_data_set: (pytorch dataset object) target dataset. target_classes: (list) list of class names. figure_size: (tuple) size of the displayed figure. (default=8,6) cmap: (str) the colormap of the generated figure (default=None, which is Blues) Output Output: (figure)","title":"Visutils Module"},{"location":"visutils/#visualization-module-radtorchvisutils","text":"Different tools and utilities for data visualization. Based upon Matplotlib. from radtorch import visutils","title":"Visualization Module  radtorch.visutils "},{"location":"visutils/#show_dataloader_sample","text":"visutils.show_dataloader_sample(dataloader, num_of_images_per_row=10, figsize=(10,10), show_labels=True) Displays sample of certain dataloader with corresponding class idx Arguments dataloader: (dataloader object) selected pytorch dataloader. num_of_images_per_row: (int) number of images per row. (default=10) figsize: (tuple) size of displayed figure. (default = (10,10)) show_labels: (boolen) display class idx of the sample displayed .(default=True) Output Output: (figure)","title":"show_dataloader_sample"},{"location":"visutils/#show_dataset_info","text":"visutils.show_dataset_info(dataset) Displays a summary of the pytorch dataset information. Arguments dataset: (pytorch dataset object) target dataset to inspect. Output Output: (str) Dataset information including: Number of instances Number of classes Dictionary of class and class_id Class frequency breakdown.","title":"show_dataset_info"},{"location":"visutils/#show_metrics","text":"visutils.show_metrics(source, fig_size=(15,5)) Displays metrics created by the training loop. Arguments source: (list) the metrics generated during the training process as by modelsutils.train_model() fig_size: (tuple) size of the displayed figure. (default=15,5) Output Output: (figure) Matplotlib graphs of accuracy and error for training and validation.","title":"show_metrics"},{"location":"visutils/#show_dicom_sample","text":"visutils.how_dicom_sample(dataloader, figsize=(30,10)) Displays a sample image from a DICOM dataloader. Returns a single image in case of one window and 3 images in case of multiple window. Arguments dataloader: (dataloader object) selected pytorch dataloader. figsize: (tuple) size of the displayed figure. (default=30,10) Output Output: (figure) single image in case of one window and 3 images in case of multiple window.","title":"show_dicom_sample"},{"location":"visutils/#show_roc","text":"visutils.show_roc(true_labels, predictions, auc=True, figure_size=(10,10), title='ROC Curve') Displays ROC curve and AUC using true and predicted label lists. Arguments true_labels: (list) list of true labels. predictions: (list) list of predicted labels. auc: (boolen) True to display AUC. (default=True) figure_size: (tuple) size of the displayed figure. (default=10,10) title: (str) title displayed on top of the output figure. (default='ROC Curve') Output Output: (figure)","title":"show_roc"},{"location":"visutils/#show_nn_roc","text":"visutils.show_nn_roc(model, target_data_set, auc=True, figure_size=(10,10)) Displays the ROC and AUC of a certain trained model on a target(for example test) dataset. Arguments model: (pytorch model object) target model. target_data_set: (pytorch dataset object) target dataset. auc: (boolen) True to display AUC. (default=True) figure_size: (tuple) size of the displayed figure. (default=10,10) Output Output: (figure)","title":"show_nn_roc"},{"location":"visutils/#plot_confusion_matrix","text":"visutils.plot_confusion_matrix(cm,target_names, title='Confusion Matrix', cmap=None,normalize=False,figure_size=(8,6)) Given a sklearn confusion matrix (cm), make a nice plot. Code adapted from : https://www.kaggle.com/grfiv4/plot-a-confusion-matrix . Arguments cm: (numpy array) confusion matrix from sklearn.metrics.confusion_matrix. target_names: (list) list of class names. title: (str) title displayed on top of the output figure. (default='Confusion Matrix') cmap: (str) The gradient of the values displayed from matplotlib.pyplot.cm . See http://matplotlib.org/examples/color/colormaps_reference.html . (default=None which is plt.get_cmap('jet') or plt.cm.Blues) normalize: (boolean) If False, plot the raw numbers. If True, plot the proportions. (default=False) figure_size: (tuple) size of the displayed figure. (default=8,6) Output Output: (figure)","title":"plot_confusion_matrix"},{"location":"visutils/#show_confusion_matrix","text":"visutils.show_confusion_matrix(model, target_data_set, target_classes, figure_size=(8,6), cmap=None) Displays Confusion Matrix for Image Classifier Model. Arguments model: (pytorch model object) target model. target_data_set: (pytorch dataset object) target dataset. target_classes: (list) list of class names. figure_size: (tuple) size of the displayed figure. (default=8,6) cmap: (str) the colormap of the generated figure (default=None, which is Blues) Output Output: (figure)","title":"show_confusion_matrix"}]}