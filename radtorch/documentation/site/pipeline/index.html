



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../img/radtorch_icon.ico">
      <meta name="generator" content="mkdocs-1.1, mkdocs-material-4.4.0">
    
    
      
        <title>Pipeline Module - RADTorch</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.0284f74d.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-palette.01803549.css">
      
      
        
        
        <meta name="theme-color" content="#3f51b5">
      
    
    
      <script src="../assets/javascripts/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    <svg class="md-svg">
      <defs>
        
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#pipeline-module-radtorchpipeline" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href=".." title="RADTorch" class="md-header-nav__button md-logo">
          
            <i class="md-icon">dashboard</i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              RADTorch
            </span>
            <span class="md-header-nav__topic">
              
                Pipeline Module
              
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href=".." title="RADTorch" class="md-nav__button md-logo">
      
        <i class="md-icon">dashboard</i>
      
    </a>
    RADTorch
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../install/" title="Installation" class="md-nav__link">
      Installation
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Pipeline Module
      </label>
    
    <a href="./" title="Pipeline Module" class="md-nav__link md-nav__link--active">
      Pipeline Module
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#image_classification" title="Image_Classification" class="md-nav__link">
    Image_Classification
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#feature_extraction" title="Feature_Extraction" class="md-nav__link">
    Feature_Extraction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#compare_image_classifier" title="Compare_Image_Classifier" class="md-nav__link">
    Compare_Image_Classifier
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#load_pipeline" title="load_pipeline" class="md-nav__link">
    load_pipeline
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../visutils/" title="Visutils Module" class="md-nav__link">
      Visutils Module
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../datautils/" title="Datautils Module" class="md-nav__link">
      Datautils Module
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../dicomutils/" title="Dicomutils Module" class="md-nav__link">
      Dicomutils Module
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../modelsutils/" title="Modelsutils Module" class="md-nav__link">
      Modelsutils Module
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../version/" title="Releases/Versions" class="md-nav__link">
      Releases/Versions
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../copyright/" title="Copyrights" class="md-nav__link">
      Copyrights
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../license/" title="License" class="md-nav__link">
      License
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#image_classification" title="Image_Classification" class="md-nav__link">
    Image_Classification
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#feature_extraction" title="Feature_Extraction" class="md-nav__link">
    Feature_Extraction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#compare_image_classifier" title="Compare_Image_Classifier" class="md-nav__link">
    Compare_Image_Classifier
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#load_pipeline" title="load_pipeline" class="md-nav__link">
    load_pipeline
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="pipeline-module-radtorchpipeline">Pipeline Module <small> radtorch.pipeline </small></h1>
<p><p style='text-align: justify;'>
Pipelines are probably the most exciting feature of RADTorch tool kit. With few lines of code, the pipeline module allows you to run state-of-the-art image classification algorithms and much more.
</p></p>
<pre><code>from radtorch import pipeline
</code></pre>
<h2 id="image_classification">Image_Classification</h2>
<pre><code>  pipeline.Image_Classification(data_directory, name = None,
  transformations='default',custom_resize = 'default', device='default',
  optimizer='Adam', is_dicom=True, label_from_table=False, is_csv=None,
  table_source=None, path_col = 'IMAGE_PATH', label_col = 'IMAGE_LABEL',
  balance_class = False, predefined_datasets = False, mode='RAW', wl=None,
  normalize='default', batch_size=16, test_percent = 0.2, valid_percent = 0.2,
  model_arch='vgg16', pre_trained=True, unfreeze_weights=False,train_epochs=20,
  learning_rate=0.0001, loss_function='CrossEntropyLoss')
</code></pre>
<div class="admonition abstract">
<p class="admonition-title">Description</p>
<p>The Image Classification pipeline simplifies the process of binary and multi-class image classification into a single line of code.
Under the hood, the following happens:</p>
<ol>
<li>
<p>The pipeline creates a master dataset from the provided data directory and source of labels/classes either from <a href="https://pytorch.org/docs/stable/torchvision/datasets.html#datasetfolder">folder structre</a> or pandas/csv table.</p>
</li>
<li>
<p>Master dataset is subdivided into train, valid and test subsets using the percentages defined by user.</p>
</li>
<li>
<p>The following transformations are applied on the dataset images:</p>
<ol>
<li>Resize to the default image size allowed by the model architecture.</li>
<li>Window/Level adjustment according to values specified by user.</li>
<li>Single channel grayscale DICOM images are converted into 3 channel grayscale images to fit into the model.</li>
</ol>
</li>
<li>
<p>Selected Model architecture, optimizer, and loss function are downloaded/created.</p>
</li>
<li>
<p>Model is trained.</p>
</li>
<li>
<p>Training metrics are saved as training progresses and can be displayed after training is done.</p>
</li>
<li>
<p>Confusion Matrix and ROC (for binary classification) can be displayed as well (by default, the test subset is used to calculate the confusion matrix and the ROC)</p>
</li>
<li>
<p>Misclassifed samples can be displayed.</p>
</li>
<li>
<p>Trained model can be exported to outside file for future use.</p>
</li>
</ol>
</div>
<!-- ####Parameters -->

<div class="admonition info">
<p class="admonition-title">Parameters</p>
<p><strong>data_directory:</strong></p>
<ul>
<li><em>(str)</em> target data directory. <strong>(Required)</strong></li>
</ul>
<p><strong>name:</strong></p>
<ul>
<li><em>(str)</em> preferred name to be given to classifier.</li>
</ul>
<p><strong>is_dicom:</strong></p>
<ul>
<li><em>(boolean)</em> True for DICOM images. (default=True)</li>
</ul>
<p><strong>label_from_table:</strong></p>
<ul>
<li><em>(boolean)</em> True if labels are to extracted from table, False if labels are to be extracted from subfolders names. (default=False)</li>
</ul>
<p><strong>is_csv:</strong></p>
<ul>
<li><em>(boolean)</em> True for csv, False for pandas dataframe.</li>
</ul>
<p><strong>table_source:</strong></p>
<ul>
<li><em>(str or pandas dataframe object)</em> source for labelling data.This is path to csv file or name of pandas dataframe if pandas to be used. (default=None).</li>
</ul>
<p><strong>predefined_datasets</strong></p>
<ul>
<li><em>(dict)</em> dictionary of predefined pandas dataframes for training. This follows the following scheme: {'train': dataframe, 'valid': dataframe, 'test':dataframe }</li>
</ul>
<p><strong>path_col:</strong></p>
<ul>
<li><em>(str)</em>  name of the column with the image path. (default='IMAGE_PATH')</li>
</ul>
<p><strong>label_col:</strong></p>
<ul>
<li><em>(str)</em>  name of the label/class column. (default='IMAGE_LABEL')</li>
</ul>
<p><strong>mode:</strong></p>
<ul>
<li><em>(str)</em>  output mode for DICOM images only where RAW= Raw pixels, HU= Image converted to Hounsefield Units, WIN= 'window' image windowed to certain W and L, MWIN = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together. (default='RAW')</li>
</ul>
<p><strong>wl:</strong></p>
<ul>
<li><em>(list)</em> list of lists of combinations of window level and widths to be used with WIN and MWIN.In the form of : [[Level,Width], [Level,Width],...].  </li>
<li>Only 3 combinations are allowed for MWIN (for now). (default=None)</li>
</ul>
<p><strong>balance_class:</strong></p>
<ul>
<li><em>(boolen)</em> balance classes in train/valid/test subsets. Under the hood, oversampling is done for the classes with fewer number of instances. (default=False)</li>
</ul>
<p><strong>normalize:</strong></p>
<ul>
<li><em>(str)</em> Normalization algorithm applied to data. Options: 'default' normalizes data with mean of 0.5 and standard deviation of 0.5, 'auto' normalizes the data using mean and standard deviation calculated from the datasets, 'False' applies no normalization. (default='default')</li>
</ul>
<p><strong>transformations:</strong></p>
<ul>
<li><em>(pytorch transforms list)</em> pytroch transforms to be performed on the dataset. (default=Convert to tensor)</li>
</ul>
<p><strong>custom_resize:</strong>
- <em>(int)</em> by default, a radtorch pipeline will resize the input images into the default training model input image size as demonstrated in the table shown below. This default size can be changed here if needed.</p>
<p><strong>batch_size:</strong></p>
<ul>
<li><em>(int)</em> batch size of the dataset (default=16)</li>
</ul>
<p><strong>test_percent:</strong></p>
<ul>
<li><em>(float)</em> percentage of dataset to use for testing. Float value between 0 and 1.0. (default=0.2)</li>
</ul>
<p><strong>valid_percent:</strong></p>
<ul>
<li><em>(float)</em> percentage of dataset to use for validation. Float value between 0 and 1.0. (default=0.2)</li>
</ul>
<p><strong>model_arch:</strong></p>
<ul>
<li><em>(str)</em> PyTorch neural network architecture (default='vgg16')</li>
</ul>
<p><strong>pre_trained:</strong></p>
<ul>
<li><em>(boolean)</em> Load the pretrained weights of the neural network. (default=True)</li>
</ul>
<p><strong>unfreeze_weights:</strong></p>
<ul>
<li><em>(boolean)</em> if True, all model weights will be retrained. This note that if no pre_trained weights are applied, this option will be set to True automatically. (default=False)</li>
</ul>
<p><strong>train_epochs:</strong></p>
<ul>
<li><em>(int)</em> Number of training epochs. (default=20)</li>
</ul>
<p><strong>learning_rate:</strong></p>
<ul>
<li><em>(str)</em> training learning rate. (default = 0.0001)</li>
</ul>
<p><strong>loss_function:</strong></p>
<ul>
<li><em>(str)</em> training loss function. (default='CrossEntropyLoss')</li>
</ul>
<p><strong>optimizer:</strong></p>
<ul>
<li><em>(str)</em> Optimizer to be used during training. (default='Adam')</li>
</ul>
<p><strong>device:</strong></p>
<ul>
<li><em>(str)</em> device to be used for training. This can be adjusted to 'cpu' or 'cuda'. If nothing is selected, the pipeline automatically detects if CUDA is available and trains on it.</li>
</ul>
</div>
<!-- ####Methods -->

<div class="admonition info">
<p class="admonition-title">Methods</p>
<p><strong>.info()</strong></p>
<ul>
<li>Display table with properties of the Image Classification Pipeline.</li>
</ul>
<p><strong>.dataset_info(plot=True, plot_size=(500,300))</strong></p>
<ul>
<li>
<p>Display Dataset Information.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>plot: <em>(boolean)</em> displays dataset information in graphical representation. (default=True)</li>
<li>plot_size: <em>(tuple)</em> figures size.</li>
</ul>
</li>
</ul>
<p><strong>.sample(fig_size=(10,10), show_labels=True, show_file_name=False)</strong></p>
<ul>
<li>
<p>Display sample of the training dataset.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>fig_size: <em>(tuple)</em> figure size. (default=(10,10))</li>
<li>show_labels: <em>(boolean)</em> show the image label idx. (default=True)</li>
<li>show_file_name: <em>(boolean)</em> show the file name as label. (default=False)</li>
</ul>
</li>
</ul>
<p><strong>.run(verbose=True)</strong></p>
<ul>
<li>
<p>Start the image classification pipeline training.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>verbose: <em>(boolean)</em> Show display progress after each epoch. (default=True)</li>
</ul>
</li>
</ul>
<p><strong>.metrics(fig_size=(500,300))</strong></p>
<ul>
<li>Display the training metrics.</li>
</ul>
<p><strong>.export_model(output_path)</strong></p>
<ul>
<li>
<p>Export the trained model into a target file.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>output_path: <em>(str)</em> path to output file. For example 'foler/folder/model.pth'</li>
</ul>
</li>
</ul>
<p><strong>.export(output_path)</strong></p>
<ul>
<li>
<p>Exports the whole image classification pipeline for future use</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>target_path: <em>(str)</em> target location for export.</li>
</ul>
</li>
</ul>
<p><strong>.set_trained_model(model_path, mode)</strong></p>
<ul>
<li>
<p>Loads a previously trained model into pipeline</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>model_path: <em>(str)</em> path to target model</li>
<li>mode: <em>(str)</em> either 'train' or 'infer'.'train' will load the model to be trained. 'infer' will load the model for inference.</li>
</ul>
</li>
</ul>
<p><strong>.inference(test_img_path, transformations='default',  all_predictions=False)</strong></p>
<ul>
<li>
<p>Performs inference using the trained model on a target image.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>test_img_path: <em>(str)</em> path to target image.</li>
<li>transformations: <em>(pytorch transforms list)</em> list of transforms to be performed on the target image. (default='default' which is the same transforms using for training the pipeline)</li>
<li>all_predictions: <em>(boolean)</em>  if True , shows table of all prediction classes and accuracy percentages. (default=False)</li>
</ul>
</li>
</ul>
<p><strong>.roc(target_data_set='default', figure_size=(600,400))</strong></p>
<ul>
<li>
<p>Display ROC and AUC.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>target_data_set: <em>(pytorch dataset object)</em> dataset used for predictions to create the ROC. By default, the image classification pipeline uses the test dataset created to calculate the ROC. If no test dataset was created in the pipeline (e.g. test_percent=0), then an external test dataset is required. (default=default')</li>
<li>figure_size: <em>(tuple)</em> figure size. (default=(600,400))</li>
</ul>
</li>
</ul>
<p><strong>.confusion_matrix(target_data_set='default', target_classes='default', figure_size=(7,7), cmap=None)</strong></p>
<ul>
<li>
<p>Display Confusion Matrix</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>target_data_set: <em>(pytorch dataset object)</em> dataset used for predictions to create the confusion matrix. By default, the image classification - pipeline uses the test dataset created to calculate the matrix.</li>
<li>target_classes: <em>(list)</em> list of classes. By default, the image classification pipeline uses the training classes.</li>
<li>figure_size: <em>(tuple)</em> figure size. (default=(7,7))</li>
<li>cmap: <em>(str)</em> user specific matplotlib cmap.</li>
</ul>
</li>
</ul>
<p><strong>.misclassfied(target_data_set='default', num_of_images=16, figure_size=(10,10), show_table=False)</strong></p>
<ul>
<li>
<p>Displays sample of misclassfied images from confusion matrix or ROC.</p>
</li>
<li>
<p>Arguments:</p>
</li>
<li>target_data_set: <em>(pytorch dataset object)</em> dataset used for predictions. By default, the image classification pipeline uses the test dataset. If no test dataset was created in the pipeline (e.g. test_percent=0), then an external test dataset is required. (default=default')</li>
<li>num_of_images: <em>(int)</em> number of images to be displayed.</li>
<li>figure_size: <em>(tuple)</em> figure size (default=(10,10))</li>
<li>show_table: <em>(boolean)</em> display table of misclassied images. (default=False)</li>
</ul>
</div>
<!-- ####Examples -->

<div class="admonition success">
<p class="admonition-title">Example</p>
<p>Full example for Image Classification Pipeline can be found <a href="https://colab.research.google.com/drive/1O7op_RtuNs12uIs0QVbwoeZdtbyQ4Q9i#scrollTo=njIH9PnCLhHp">HERE</a></p>
</div>
<hr>

<h2 id="feature_extraction">Feature_Extraction</h2>
<pre><code>pipeline.Feature_Extraction(data_directory, transformations='default',
custom_resize = 'default', is_dicom=True,label_from_table=False,
is_csv=None,table_source=None, device='default', path_col = 'IMAGE_PATH',
label_col = 'IMAGE_LABEL', mode='RAW', wl=None, model_arch='vgg16',
pre_trained=True, unfreeze_weights=False, shuffle=True)
</code></pre>
<div class="admonition abstract">
<p class="admonition-title">Description</p>
<p>The feature extraction pipeline utilizes a pre-trained model to extract a set of features that can be used in another machine learning algorithms e.g. Adaboost or KNN.</p>
<p>The trained model by default can one of the supported model architectures trained with default weights trained on the ImageNet dataset. (The ability to use a model that has been trained and exported using the image classification pipeline will be added later.)</p>
<p>The output is a pandas dataframe that has feature columns, label column and file path column.</p>
<p>Under the hood, the pipeline removes the last FC layer of the pretrained models to output the features.</p>
<p>The number of extracted features depends on the model architecture selected:</p>
</div>
<div align='center'>

<table>
<thead>
<tr>
<th>Model Architecture</th>
<th align="center">Default Input Image Size</th>
<th align="center">Output Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>vgg11</td>
<td align="center">224 x 224</td>
<td align="center">4096</td>
</tr>
<tr>
<td>vgg13</td>
<td align="center">224 x 224</td>
<td align="center">4096</td>
</tr>
<tr>
<td>vgg16</td>
<td align="center">224 x 224</td>
<td align="center">4096</td>
</tr>
<tr>
<td>vgg19</td>
<td align="center">224 x 224</td>
<td align="center">4096</td>
</tr>
<tr>
<td>vgg11_bn</td>
<td align="center">224 x 224</td>
<td align="center">4096</td>
</tr>
<tr>
<td>vgg13_bn</td>
<td align="center">224 x 224</td>
<td align="center">4096</td>
</tr>
<tr>
<td>vgg16_bn</td>
<td align="center">224 x 224</td>
<td align="center">4096</td>
</tr>
<tr>
<td>vgg19_bn</td>
<td align="center">224 x 224</td>
<td align="center">4096</td>
</tr>
<tr>
<td>resnet18</td>
<td align="center">224 x 224</td>
<td align="center">512</td>
</tr>
<tr>
<td>resnet34</td>
<td align="center">224 x 224</td>
<td align="center">512</td>
</tr>
<tr>
<td>resnet50</td>
<td align="center">224 x 224</td>
<td align="center">2048</td>
</tr>
<tr>
<td>resnet101</td>
<td align="center">224 x 224</td>
<td align="center">2048</td>
</tr>
<tr>
<td>resnet152</td>
<td align="center">224 x 224</td>
<td align="center">2048</td>
</tr>
<tr>
<td>wide_resnet50_2</td>
<td align="center">224 x 224</td>
<td align="center">2048</td>
</tr>
<tr>
<td>wide_resnet101_2</td>
<td align="center">224 x 224</td>
<td align="center">2048</td>
</tr>
<tr>
<td>alexnet</td>
<td align="center">256 x 256</td>
<td align="center">4096</td>
</tr>
</tbody>
</table>
</div>

<!-- ####Parameters -->

<div class="admonition info">
<p class="admonition-title">Parameters</p>
<p><strong>data_directory:</strong></p>
<ul>
<li><em>(str)</em> target data directory. <strong>(Required)</strong></li>
</ul>
<p><strong>is_dicom:</strong></p>
<ul>
<li><em>(boolean)</em>  True for DICOM images, False for regular images.(default=True)</li>
</ul>
<p><strong>label_from_table:</strong></p>
<ul>
<li><em>(boolean)</em> True if labels are to extracted from table, False if labels are to be extracted from subfolders. (default=False)</li>
</ul>
<p><strong>is_csv:</strong></p>
<ul>
<li><em>(boolean)</em>  True for csv, False for pandas dataframe.</li>
</ul>
<p><strong>table_source:</strong></p>
<ul>
<li><em>(str or pandas dataframe object)</em> source for labelling data. (default=None). This is path to csv file or name of pandas dataframe if pandas to be used.</li>
</ul>
<p><strong>path_col:</strong></p>
<ul>
<li><em>(str)</em> name of the column with the image path. (default='IMAGE_PATH')</li>
</ul>
<p><strong>label_col:</strong></p>
<ul>
<li><em>(str)</em> name of the label/class column. (default='IMAGE_LABEL')</li>
</ul>
<p><strong>shuffle</strong>
- <em>(boolean)</em> shuffles items in dataset.(default=True)</p>
<p><strong>mode:</strong></p>
<ul>
<li><em>(str)</em> output mode for DICOM images only.</li>
<li>Options:
               RAW= Raw pixels,
               HU= Image converted to Hounsefield Units,
               WIN= 'window' image windowed to certain W and L,
               MWIN = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together]. (default='RAW')</li>
</ul>
<p><strong>wl:</strong></p>
<ul>
<li><em>(list)</em> list of lists of combinations of window level and widths to be used with WIN and MWIN.
          In the form of : [[Level,Width], [Level,Width],...].
          Only 3 combinations are allowed for MWIN (for now).(default=None)</li>
</ul>
<p><strong>transformations:</strong></p>
<ul>
<li><em>(pytorch transforms)</em> pytroch transforms to be performed on the dataset. (default=Convert to tensor)</li>
</ul>
<p><strong>custom_resize:</strong></p>
<ul>
<li><em>(int)</em> by default, a radtorch pipeline will resize the input images into the default training model input image
size as demosntrated in the table shown in radtorch home page. This default size can be changed here if needed.
model_arch: [str] PyTorch neural network architecture (default='vgg16')</li>
</ul>
<p><strong>pre_trained:</strong></p>
<ul>
<li><em>(boolean)</em>  Load the pretrained weights of the neural network. If False, the last layer is only retrained = Transfer Learning. (default=True)</li>
</ul>
<p><strong>device:</strong></p>
<ul>
<li><em>(str)</em> device to be used for training. This can be adjusted to 'cpu' or 'cuda'. If nothing is selected, the pipeline automatically detects if cuda is available and trains on it.</li>
</ul>
</div>
<!-- ####Methods -->

<div class="admonition info">
<p class="admonition-title">Methods</p>
<p><strong>.info()</strong></p>
<ul>
<li>Display Pandas Dataframe with properties of the Pipeline.</li>
</ul>
<p><strong>.dataset_info(plot=True)</strong></p>
<ul>
<li>
<p>Display Dataset Information.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>plot: <em>(boolean)</em> displays dataset information in graphical representation. (default=True)</li>
</ul>
</li>
</ul>
<p><strong>.sample(fig_size=(10,10), show_labels=True, show_file_name=False)</strong></p>
<ul>
<li>
<p>Display sample of the training dataset.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>fig_size: <em>(tuple)</em> figure size. (default=(10,10))</li>
<li>show_labels: <em>(boolean)</em> show the image label idx. (default=True)</li>
<li>show_file_name: <em>(boolean)</em> show the image name as label. (default=False)</li>
</ul>
</li>
</ul>
<p><strong>.num_features()</strong></p>
<ul>
<li>Displays number of features to be extracted.</li>
</ul>
<p><strong>.run(verbose=True)</strong></p>
<ul>
<li>
<p>Extracts features from dataset.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>verbose: <em>(boolean)</em> Show the feature table. (default=True)</li>
</ul>
</li>
</ul>
<p><strong>.export_features(csv_path)</strong></p>
<ul>
<li>
<p>Exports the features to csv.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>csv_path: <em>(str)</em> Path to output csv file.</li>
</ul>
</li>
</ul>
<p><strong>.export(target_path)</strong></p>
<ul>
<li>
<p>Exports the whole image classification pipeline for future use</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>target_path: <em>(str)</em> target location for export.</li>
</ul>
</li>
</ul>
<p><strong>.plot_extracted_features(feature_table=None, feature_names=None, num_features=100, num_images=100,image_path_col='img_path', image_label_col='label_idx')</strong></p>
<ul>
<li>
<p>Plots a graphical representation of extracted features.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>num_features: <em>(int)</em> number of features to be displayed (default=100)</li>
<li>num_images: <em>(int)</em> number of images to display features for (default=100)</li>
<li>image_path_col: <em>(str)</em> name of column containing image paths in the feature table. (default='img_path')</li>
<li>image_label_col: <em>(str)</em> name of column containing image label idx in the feature table. (default='label_idx')</li>
</ul>
</li>
</ul>
</div>
<div class="admonition success">
<p class="admonition-title">Examples</p>
<p>Full example for Feature Extraction Pipeline can be found <a href="https://colab.research.google.com/drive/1O7op_RtuNs12uIs0QVbwoeZdtbyQ4Q9i#scrollTo=iTAp7Zz6CrJ3">HERE</a></p>
<hr>

</div>
<h2 id="compare_image_classifier">Compare_Image_Classifier</h2>
<pre><code>  pipeline.Compare_Image_Classifier(data_directory,transformations='default',
  custom_resize = 'default', device='default', optimizer='Adam', is_dicom=True,
  label_from_table=False, is_csv=None, table_source=None, path_col = 'IMAGE_PATH',
  label_col = 'IMAGE_LABEL', balance_class =[False], multi_label = False,
  mode='RAW', wl=None,  normalize=['default'], batch_size=[8],
  test_percent = [0.2], valid_percent = [0.2], model_arch=['vgg16'],
  pre_trained=[True], unfreeze_weights=False, train_epochs=[10],
  learning_rate=[0.0001],loss_function='CrossEntropyLoss')
</code></pre>
<div class="admonition abstract">
<p class="admonition-title">Description</p>
<p>The Compare Image Classifier class performs analysis and comparison of different image classification pipelines. This is particularly useful when comparing different model architectures and/or different training parameters.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Important</p>
<p>Please note that this pipeline performs training from scratch on the selected model architectures. The ability to compared outside trained models will be added in a future release.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Supported Parameters</p>
<p>The currently supported parameters that can be compared include:</p>
<ol>
<li>balance_class</li>
<li>normalize</li>
<li>batch_size</li>
<li>test_percent</li>
<li>valid_percent</li>
<li>train_epochs</li>
<li>learning_rate</li>
<li>model_arch</li>
<li>self.pre_trained</li>
</ol>
</div>
<div class="admonition warning">
<p class="admonition-title">Use of supported parameters </p>
<p>Please note that the supported parameters are supplied as <strong>List</strong> e.g. model_arch=['renet50'] or train_epochs=[10,20].</p>
</div>
<div class="admonition info">
<p class="admonition-title">Parameters</p>
<p>This pipeline follows the same parameters used for the image classification as above. <strong>Please take note of the warning on how to use the supported parameters above.</strong></p>
</div>
<div class="admonition info">
<p class="admonition-title">Methods</p>
<p><strong>.info()</strong></p>
<ul>
<li>Display Pandas Dataframe with properties of the Pipeline.</li>
</ul>
<p><strong>.grid()</strong></p>
<ul>
<li>Display table with all generated image classifier objects that will be used for comparison.</li>
</ul>
<p><strong>.parameters()</strong></p>
<ul>
<li>Displays a list of supported comparison parameters.</li>
</ul>
<p><strong>.dataset_info(plot=True)</strong></p>
<ul>
<li>
<p>Display Dataset Information.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>plot: <em>(boolean)</em> displays dataset information in graphical representation. (default=True)</li>
</ul>
</li>
</ul>
<p><strong>.sample(fig_size=(10,10), show_labels=True, show_file_name=False)</strong></p>
<ul>
<li>
<p>Display sample of the training dataset.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>fig_size: <em>(tuple)</em> figure size. (default=(10,10))</li>
<li>show_labels: <em>(boolean)</em> show the image label idx. (default=True)</li>
<li>show_file_name: <em>(boolean)</em> show the image name as label. (default=False)</li>
</ul>
</li>
</ul>
<p><strong>.run(verbose=True)</strong></p>
<ul>
<li>Runs the pipeline.</li>
</ul>
<p><strong>.metrics(fig_size=(650,400)))</strong></p>
<ul>
<li>Display the training metrics.</li>
</ul>
<p><strong>.roc(fig_size=(700,400))</strong></p>
<ul>
<li>Displays comparison between ROC curves of different classifiers with AUC.</li>
</ul>
<p><strong>.best(path=None, export_classifier=False, export_model=False))</strong></p>
<ul>
<li>
<p>Displays the best classifier based on AUC.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>path: <em>(str)</em> exporting path.</li>
<li>export_classifier: <em>(boolen)</em> export the best classifier.</li>
<li>export_model: <em>(boolen)</em> export the best model.</li>
</ul>
</li>
</ul>
</div>
<div class="admonition success">
<p class="admonition-title">Examples</p>
<p>Full example for Compare_Image_Classifier can be found <a href="https://colab.research.google.com/drive/1O7op_RtuNs12uIs0QVbwoeZdtbyQ4Q9i#scrollTo=HNBKoWg_WyUW&amp;line=1&amp;uniqifier=1">HERE</a></p>
</div>
<hr>

<h2 id="load_pipeline">load_pipeline</h2>
<pre><code>  pipeline.load_pipeline(target_path)
</code></pre>
<div class="admonition abstract">
<p class="admonition-title">Description</p>
<p>Loads a previously saved pipeline for future use.</p>
<p><strong>Arguments</strong></p>
<ul>
<li>target_path: <em>(str)</em> target path of the target pipeline.</li>
</ul>
</div>
<div class="admonition success">
<p class="admonition-title">Examples<p>my_classifier = load_pipeline('/path/to/pipeline.dump')</p>
</p>
</div>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../install/" title="Installation" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Installation
              </span>
            </div>
          </a>
        
        
          <a href="../visutils/" title="Visutils Module" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Visutils Module
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.245445c6.js"></script>
      
      <script>app.initialize({version:"1.1",url:{base:".."}})</script>
      
    
  </body>
</html>