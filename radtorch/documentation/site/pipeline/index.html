



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../img/radtorch_icon.ico">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.4.0">
    
    
      
        <title>Pipeline Module - RADTorch</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.0284f74d.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-palette.01803549.css">
      
      
        
        
        <meta name="theme-color" content="#3f51b5">
      
    
    
      <script src="../assets/javascripts/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    <svg class="md-svg">
      <defs>
        
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#pipeline-module-radtorchpipeline" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href=".." title="RADTorch" class="md-header-nav__button md-logo">
          
            <i class="md-icon">dashboard</i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              RADTorch
            </span>
            <span class="md-header-nav__topic">
              
                Pipeline Module
              
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href=".." title="RADTorch" class="md-nav__button md-logo">
      
        <i class="md-icon">dashboard</i>
      
    </a>
    RADTorch
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../install/" title="Installation" class="md-nav__link">
      Installation
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Pipeline Module
      </label>
    
    <a href="./" title="Pipeline Module" class="md-nav__link md-nav__link--active">
      Pipeline Module
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#image_classification" title="Image_Classification" class="md-nav__link">
    Image_Classification
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters" title="Parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" title="Methods" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#examples" title="Examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#feature_extraction" title="Feature_Extraction" class="md-nav__link">
    Feature_Extraction
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_1" title="Parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_1" title="Methods" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#examples_1" title="Examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#load_pipeline" title="load_pipeline" class="md-nav__link">
    load_pipeline
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../visutils/" title="Visutils Module" class="md-nav__link">
      Visutils Module
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../datautils/" title="Datautils Module" class="md-nav__link">
      Datautils Module
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../dicomutils/" title="Dicomutils Module" class="md-nav__link">
      Dicomutils Module
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../modelsutils/" title="Modelsutils Module" class="md-nav__link">
      Modelsutils Module
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../version/" title="Version log" class="md-nav__link">
      Version log
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../copyright/" title="Copyrights" class="md-nav__link">
      Copyrights
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../license/" title="License" class="md-nav__link">
      License
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#image_classification" title="Image_Classification" class="md-nav__link">
    Image_Classification
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters" title="Parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" title="Methods" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#examples" title="Examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#feature_extraction" title="Feature_Extraction" class="md-nav__link">
    Feature_Extraction
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_1" title="Parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_1" title="Methods" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#examples_1" title="Examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#load_pipeline" title="load_pipeline" class="md-nav__link">
    load_pipeline
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="pipeline-module-radtorchpipeline">Pipeline Module <small> radtorch.pipeline </small></h1>
<p><p style='text-align: justify;'>
Pipelines are probably the most exciting feature of RADTorch tool kit. With few lines of code, the pipeline module allows you to run state-of-the-art image classification algorithms and much more.
</p></p>
<h2 id="image_classification">Image_Classification</h2>
<pre><code>  pipeline.Image_Classification(data_directory, transformations='default',
  custom_resize='default', device='default', optimizer='Adam', is_dicom=True,
  label_from_table=False, is_csv=None, table_source=None, path_col='IMAGE_PATH',
  label_col='IMAGE_LABEL', mode='RAW', wl=None, batch_size=16, test_percent=0.2,
  valid_percent=0.2, model_arch='vgg16', pre_trained=True, unfreeze_weights=True,
  train_epochs=20, learning_rate=0.0001, loss_function='CrossEntropyLoss')
</code></pre>
<div class="admonition quote">
<p>The Image Classification pipeline simplifies the process of binary and multi-class image classification into a single line of code.
Under the hood, the following happens:</p>
<ol>
<li>
<p>The pipeline creates a master dataset from the provided data directory and source of labels/classes either from <a href="https://pytorch.org/docs/stable/torchvision/datasets.html#datasetfolder">folder structre</a> or pandas/csv table.</p>
</li>
<li>
<p>Master dataset is subdivided into train, valid and test subsets using the percentages defined by user.</p>
</li>
<li>
<p>The following transformations are applied on the dataset images:</p>
<ol>
<li>Resize to the default image size allowed by the model architecture.</li>
<li>Window/Level adjustment according to values specified by user.</li>
<li>Single channel grayscale DICOM images are converted into 3 channel grayscale images to fit into the model.</li>
</ol>
</li>
<li>
<p>Selected Model architecture, optimizer, and loss function are downloaded/created.</p>
</li>
<li>
<p>Model is trained.</p>
</li>
<li>
<p>Training metrics are saved as training progresses and can be displayed after training is done.</p>
</li>
<li>
<p>Confusion Matrix and ROC (for binary classification) can be displayed as well (by default, the test subset is used to calculate the confusion matrix and the ROC)</p>
</li>
<li>
<p>Trained model can be exported to outside file for future use.</p>
</li>
</ol>
</div>
<h4 id="parameters">Parameters</h4>
<div class="admonition quote">
<p><strong>data_directory:</strong></p>
<ul>
<li><em>(str)</em> target data directory. <strong><em>(Required)</em></strong></li>
</ul>
<p><strong>is_dicom:</strong></p>
<ul>
<li><em>(boolean)</em> True for DICOM images, False for regular images.(default=True)</li>
</ul>
<p><strong>label_from_table:</strong></p>
<ul>
<li><em>(boolean)</em> True if labels are to extracted from table, False if labels are to be extracted from subfolders. (default=False)</li>
</ul>
<p><strong>is_csv:</strong></p>
<ul>
<li><em>(boolean)</em> True for csv, False for pandas dataframe.</li>
</ul>
<p><strong>table_source:</strong></p>
<ul>
<li><em>(str or pandas dataframe object)</em> source for labelling data.This is path to csv file or name of pandas dataframe if pandas to be used. (default=None).</li>
</ul>
<p><strong>path_col:</strong></p>
<ul>
<li><em>(str)</em>  name of the column with the image path. (default='IMAGE_PATH')</li>
</ul>
<p><strong>label_col:</strong></p>
<ul>
<li><em>(str)</em>  name of the label/class column. (default='IMAGE_LABEL')</li>
</ul>
<p><strong>mode:</strong></p>
<ul>
<li><em>(str)</em>  output mode for DICOM images only where RAW= Raw pixels, HU= Image converted to Hounsefield Units, WIN= 'window' image windowed to certain W and L, MWIN = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together. (default='RAW')</li>
</ul>
<p><strong>wl:</strong></p>
<ul>
<li><em>(list)</em> list of lists of combinations of window level and widths to be used with WIN and MWIN.In the form of : [[Level,Width], [Level,Width],...].  </li>
<li>Only 3 combinations are allowed for MWIN (for now). (default=None)</li>
</ul>
<p><strong>transformations:</strong></p>
<ul>
<li><em>(pytorch transforms list)</em> pytroch transforms to be performed on the dataset. (default=Convert to tensor)</li>
</ul>
<p><strong>custom_resize:</strong>
- <em>(int)</em> by default, a radtorch pipeline will resize the input images into the default training model input image size as demosntrated in the table shown in radtorch home page. This default size can be changed here if needed.</p>
<p><strong>batch_size:</strong></p>
<ul>
<li><em>(int)</em> batch size of the dataset (default=16)</li>
</ul>
<p><strong>test_percent:</strong></p>
<ul>
<li><em>(float)</em> percentage of dataset to use for testing. Float value between 0 and 1.0. (default=0.2)</li>
</ul>
<p><strong>valid_percent:</strong></p>
<ul>
<li><em>(float)</em> percentage of dataset to use for validation. Float value between 0 and 1.0. (default=0.2)</li>
</ul>
<p><strong>model_arch:</strong></p>
<ul>
<li><em>(str)</em> PyTorch neural network architecture (default='vgg16')</li>
</ul>
<p><strong>pre_trained:</strong></p>
<ul>
<li><em>(boolean)</em> Load the pretrained weights of the neural network. (default=True)</li>
</ul>
<p><strong>unfreeze_weights:</strong></p>
<ul>
<li><em>(boolean)</em> if True, all model weights will be retrained. (default=True)</li>
</ul>
<p><strong>train_epochs:</strong></p>
<ul>
<li><em>(int)</em> Number of training epochs. (default=20)</li>
</ul>
<p><strong>learning_rate:</strong></p>
<ul>
<li><em>(str)</em> training learning rate. (default = 0.0001)</li>
</ul>
<p><strong>loss_function:</strong></p>
<ul>
<li><em>(str)</em> training loss function. (default='CrossEntropyLoss')</li>
</ul>
<p><strong>optimizer:</strong></p>
<ul>
<li><em>(str)</em> Optimizer to be used during training. (default='Adam')</li>
</ul>
<p><strong>device:</strong></p>
<ul>
<li><em>(str)</em> device to be used for training. This can be adjusted to 'cpu' or 'cuda'. If nothing is selected, the pipeline automatically detects if cuda is available and trains on it.</li>
</ul>
</div>
<h4 id="methods">Methods</h4>
<div class="admonition quote">
<p><strong>.info()</strong></p>
<ul>
<li>Display Parameters of the Image Classification Pipeline.</li>
</ul>
<p><strong>.dataset_info()</strong></p>
<ul>
<li>Display Dataset Information.</li>
</ul>
<p><strong>.sample()</strong></p>
<ul>
<li>
<p>Display sample of the training dataset.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>num_of_images_per_row: <em>(int)</em> number of images per column. (default=5)</li>
<li>fig_size: <em>(tuple)</em> figure size. (default=(10,10))</li>
<li>show_labels: <em>(boolean)</em> show the image label idx. (default=True)</li>
</ul>
</li>
</ul>
<p><strong>.train()</strong></p>
<ul>
<li>
<p>Train the image classification pipeline.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>verbose: <em>(boolean)</em> Show display progress after each epoch. (default=True)</li>
</ul>
</li>
</ul>
<p><strong>.metrics()</strong></p>
<ul>
<li>Display the training metrics.</li>
</ul>
<p><strong>.export_model()</strong></p>
<ul>
<li>
<p>Export the trained model into a target file.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>output_path: <em>(str)</em> path to output file. For example 'foler/folder/model.pth'</li>
</ul>
</li>
</ul>
<p><strong>.export()</strong></p>
<ul>
<li>
<p>Exports the whole image classification pipeline for future use</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>target_path: <em>(str)</em> target location for export.</li>
</ul>
</li>
</ul>
<p><strong>.set_trained_model()</strong></p>
<ul>
<li>
<p>Loads a previously trained model into pipeline</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>model_path: <em>(str)</em> path to target model</li>
<li>mode: <em>(str)</em> either 'train' or 'infer'.'train' will load the model to be trained. 'infer' will load the model for inference.</li>
</ul>
</li>
</ul>
<p><strong>.inference()</strong></p>
<ul>
<li>
<p>Performs inference using the trained model on a target image.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>test_img_path: <em>(str)</em> path to target image.</li>
<li>transformations: <em>(pytorch transforms list)</em> list of transforms to be performed on the target image. (default='default' which is the same transforms using for training the pipeline)</li>
</ul>
</li>
<li>
<p>Outputs:</p>
<ul>
<li>Output: <em>(tuple)</em> tuple of prediction (class idx , accuracy percentage).</li>
</ul>
</li>
</ul>
<p><strong>.roc()</strong></p>
<ul>
<li>
<p>Display ROC and AUC.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>target_data_set: <em>(pytorch dataset object)</em> dataset used for predictions to create the ROC. By default, the image classification pipeline uses the test dataset created to calculate the ROC. If no test dataset was created in the pipeline (e.g. test_percent=0), then an external test dataset is required. (default=default')</li>
<li>auc: <em>(boolen)</em> Display area under curve. (default=True)</li>
<li>figure_size: <em>(tuple)</em> figure size. (default=(7,7))</li>
</ul>
</li>
</ul>
<p><strong>.confusion_matrix()</strong></p>
<ul>
<li>
<p>Display Confusion Matrix</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>target_data_set: <em>(pytorch dataset object)</em> dataset used for predictions to create the confusion matrix. By default, the image classification - pipeline uses the test dataset created to calculate the matrix.</li>
<li>target_classes: <em>(list)</em> list of classes. By default, the image classification pipeline uses the training classes.</li>
<li>figure_size: <em>(tuple)</em> figure size. (default=(7,7))</li>
</ul>
</li>
</ul>
</div>
<h4 id="examples">Examples</h4>
<div class="admonition quote">
<p><strong>Importing the pipeline and setting up data directory</strong></p>
<pre><code>from radtorch import pipeline
data_root = '/content/data'
</code></pre>
<p><strong>Create the image classifier pipeline</strong></p>
<p>The below example will create the pipeline using resnet50 model architecture with trained weights loaded and will train for 30 epochs.</p>
<pre><code>clf = pipeline.Image_Classification(data_directory=data_root,
      mode='HU', model_arch='resnet50', train_epochs=30)
</code></pre>
<p><strong>Show dataset Information</strong></p>
<pre><code>clf.dataset_info()
</code></pre>
<!-- **** -->

<pre><code>Number of intances = 77
Number of classes =  2
Class IDX =  {'axr': 0, 'cxr': 1}

Class Frequency:
Class Number of instances
 1       39
 0       38
None
Train Dataset Size  47
Valid Dataset Size  15
Test Dataset Size  15
</code></pre>
<p><strong>Display sample of the dataset</strong></p>
<pre><code>clf.sample()
</code></pre>
<p><img alt="" src="../img/sample.png" /></p>
<p><strong>Train the classifier</strong></p>
<pre><code>clf.train()
</code></pre>
<!-- **** -->

<pre><code>Starting training at 2020-02-27 17:47:54.918173
Epoch : 000/30 : [Training: Loss: 0.7937, Accuracy: 46.8085%]  [Validation : Loss : 0.7436, Accuracy: 40.0000%] [Time: 1.4200s]
Epoch : 001/30 : [Training: Loss: 0.6054, Accuracy: 63.8298%]  [Validation : Loss : 0.7793, Accuracy: 40.0000%] [Time: 1.3436s]
Epoch : 002/30 : [Training: Loss: 0.5504, Accuracy: 80.8511%]  [Validation : Loss : 1.2314, Accuracy: 40.0000%] [Time: 1.3500s]
...
...
...
Epoch : 026/30 : [Training: Loss: 0.0499, Accuracy: 97.8723%]  [Validation : Loss : 0.4143, Accuracy: 93.3333%] [Time: 1.3612s]
Epoch : 027/30 : [Training: Loss: 0.0235, Accuracy: 97.8723%]  [Validation : Loss : 0.0321, Accuracy: 100.000%] [Time: 1.3540s]
Epoch : 028/30 : [Training: Loss: 0.0142, Accuracy: 100.000%]  [Validation : Loss : 0.2476, Accuracy: 93.3333%] [Time: 1.3584s]
Epoch : 029/30 : [Training: Loss: 0.0067, Accuracy: 100.000%]  [Validation : Loss : 0.4216, Accuracy: 93.3333%] [Time: 1.3728s]

Total training time = 0:00:40.758802
</code></pre>
<p><strong>Display training metrics</strong></p>
<pre><code>clf.metrics()
</code></pre>
<p><img alt="" src="../img/metrics.png" /></p>
<p><strong>Display Confusion Matrix</strong></p>
<pre><code>clf.confusion_matrix()
</code></pre>
<p><img alt="" src="../img/cm.png" /></p>
<p><strong>Display ROC</strong></p>
<pre><code>clf.roc()
</code></pre>
<p><img alt="" src="../img/roc.png" /></p>
<p><strong>Export Trained Model</strong></p>
<pre><code>clf.export_model('/folder/model.pth')
</code></pre>
</div>
<hr>

<h2 id="feature_extraction">Feature_Extraction</h2>
<pre><code>pipeline.Feature_Extraction(data_directory, transformations='default',
custom_resize = 'default', is_dicom=True,label_from_table=False,
is_csv=None,table_source=None, device='default', path_col = 'IMAGE_PATH',
label_col = 'IMAGE_LABEL', mode='RAW', wl=None, model_arch='vgg16',
pre_trained=True, unfreeze_weights=False, shuffle=True)
</code></pre>
<div class="admonition quote">
<p>The feature extraction pipeline utilizes a pre-trained model to extract a set of features that can be used in another machine learning algorithms e.g. XGBoost. The trained model by default can one of the supported model architectures trained with default weights trained on the ImageNet dataset or a model that has been trained and exported using the image classification pipeline.</p>
<p>The output is a pandas dataframe that has feature columns, label column and file path column.</p>
<p>Under the hood, the pipeline removes the last FC layer of the pretrained models to output the features.</p>
<p>The number of extracted features depends on the model architecture selected:</p>
<div align='center'>

<table>
<thead>
<tr>
<th>Model Architecture</th>
<th align="center">Default Input Image Size</th>
<th align="center">Output Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>VGG16</td>
<td align="center">244 x 244</td>
<td align="center">4096</td>
</tr>
<tr>
<td>VGG19</td>
<td align="center">244 x 244</td>
<td align="center">4096</td>
</tr>
<tr>
<td>resnet50</td>
<td align="center">244 x 244</td>
<td align="center">2048</td>
</tr>
<tr>
<td>resnet152</td>
<td align="center">244 x 244</td>
<td align="center">2048</td>
</tr>
<tr>
<td>resnet101</td>
<td align="center">244 x 244</td>
<td align="center">2048</td>
</tr>
<tr>
<td>wide_resnet50_2</td>
<td align="center">244 x 244</td>
<td align="center">2048</td>
</tr>
<tr>
<td>wide_resnet101_2</td>
<td align="center">244 x 244</td>
<td align="center">2048</td>
</tr>
<tr>
<td>inception_v3</td>
<td align="center">299 x 299</td>
<td align="center">2048</td>
</tr>
</tbody>
</table>
</div>

</div>
<h4 id="parameters_1">Parameters</h4>
<div class="admonition quote">
<p><strong>data_directory:</strong></p>
<ul>
<li><em>(str)</em> target data directory. <strong><em>(Required)</em></strong></li>
</ul>
<p><strong>is_dicom:</strong></p>
<ul>
<li><em>(boolean)</em>  True for DICOM images, False for regular images.(default=True)</li>
</ul>
<p><strong>label_from_table:</strong> [boolean] True if labels are to extracted from table, False if labels are to be extracted from subfolders. (default=False)</p>
<p><strong>is_csv:</strong></p>
<ul>
<li><em>(boolean)</em>  True for csv, False for pandas dataframe.</li>
</ul>
<p><strong>table_source:</strong></p>
<ul>
<li><em>(str or pandas dataframe object)</em> source for labelling data. (default=None). This is path to csv file or name of pandas dataframe if pandas to be used.</li>
</ul>
<p><strong>path_col:</strong></p>
<ul>
<li><em>(str)</em> name of the column with the image path. (default='IMAGE_PATH')</li>
</ul>
<p><strong>label_col:</strong></p>
<ul>
<li><em>(str)</em> name of the label/class column. (default='IMAGE_LABEL')</li>
</ul>
<p><strong>shuffle</strong>
- <em>(boolean)</em> shuffles items in dataset.(default=True)</p>
<p><strong>mode:</strong></p>
<ul>
<li><em>(str)</em> output mode for DICOM images only.</li>
<li>Options:
               RAW= Raw pixels,
               HU= Image converted to Hounsefield Units,
               WIN= 'window' image windowed to certain W and L,
               MWIN = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together]. (default='RAW')</li>
</ul>
<p><strong>wl:</strong></p>
<ul>
<li><em>(list)</em> list of lists of combinations of window level and widths to be used with WIN and MWIN.
          In the form of : [[Level,Width], [Level,Width],...].
          Only 3 combinations are allowed for MWIN (for now).(default=None)</li>
</ul>
<p><strong>transformations:</strong></p>
<ul>
<li><em>(pytorch transforms)</em> pytroch transforms to be performed on the dataset. (default=Convert to tensor)</li>
</ul>
<p><strong>custom_resize:</strong></p>
<ul>
<li><em>(int)</em> by default, a radtorch pipeline will resize the input images into the default training model input image
size as demosntrated in the table shown in radtorch home page. This default size can be changed here if needed.
model_arch: [str] PyTorch neural network architecture (default='vgg16')</li>
</ul>
<p><strong>pre_trained:</strong></p>
<ul>
<li><em>(boolean)</em>  Load the pretrained weights of the neural network. If False, the last layer is only retrained = Transfer Learning. (default=True)</li>
</ul>
<p><strong>unfreeze_weights:</strong></p>
<ul>
<li><em>(boolean)</em>  if True, all model weights, not just final layer, will be retrained. (default=False)</li>
</ul>
<p><strong>device:</strong></p>
<ul>
<li><em>(str)</em> device to be used for training. This can be adjusted to 'cpu' or 'cuda'. If nothing is selected, the pipeline automatically detects if cuda is available and trains on it.</li>
</ul>
</div>
<h4 id="methods_1">Methods</h4>
<div class="admonition quote">
<p><strong>.info()</strong></p>
<ul>
<li>Displays Feature Extraction Pipeline Parameters.</li>
</ul>
<p><strong>.dataset_info()</strong></p>
<ul>
<li>Display Dataset Information.</li>
</ul>
<p><strong>.sample()</strong></p>
<ul>
<li>Display sample of the training dataset.</li>
</ul>
<p><strong>.num_features()</strong></p>
<ul>
<li>Displays number of features to be extracted.</li>
</ul>
<p><strong>.run()</strong></p>
<ul>
<li>
<p>Extracts features from dataset.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>verbose: <em>(boolean)</em> Show the feature table. (default=True)</li>
</ul>
</li>
</ul>
<p><strong>.export_features()</strong></p>
<ul>
<li>
<p>Exports the features to csv.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>csv_path: <em>(str)</em> Path to output csv file.</li>
</ul>
</li>
</ul>
<p><strong>.export()</strong></p>
<ul>
<li>
<p>Exports the whole image classification pipeline for future use</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>target_path: <em>(str)</em> target location for export.</li>
</ul>
</li>
</ul>
<p><strong>.set_trained_model()</strong> : <strong><em>Temporarily Disabled for future update.</em></strong></p>
</div>
<h4 id="examples_1">Examples</h4>
<div class="admonition quote">
<p><strong>Importing the pipeline and setting up data directory</strong>
<div class="highlight"><pre><span></span>from radtorch import pipeline
data_root = &#39;/content/data&#39;
</pre></div>
<strong>Create the feature extractor pipeline</strong></p>
<p>The below example will create the pipeline using resnet152 model architecture with trained weights loaded.</p>
<div class="highlight"><pre><span></span>extractor = pipeline.Feature_Extraction(data_directory=data_root, mode=&#39;HU&#39;,
            model_arch=&#39;resnet152&#39;)
</pre></div>

<p><strong>Display number of Features to be extracted</strong>
<div class="highlight"><pre><span></span>extractor.num_features()
</pre></div>
<div class="highlight"><pre><span></span>2048
</pre></div></p>
<p><strong>Show Dataset information</strong>
<div class="highlight"><pre><span></span>extractor.dataset_info()
</pre></div>
<div class="highlight"><pre><span></span>Number of intances = 77
Number of classes =  2
Class IDX =  {&#39;axr&#39;: 0, &#39;cxr&#39;: 1}

Class Frequency:
Class Number of instances
0       38
1       39
</pre></div></p>
<p><strong>Display sample of the dataset</strong>
<div class="highlight"><pre><span></span>extractor.sample()
</pre></div>
<img alt="" src="../img/sample.png" /></p>
<p><strong>Run pipeline to extract features</strong>
<div class="highlight"><pre><span></span>extractor.run()
</pre></div>
<div class="highlight"><pre><span></span>  |    | img_path       |  label_idx |       f_0 |      f_1 |         f_2 |      f_3 | ........ |
  |---:|:---------------|-----------:|----------:|---------:|------------:|---------:|---------:|
  |  0 | /content/dat...|          0 | 0.135294  | 0.368051 | 0.000352088 | 0.378677 | ........ |
  |  1 | /content/dat...|          0 | 0.0721618 | 0.930238 | 0.0286931   | 0.732228 | ........ |
  |  2 | /content/dat...|          0 | 0.0780637 | 0.432966 | 0.0175741   | 0.685681 | ........ |
  |  3 | /content/dat...|          0 | 0.560777  | 0.449213 | 0.0432512   | 0.432942 | ........ |
  |  4 | /content/dat...|          0 | 0.176524  | 0.669066 | 0.0396659   | 0.273474 | ........ |
</pre></div></p>
<p><strong>Show feature names list</strong>
<div class="highlight"><pre><span></span>extractor.feature_names
</pre></div>
<div class="highlight"><pre><span></span>[&#39;f_0&#39;,&#39;f_1&#39;,&#39;f_2&#39;,&#39;f_3&#39;,&#39;f_4&#39;, &#39;f_5&#39;, ... ]
</pre></div></p>
</div>
<h2 id="load_pipeline">load_pipeline</h2>
<pre><code>  pipeline.load_pipeline(target_path)
</code></pre>
<div class="admonition quote">
<p>Loads a previously saved pipeline for future use.</p>
<p><strong>Arguments</strong></p>
<ul>
<li>target_path: <em>(str)</em> target path of the target pipeline.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code>my_classifier = load_pipeline('/path/to/pipeline.dump')
</code></pre>
</div>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../install/" title="Installation" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Installation
              </span>
            </div>
          </a>
        
        
          <a href="../visutils/" title="Visutils Module" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Visutils Module
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.245445c6.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
      
    
  </body>
</html>