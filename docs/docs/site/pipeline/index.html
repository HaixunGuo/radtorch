


<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="shortcut icon" href="../img/radtorch_icon.ico">
      <meta name="generator" content="mkdocs-1.1, mkdocs-material-5.1.0">
    
    
      
        <title>radtorch.pipeline - RADTorch - API Documentation</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.89dc9fe3.min.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/palette.ecd4686e.min.css">
      
      
        
        
        <meta name="theme-color" content="">
      
    
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    
      
        
<link rel="preconnect dns-prefetch" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-116382803-2","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})}),document.addEventListener("DOMContentSwitch",function(){ga("send","pageview")})</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
  
    
    
    <body dir="ltr" data-md-color-primary="black" data-md-color-accent="deep-orange">
  
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#pipeline-module-radtorchpipeline" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href=".." title="RADTorch - API Documentation" class="md-header-nav__button md-logo" aria-label="RADTorch - API Documentation">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18,22A2,2 0 0,0 20,20V4C20,2.89 19.1,2 18,2H12V9L9.5,7.5L7,9V2H6A2,2 0 0,0 4,4V20A2,2 0 0,0 6,22H18Z" /></svg>

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3,6H21V8H3V6M3,11H21V13H3V11M3,16H21V18H3V16Z" /></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis">
            RADTorch - API Documentation
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              radtorch.pipeline
            
          </span>
        </div>
      
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5,3A6.5,6.5 0 0,1 16,9.5C16,11.11 15.41,12.59 14.44,13.73L14.71,14H15.5L20.5,19L19,20.5L14,15.5V14.71L13.73,14.44C12.59,15.41 11.11,16 9.5,16A6.5,6.5 0 0,1 3,9.5A6.5,6.5 0 0,1 9.5,3M9.5,5C7,5 5,7 5,9.5C5,12 7,14 9.5,14C12,14 14,12 14,9.5C14,7 12,5 9.5,5Z" /></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active">
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5,3A6.5,6.5 0 0,1 16,9.5C16,11.11 15.41,12.59 14.44,13.73L14.71,14H15.5L20.5,19L19,20.5L14,15.5V14.71L13.73,14.44C12.59,15.41 11.11,16 9.5,16A6.5,6.5 0 0,1 3,9.5A6.5,6.5 0 0,1 9.5,3M9.5,5C7,5 5,7 5,9.5C5,12 7,14 9.5,14C12,14 14,12 14,9.5C14,7 12,5 9.5,5Z" /></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19,6.41L17.59,5L12,10.59L6.41,5L5,6.41L10.59,12L5,17.59L6.41,19L12,13.41L17.59,19L19,17.59L13.41,12L19,6.41Z" /></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header-nav__source">
        
<a href="https://github.com/radtorch/radtorch/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    radtorch/radtorch
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
        
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="RADTorch - API Documentation" class="md-nav__button md-logo" aria-label="RADTorch - API Documentation">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18,22A2,2 0 0,0 20,20V4C20,2.89 19.1,2 18,2H12V9L9.5,7.5L7,9V2H6A2,2 0 0,0 4,4V20A2,2 0 0,0 6,22H18Z" /></svg>

    </a>
    RADTorch - API Documentation
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/radtorch/radtorch/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    radtorch/radtorch
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../start/" title="Getting Started" class="md-nav__link">
      Getting Started
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../install/" title="Installation" class="md-nav__link">
      Installation
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../core/" title="radtorch.core" class="md-nav__link">
      radtorch.core
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        radtorch.pipeline
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3,9H17V7H3V9M3,13H17V11H3V13M3,17H17V15H3V17M19,17H21V15H19V17M19,7V9H21V7H19M19,13H21V11H19V13Z" /></svg>
        </span>
      </label>
    
    <a href="./" title="radtorch.pipeline" class="md-nav__link md-nav__link--active">
      radtorch.pipeline
    </a>
    
      
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#image_classification" class="md-nav__link">
    Image_Classification
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#feature_extraction" class="md-nav__link">
    Feature_Extraction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#compare_image_classifier" class="md-nav__link">
    Compare_Image_Classifier
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#load_pipeline" class="md-nav__link">
    load_pipeline
  </a>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../involve/" title="Get Involved" class="md-nav__link">
      Get Involved
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../version/" title="Releases/Versions" class="md-nav__link">
      Releases/Versions
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../copyright/" title="Copyrights" class="md-nav__link">
      Copyrights
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../license/" title="License" class="md-nav__link">
      License
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#image_classification" class="md-nav__link">
    Image_Classification
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#feature_extraction" class="md-nav__link">
    Feature_Extraction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#compare_image_classifier" class="md-nav__link">
    Compare_Image_Classifier
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#load_pipeline" class="md-nav__link">
    load_pipeline
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  
                
                
                <h1 id="pipeline-module-radtorchpipeline">Pipeline Module <small> radtorch.pipeline </small></h1>
<div class="admonition bug">
<p class="admonition-title">DOCUMENTATION OUT OF DATE</p>
<p>Documentation not updated. Please check again later.</p>
</div>
<p style='text-align: justify;'>
Pipelines are probably the most exciting feature of RADTorch tool kit. With few lines of code, the pipeline module allows you to run state-of-the-art image classification algorithms and much more.
</p>

<pre><code>from radtorch import pipeline
</code></pre>
<p style='text-align: justify;'>
RADTorch follows principles of <b>object-oriented-programming</b> (OOP) in the sense that RADTorch pipelines are made of core building blocks and each of these blocks has specific functions/methods that can be accessed accordingly.
</p>

<p style='text-align: justify;'>

For example,
</p>

<pre><code>pipeline.Image_Classification.data_processor.dataset_info()
</code></pre>
<p style='text-align: justify;'>
can be used to access the dataset information for that particular Image Classification pipeline.
</p>

<h2 id="image_classification">Image_Classification</h2>
<pre><code>  pipeline.Image_Classification(data_directory, name = None,
  transformations='default',custom_resize = 'default', device='default',
  optimizer='Adam', is_dicom=True, label_from_table=False, is_csv=None,
  table_source=None, path_col = 'IMAGE_PATH', label_col = 'IMAGE_LABEL',
  balance_class = False, predefined_datasets = False, mode='RAW', wl=None,
  normalize='default', batch_size=16, test_percent = 0.2, valid_percent = 0.2,
  model_arch='vgg16', pre_trained=True, unfreeze_weights=False,train_epochs=20,
  learning_rate=0.0001, loss_function='CrossEntropyLoss')
</code></pre>
<div class="admonition abstract">
<p class="admonition-title">Description</p>
<p>The Image Classification pipeline simplifies the process of binary and multi-class image classification into a single line of code.
Under the hood, the following happens:</p>
<ol>
<li>
<p>The pipeline creates a master dataset from the provided data directory and source of labels/classes either from <a href="https://pytorch.org/docs/stable/torchvision/datasets.html#datasetfolder">folder structre</a> or pandas/csv table.</p>
</li>
<li>
<p>Master dataset is subdivided into train, valid and test subsets using the percentages defined by user.</p>
</li>
<li>
<p>The following transformations are applied on the dataset images:</p>
<ol>
<li>Resize to the default image size allowed by the model architecture.</li>
<li>Window/Level adjustment according to values specified by user.</li>
<li>Single channel grayscale DICOM images are converted into 3 channel grayscale images to fit into the model.</li>
</ol>
</li>
<li>
<p>Selected Model architecture, optimizer, and loss function are downloaded/created.</p>
</li>
<li>
<p>Model is trained.</p>
</li>
<li>
<p>Training metrics are saved as training progresses and can be displayed after training is done.</p>
</li>
<li>
<p>Confusion Matrix and ROC (for binary classification) can be displayed as well (by default, the test subset is used to calculate the confusion matrix and the ROC)</p>
</li>
<li>
<p>Misclassifed samples can be displayed.</p>
</li>
<li>
<p>Trained model can be exported to outside file for future use.</p>
</li>
</ol>
</div>
<!-- ####Parameters -->

<div class="admonition info">
<p class="admonition-title">Parameters</p>
<p><strong>data_directory:</strong></p>
<ul>
<li><em>(str)</em> target data directory. <strong>(Required)</strong></li>
</ul>
<p><strong>name:</strong></p>
<ul>
<li><em>(str)</em> preferred name to be given to classifier.</li>
</ul>
<p><strong>is_dicom:</strong></p>
<ul>
<li><em>(boolean)</em> True for DICOM images. (default=True)</li>
</ul>
<p><strong>label_from_table:</strong></p>
<ul>
<li><em>(boolean)</em> True if labels are to extracted from table, False if labels are to be extracted from subfolders names. (default=False)</li>
</ul>
<p><strong>is_csv:</strong></p>
<ul>
<li><em>(boolean)</em> True for csv, False for pandas dataframe.</li>
</ul>
<p><strong>table_source:</strong></p>
<ul>
<li><em>(str or pandas dataframe object)</em> source for labelling data.This is path to csv file or name of pandas dataframe if pandas to be used. (default=None).</li>
</ul>
<p><strong>predefined_datasets</strong></p>
<ul>
<li><em>(dict)</em> dictionary of predefined pandas dataframes for training. This follows the following scheme: {'train': dataframe, 'valid': dataframe, 'test':dataframe }</li>
</ul>
<p><strong>path_col:</strong></p>
<ul>
<li><em>(str)</em>  name of the column with the image path. (default='IMAGE_PATH')</li>
</ul>
<p><strong>label_col:</strong></p>
<ul>
<li><em>(str)</em>  name of the label/class column. (default='IMAGE_LABEL')</li>
</ul>
<p><strong>mode:</strong></p>
<ul>
<li><em>(str)</em>  output mode for DICOM images only where RAW= Raw pixels, HU= Image converted to Hounsefield Units, WIN= 'window' image windowed to certain W and L, MWIN = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together. (default='RAW')</li>
</ul>
<p><strong>wl:</strong></p>
<ul>
<li><em>(list)</em> list of lists of combinations of window level and widths to be used with WIN and MWIN.In the form of : [[Level,Width], [Level,Width],...].  </li>
<li>Only 3 combinations are allowed for MWIN (for now). (default=None)</li>
</ul>
<p><strong>balance_class:</strong></p>
<ul>
<li><em>(boolen)</em> balance classes in train/valid/test subsets. Under the hood, oversampling is done for the classes with fewer number of instances. (default=False)</li>
</ul>
<p><strong>normalize:</strong></p>
<ul>
<li><em>(str)</em> Normalization algorithm applied to data. Options: 'default' normalizes data with mean of 0.5 and standard deviation of 0.5, 'auto' normalizes the data using mean and standard deviation calculated from the datasets, 'False' applies no normalization. (default='default')</li>
</ul>
<p><strong>transformations:</strong></p>
<ul>
<li><em>(pytorch transforms list)</em> pytroch transforms to be performed on the dataset. (default=Convert to tensor)</li>
</ul>
<p><strong>custom_resize:</strong>
- <em>(int)</em> by default, a radtorch pipeline will resize the input images into the default training model input image size as demonstrated in the table shown below. This default size can be changed here if needed.</p>
<p><strong>batch_size:</strong></p>
<ul>
<li><em>(int)</em> batch size of the dataset (default=16)</li>
</ul>
<p><strong>test_percent:</strong></p>
<ul>
<li><em>(float)</em> percentage of dataset to use for testing. Float value between 0 and 1.0. (default=0.2)</li>
</ul>
<p><strong>valid_percent:</strong></p>
<ul>
<li><em>(float)</em> percentage of dataset to use for validation. Float value between 0 and 1.0. (default=0.2)</li>
</ul>
<p><strong>model_arch:</strong></p>
<ul>
<li><em>(str)</em> PyTorch neural network architecture (default='vgg16')</li>
</ul>
<p><strong>pre_trained:</strong></p>
<ul>
<li><em>(boolean)</em> Load the pretrained weights of the neural network. (default=True)</li>
</ul>
<p><strong>unfreeze_weights:</strong></p>
<ul>
<li><em>(boolean)</em> if True, all model weights will be retrained. This note that if no pre_trained weights are applied, this option will be set to True automatically. (default=False)</li>
</ul>
<p><strong>train_epochs:</strong></p>
<ul>
<li><em>(int)</em> Number of training epochs. (default=20)</li>
</ul>
<p><strong>learning_rate:</strong></p>
<ul>
<li><em>(str)</em> training learning rate. (default = 0.0001)</li>
</ul>
<p><strong>loss_function:</strong></p>
<ul>
<li><em>(str)</em> training loss function. (default='CrossEntropyLoss')</li>
</ul>
<p><strong>optimizer:</strong></p>
<ul>
<li><em>(str)</em> Optimizer to be used during training. (default='Adam')</li>
</ul>
<p><strong>device:</strong></p>
<ul>
<li><em>(str)</em> device to be used for training. This can be adjusted to 'cpu' or 'cuda'. If nothing is selected, the pipeline automatically detects if CUDA is available and trains on it.</li>
</ul>
</div>
<!-- ####Methods -->

<div class="admonition info">
<p class="admonition-title">Methods</p>
<p><strong>.info()</strong></p>
<ul>
<li>Display table with properties of the Image Classification Pipeline.</li>
</ul>
<p><strong>.dataset_info(plot=True, plot_size=(500,300))</strong></p>
<ul>
<li>
<p>Display Dataset Information.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>plot: <em>(boolean)</em> displays dataset information in graphical representation. (default=True)</li>
<li>plot_size: <em>(tuple)</em> figures size.</li>
</ul>
</li>
</ul>
<p><strong>.sample(fig_size=(10,10), show_labels=True, show_file_name=False)</strong></p>
<ul>
<li>
<p>Display sample of the training dataset.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>fig_size: <em>(tuple)</em> figure size. (default=(10,10))</li>
<li>show_labels: <em>(boolean)</em> show the image label idx. (default=True)</li>
<li>show_file_name: <em>(boolean)</em> show the file name as label. (default=False)</li>
</ul>
</li>
</ul>
<p><strong>.run(verbose=True)</strong></p>
<ul>
<li>
<p>Start the image classification pipeline training.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>verbose: <em>(boolean)</em> Show display progress after each epoch. (default=True)</li>
</ul>
</li>
</ul>
<p><strong>.metrics(fig_size=(500,300))</strong></p>
<ul>
<li>Display the training metrics.</li>
</ul>
<p><strong>.export_model(output_path)</strong></p>
<ul>
<li>
<p>Export the trained model into a target file.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>output_path: <em>(str)</em> path to output file. For example 'foler/folder/model.pth'</li>
</ul>
</li>
</ul>
<p><strong>.export(output_path)</strong></p>
<ul>
<li>
<p>Exports the whole image classification pipeline for future use</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>target_path: <em>(str)</em> target location for export.</li>
</ul>
</li>
</ul>
<p><strong>.set_trained_model(model_path, mode)</strong></p>
<ul>
<li>
<p>Loads a previously trained model into pipeline</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>model_path: <em>(str)</em> path to target model</li>
<li>mode: <em>(str)</em> either 'train' or 'infer'.'train' will load the model to be trained. 'infer' will load the model for inference.</li>
</ul>
</li>
</ul>
<p><strong>.inference(test_img_path, transformations='default',  all_predictions=False)</strong></p>
<ul>
<li>
<p>Performs inference using the trained model on a target image.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>test_img_path: <em>(str)</em> path to target image.</li>
<li>transformations: <em>(pytorch transforms list)</em> list of transforms to be performed on the target image. (default='default' which is the same transforms using for training the pipeline)</li>
<li>all_predictions: <em>(boolean)</em>  if True , shows table of all prediction classes and accuracy percentages. (default=False)</li>
</ul>
</li>
</ul>
<p><strong>.roc(target_data_set='default', figure_size=(600,400))</strong></p>
<ul>
<li>
<p>Display ROC and AUC.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>target_data_set: <em>(pytorch dataset object)</em> dataset used for predictions to create the ROC. By default, the image classification pipeline uses the test dataset created to calculate the ROC. If no test dataset was created in the pipeline (e.g. test_percent=0), then an external test dataset is required. (default=default')</li>
<li>figure_size: <em>(tuple)</em> figure size. (default=(600,400))</li>
</ul>
</li>
</ul>
<p><strong>.confusion_matrix(target_data_set='default', target_classes='default', figure_size=(7,7), cmap=None)</strong></p>
<ul>
<li>
<p>Display Confusion Matrix</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>target_data_set: <em>(pytorch dataset object)</em> dataset used for predictions to create the confusion matrix. By default, the image classification - pipeline uses the test dataset created to calculate the matrix.</li>
<li>target_classes: <em>(list)</em> list of classes. By default, the image classification pipeline uses the training classes.</li>
<li>figure_size: <em>(tuple)</em> figure size. (default=(7,7))</li>
<li>cmap: <em>(str)</em> user specific matplotlib cmap.</li>
</ul>
</li>
</ul>
<p><strong>.misclassfied(target_data_set='default', num_of_images=16, figure_size=(10,10), show_table=False)</strong></p>
<ul>
<li>
<p>Displays sample of misclassfied images from confusion matrix or ROC.</p>
</li>
<li>
<p>Arguments:</p>
</li>
<li>target_data_set: <em>(pytorch dataset object)</em> dataset used for predictions. By default, the image classification pipeline uses the test dataset. If no test dataset was created in the pipeline (e.g. test_percent=0), then an external test dataset is required. (default=default')</li>
<li>num_of_images: <em>(int)</em> number of images to be displayed.</li>
<li>figure_size: <em>(tuple)</em> figure size (default=(10,10))</li>
<li>show_table: <em>(boolean)</em> display table of misclassied images. (default=False)</li>
</ul>
</div>
<!-- ####Examples -->

<div class="admonition success">
<p class="admonition-title">Example</p>
<p>Full example for Image Classification Pipeline can be found <a href="https://colab.research.google.com/drive/1O7op_RtuNs12uIs0QVbwoeZdtbyQ4Q9i#scrollTo=njIH9PnCLhHp">HERE</a></p>
</div>
<hr>

<h2 id="feature_extraction">Feature_Extraction</h2>
<pre><code>pipeline.Feature_Extraction(data_directory, transformations='default',
custom_resize = 'default', is_dicom=True,label_from_table=False,
is_csv=None,table_source=None, device='default', path_col = 'IMAGE_PATH',
label_col = 'IMAGE_LABEL', mode='RAW', wl=None, model_arch='vgg16',
pre_trained=True, unfreeze_weights=False, shuffle=True)
</code></pre>
<div class="admonition abstract">
<p class="admonition-title">Description</p>
<p>The feature extraction pipeline utilizes a pre-trained model to extract a set of features that can be used in another machine learning algorithms e.g. Adaboost or KNN.</p>
<p>The trained model by default can one of the supported model architectures trained with default weights trained on the ImageNet dataset. (The ability to use a model that has been trained and exported using the image classification pipeline will be added later.)</p>
<p>The output is a pandas dataframe that has feature columns, label column and file path column.</p>
<p>Under the hood, the pipeline removes the last FC layer of the pretrained models to output the features.</p>
<p>The number of extracted features depends on the model architecture selected:</p>
</div>
<div align='center'>

<table>
<thead>
<tr>
<th>Model Architecture</th>
<th align="center">Default Input Image Size</th>
<th align="center">Output Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>vgg11</td>
<td align="center">224 x 224</td>
<td align="center">4096</td>
</tr>
<tr>
<td>vgg13</td>
<td align="center">224 x 224</td>
<td align="center">4096</td>
</tr>
<tr>
<td>vgg16</td>
<td align="center">224 x 224</td>
<td align="center">4096</td>
</tr>
<tr>
<td>vgg19</td>
<td align="center">224 x 224</td>
<td align="center">4096</td>
</tr>
<tr>
<td>vgg11_bn</td>
<td align="center">224 x 224</td>
<td align="center">4096</td>
</tr>
<tr>
<td>vgg13_bn</td>
<td align="center">224 x 224</td>
<td align="center">4096</td>
</tr>
<tr>
<td>vgg16_bn</td>
<td align="center">224 x 224</td>
<td align="center">4096</td>
</tr>
<tr>
<td>vgg19_bn</td>
<td align="center">224 x 224</td>
<td align="center">4096</td>
</tr>
<tr>
<td>resnet18</td>
<td align="center">224 x 224</td>
<td align="center">512</td>
</tr>
<tr>
<td>resnet34</td>
<td align="center">224 x 224</td>
<td align="center">512</td>
</tr>
<tr>
<td>resnet50</td>
<td align="center">224 x 224</td>
<td align="center">2048</td>
</tr>
<tr>
<td>resnet101</td>
<td align="center">224 x 224</td>
<td align="center">2048</td>
</tr>
<tr>
<td>resnet152</td>
<td align="center">224 x 224</td>
<td align="center">2048</td>
</tr>
<tr>
<td>wide_resnet50_2</td>
<td align="center">224 x 224</td>
<td align="center">2048</td>
</tr>
<tr>
<td>wide_resnet101_2</td>
<td align="center">224 x 224</td>
<td align="center">2048</td>
</tr>
<tr>
<td>alexnet</td>
<td align="center">256 x 256</td>
<td align="center">4096</td>
</tr>
</tbody>
</table>
</div>

<!-- ####Parameters -->

<div class="admonition info">
<p class="admonition-title">Parameters</p>
<p><strong>data_directory:</strong></p>
<ul>
<li><em>(str)</em> target data directory. <strong>(Required)</strong></li>
</ul>
<p><strong>is_dicom:</strong></p>
<ul>
<li><em>(boolean)</em>  True for DICOM images, False for regular images.(default=True)</li>
</ul>
<p><strong>label_from_table:</strong></p>
<ul>
<li><em>(boolean)</em> True if labels are to extracted from table, False if labels are to be extracted from subfolders. (default=False)</li>
</ul>
<p><strong>is_csv:</strong></p>
<ul>
<li><em>(boolean)</em>  True for csv, False for pandas dataframe.</li>
</ul>
<p><strong>table_source:</strong></p>
<ul>
<li><em>(str or pandas dataframe object)</em> source for labelling data. (default=None). This is path to csv file or name of pandas dataframe if pandas to be used.</li>
</ul>
<p><strong>path_col:</strong></p>
<ul>
<li><em>(str)</em> name of the column with the image path. (default='IMAGE_PATH')</li>
</ul>
<p><strong>label_col:</strong></p>
<ul>
<li><em>(str)</em> name of the label/class column. (default='IMAGE_LABEL')</li>
</ul>
<p><strong>shuffle</strong>
- <em>(boolean)</em> shuffles items in dataset.(default=True)</p>
<p><strong>mode:</strong></p>
<ul>
<li><em>(str)</em> output mode for DICOM images only.</li>
<li>Options:
               RAW= Raw pixels,
               HU= Image converted to Hounsefield Units,
               WIN= 'window' image windowed to certain W and L,
               MWIN = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together]. (default='RAW')</li>
</ul>
<p><strong>wl:</strong></p>
<ul>
<li><em>(list)</em> list of lists of combinations of window level and widths to be used with WIN and MWIN.
          In the form of : [[Level,Width], [Level,Width],...].
          Only 3 combinations are allowed for MWIN (for now).(default=None)</li>
</ul>
<p><strong>transformations:</strong></p>
<ul>
<li><em>(pytorch transforms)</em> pytroch transforms to be performed on the dataset. (default=Convert to tensor)</li>
</ul>
<p><strong>custom_resize:</strong></p>
<ul>
<li><em>(int)</em> by default, a radtorch pipeline will resize the input images into the default training model input image
size as demosntrated in the table shown in radtorch home page. This default size can be changed here if needed.
model_arch: [str] PyTorch neural network architecture (default='vgg16')</li>
</ul>
<p><strong>pre_trained:</strong></p>
<ul>
<li><em>(boolean)</em>  Load the pretrained weights of the neural network. If False, the last layer is only retrained = Transfer Learning. (default=True)</li>
</ul>
<p><strong>device:</strong></p>
<ul>
<li><em>(str)</em> device to be used for training. This can be adjusted to 'cpu' or 'cuda'. If nothing is selected, the pipeline automatically detects if cuda is available and trains on it.</li>
</ul>
</div>
<!-- ####Methods -->

<div class="admonition info">
<p class="admonition-title">Methods</p>
<p><strong>.info()</strong></p>
<ul>
<li>Display Pandas Dataframe with properties of the Pipeline.</li>
</ul>
<p><strong>.dataset_info(plot=True)</strong></p>
<ul>
<li>
<p>Display Dataset Information.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>plot: <em>(boolean)</em> displays dataset information in graphical representation. (default=True)</li>
</ul>
</li>
</ul>
<p><strong>.sample(fig_size=(10,10), show_labels=True, show_file_name=False)</strong></p>
<ul>
<li>
<p>Display sample of the training dataset.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>fig_size: <em>(tuple)</em> figure size. (default=(10,10))</li>
<li>show_labels: <em>(boolean)</em> show the image label idx. (default=True)</li>
<li>show_file_name: <em>(boolean)</em> show the image name as label. (default=False)</li>
</ul>
</li>
</ul>
<p><strong>.num_features()</strong></p>
<ul>
<li>Displays number of features to be extracted.</li>
</ul>
<p><strong>.run(verbose=True)</strong></p>
<ul>
<li>
<p>Extracts features from dataset.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>verbose: <em>(boolean)</em> Show the feature table. (default=True)</li>
</ul>
</li>
</ul>
<p><strong>.export_features(csv_path)</strong></p>
<ul>
<li>
<p>Exports the features to csv.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>csv_path: <em>(str)</em> Path to output csv file.</li>
</ul>
</li>
</ul>
<p><strong>.export(target_path)</strong></p>
<ul>
<li>
<p>Exports the whole image classification pipeline for future use</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>target_path: <em>(str)</em> target location for export.</li>
</ul>
</li>
</ul>
<p><strong>.plot_extracted_features(feature_table=None, feature_names=None, num_features=100, num_images=100,image_path_col='img_path', image_label_col='label_idx')</strong></p>
<ul>
<li>
<p>Plots a graphical representation of extracted features.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>num_features: <em>(int)</em> number of features to be displayed (default=100)</li>
<li>num_images: <em>(int)</em> number of images to display features for (default=100)</li>
<li>image_path_col: <em>(str)</em> name of column containing image paths in the feature table. (default='img_path')</li>
<li>image_label_col: <em>(str)</em> name of column containing image label idx in the feature table. (default='label_idx')</li>
</ul>
</li>
</ul>
</div>
<div class="admonition success">
<p class="admonition-title">Examples</p>
<p>Full example for Feature Extraction Pipeline can be found <a href="https://colab.research.google.com/drive/1O7op_RtuNs12uIs0QVbwoeZdtbyQ4Q9i#scrollTo=iTAp7Zz6CrJ3">HERE</a></p>
<hr>

</div>
<h2 id="compare_image_classifier">Compare_Image_Classifier</h2>
<pre><code>  pipeline.Compare_Image_Classifier(data_directory,transformations='default',
  custom_resize = 'default', device='default', optimizer='Adam', is_dicom=True,
  label_from_table=False, is_csv=None, table_source=None, path_col = 'IMAGE_PATH',
  label_col = 'IMAGE_LABEL', balance_class =[False], multi_label = False,
  mode='RAW', wl=None,  normalize=['default'], batch_size=[8],
  test_percent = [0.2], valid_percent = [0.2], model_arch=['vgg16'],
  pre_trained=[True], unfreeze_weights=False, train_epochs=[10],
  learning_rate=[0.0001],loss_function='CrossEntropyLoss')
</code></pre>
<div class="admonition abstract">
<p class="admonition-title">Description</p>
<p>The Compare Image Classifier class performs analysis and comparison of different image classification pipelines. This is particularly useful when comparing different model architectures and/or different training parameters.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Important</p>
<p>Please note that this pipeline performs training from scratch on the selected model architectures. The ability to compared outside trained models will be added in a future release.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Supported Parameters</p>
<p>The currently supported parameters that can be compared include:</p>
<ol>
<li>balance_class</li>
<li>normalize</li>
<li>batch_size</li>
<li>test_percent</li>
<li>valid_percent</li>
<li>train_epochs</li>
<li>learning_rate</li>
<li>model_arch</li>
<li>self.pre_trained</li>
</ol>
</div>
<div class="admonition warning">
<p class="admonition-title">Use of supported parameters </p>
<p>Please note that the supported parameters are supplied as <strong>List</strong> e.g. model_arch=['renet50'] or train_epochs=[10,20].</p>
</div>
<div class="admonition info">
<p class="admonition-title">Parameters</p>
<p>This pipeline follows the same parameters used for the image classification as above. <strong>Please take note of the warning on how to use the supported parameters above.</strong></p>
</div>
<div class="admonition info">
<p class="admonition-title">Methods</p>
<p><strong>.info()</strong></p>
<ul>
<li>Display Pandas Dataframe with properties of the Pipeline.</li>
</ul>
<p><strong>.grid()</strong></p>
<ul>
<li>Display table with all generated image classifier objects that will be used for comparison.</li>
</ul>
<p><strong>.parameters()</strong></p>
<ul>
<li>Displays a list of supported comparison parameters.</li>
</ul>
<p><strong>.dataset_info(plot=True)</strong></p>
<ul>
<li>
<p>Display Dataset Information.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>plot: <em>(boolean)</em> displays dataset information in graphical representation. (default=True)</li>
</ul>
</li>
</ul>
<p><strong>.sample(fig_size=(10,10), show_labels=True, show_file_name=False)</strong></p>
<ul>
<li>
<p>Display sample of the training dataset.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>fig_size: <em>(tuple)</em> figure size. (default=(10,10))</li>
<li>show_labels: <em>(boolean)</em> show the image label idx. (default=True)</li>
<li>show_file_name: <em>(boolean)</em> show the image name as label. (default=False)</li>
</ul>
</li>
</ul>
<p><strong>.run(verbose=True)</strong></p>
<ul>
<li>Runs the pipeline.</li>
</ul>
<p><strong>.metrics(fig_size=(650,400)))</strong></p>
<ul>
<li>Display the training metrics.</li>
</ul>
<p><strong>.roc(fig_size=(700,400))</strong></p>
<ul>
<li>Displays comparison between ROC curves of different classifiers with AUC.</li>
</ul>
<p><strong>.best(path=None, export_classifier=False, export_model=False))</strong></p>
<ul>
<li>
<p>Displays the best classifier based on AUC.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>path: <em>(str)</em> exporting path.</li>
<li>export_classifier: <em>(boolen)</em> export the best classifier.</li>
<li>export_model: <em>(boolen)</em> export the best model.</li>
</ul>
</li>
</ul>
</div>
<div class="admonition success">
<p class="admonition-title">Examples</p>
<p>Full example for Compare_Image_Classifier can be found <a href="https://colab.research.google.com/drive/1O7op_RtuNs12uIs0QVbwoeZdtbyQ4Q9i#scrollTo=HNBKoWg_WyUW&amp;line=1&amp;uniqifier=1">HERE</a></p>
</div>
<hr>

<h2 id="load_pipeline">load_pipeline</h2>
<pre><code>  pipeline.load_pipeline(target_path)
</code></pre>
<div class="admonition abstract">
<p class="admonition-title">Description</p>
<p>Loads a previously saved pipeline for future use.</p>
<p><strong>Arguments</strong></p>
<ul>
<li>target_path: <em>(str)</em> target path of the target pipeline.</li>
</ul>
</div>
<div class="admonition success">
<p class="admonition-title">Examples<p>my_classifier = load_pipeline('/path/to/pipeline.dump')</p>
</p>
</div>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid" aria-label="Footer">
        
          <a href="../core/" title="radtorch.core" class="md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                radtorch.core
              </div>
            </div>
          </a>
        
        
          <a href="../involve/" title="Get Involved" class="md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Get Involved
              </div>
            </div>
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4,11V13H16L10.5,18.5L11.92,19.92L19.84,12L11.92,4.08L10.5,5.5L16,11H4Z" /></svg>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/vendor.36cbf620.min.js"></script>
      <script src="../assets/javascripts/bundle.00c583dd.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents"}</script>
      
      <script>
        app = initialize({
          base: "..",
          features: ["instant"],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.7f7c8775.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
    
  </body>
</html>