


<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="shortcut icon" href="../img/radtorch_icon.ico">
      <meta name="generator" content="mkdocs-1.1, mkdocs-material-5.1.0">
    
    
      
        <title>Image classification - RADTorch - API Documentation</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.89dc9fe3.min.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/palette.ecd4686e.min.css">
      
      
        
        
        <meta name="theme-color" content="">
      
    
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    
      
        
<link rel="preconnect dns-prefetch" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-116382803-2","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})}),document.addEventListener("DOMContentSwitch",function(){ga("send","pageview")})</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
  
    
    
    <body dir="ltr" data-md-color-primary="black" data-md-color-accent="deep-orange">
  
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href=".." title="RADTorch - API Documentation" class="md-header-nav__button md-logo" aria-label="RADTorch - API Documentation">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18,22A2,2 0 0,0 20,20V4C20,2.89 19.1,2 18,2H12V9L9.5,7.5L7,9V2H6A2,2 0 0,0 4,4V20A2,2 0 0,0 6,22H18Z" /></svg>

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3,6H21V8H3V6M3,11H21V13H3V11M3,16H21V18H3V16Z" /></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis">
            RADTorch - API Documentation
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              Image classification
            
          </span>
        </div>
      
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5,3A6.5,6.5 0 0,1 16,9.5C16,11.11 15.41,12.59 14.44,13.73L14.71,14H15.5L20.5,19L19,20.5L14,15.5V14.71L13.73,14.44C12.59,15.41 11.11,16 9.5,16A6.5,6.5 0 0,1 3,9.5A6.5,6.5 0 0,1 9.5,3M9.5,5C7,5 5,7 5,9.5C5,12 7,14 9.5,14C12,14 14,12 14,9.5C14,7 12,5 9.5,5Z" /></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active">
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5,3A6.5,6.5 0 0,1 16,9.5C16,11.11 15.41,12.59 14.44,13.73L14.71,14H15.5L20.5,19L19,20.5L14,15.5V14.71L13.73,14.44C12.59,15.41 11.11,16 9.5,16A6.5,6.5 0 0,1 3,9.5A6.5,6.5 0 0,1 9.5,3M9.5,5C7,5 5,7 5,9.5C5,12 7,14 9.5,14C12,14 14,12 14,9.5C14,7 12,5 9.5,5Z" /></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19,6.41L17.59,5L12,10.59L6.41,5L5,6.41L10.59,12L5,17.59L6.41,19L12,13.41L17.59,19L19,17.59L13.41,12L19,6.41Z" /></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header-nav__source">
        
<a href="https://github.com/radtorch/radtorch/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    radtorch/radtorch
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
        
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="RADTorch - API Documentation" class="md-nav__button md-logo" aria-label="RADTorch - API Documentation">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18,22A2,2 0 0,0 20,20V4C20,2.89 19.1,2 18,2H12V9L9.5,7.5L7,9V2H6A2,2 0 0,0 4,4V20A2,2 0 0,0 6,22H18Z" /></svg>

    </a>
    RADTorch - API Documentation
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/radtorch/radtorch/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    radtorch/radtorch
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../start/" title="Getting Started" class="md-nav__link">
      Getting Started
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../install/" title="Installation" class="md-nav__link">
      Installation
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../core/" title="radtorch.core" class="md-nav__link">
      radtorch.core
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../pipeline/" title="radtorch.pipeline" class="md-nav__link">
      radtorch.pipeline
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../involve/" title="Get Involved" class="md-nav__link">
      Get Involved
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../version/" title="Releases/Versions" class="md-nav__link">
      Releases/Versions
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../copyright/" title="Copyrights" class="md-nav__link">
      Copyrights
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../license/" title="License" class="md-nav__link">
      License
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  
                
                
                  <h1>Image classification</h1>
                
                <div class="highlight"><pre><span></span><code>pipeline.Image_Classification(
    data_directory, name=None, table=None, is_dicom=True, custom_resize=False,
    test_percent=0.2, valid_percent=0.2, balance_class=False, batch_size=16,
    normalize=((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), num_workers=1,
    model_arch=&#39;alexnet&#39;, pre_trained=True, unfreeze=False,
    type=&#39;ridge&#39;, cv=True, stratified=True, num_splits=5, parameters={})
</code></pre></div>

<p>The Image Classification pipeline simplifies the process of binary and multi-class image classification into a single line of code.
Under the hood, the following happens:</p>
<ol>
<li>
<p>The pipeline creates a master dataset from the provided data directory and source of labels/classes either from <a href="https://pytorch.org/docs/stable/torchvision/datasets.html#datasetfolder">folder structure</a> or pandas/csv table.</p>
</li>
<li>
<p>Master dataset is subdivided into train, valid and test subsets using the percentages defined by user.</p>
</li>
<li>
<p>The following transformations are applied on the dataset images:</p>
<ol>
<li>Resize to the default image size allowed by the model architecture.</li>
<li>Window/Level adjustment according to values specified by user.</li>
<li>Single channel grayscale DICOM images are converted into 3 channel grayscale images to fit into the model.</li>
</ol>
</li>
<li>
<p>Selected Model architecture, optimizer, and loss function are downloaded/created.</p>
</li>
<li>
<p>Model is trained.</p>
</li>
<li>
<p>Training metrics are saved as training progresses and can be displayed after training is done.</p>
</li>
<li>
<p>Confusion Matrix and ROC (for binary classification) can be displayed as well (by default, the test subset is used to calculate the confusion matrix and the ROC)</p>
</li>
<li>
<p>Misclassifed samples can be displayed.</p>
</li>
<li>
<p>Trained model can be exported to outside file for future use.</p>
</li>
</ol>
<!-- ####Parameters -->

<details class="quote"><summary>Parameters</summary><p><strong>data_directory:</strong></p>
<ul>
<li><em>(str)</em> target data directory. <strong>(Required)</strong></li>
</ul>
<p><strong>name:</strong></p>
<ul>
<li><em>(str)</em> preferred name to be given to classifier.</li>
</ul>
<p><strong>is_dicom:</strong></p>
<ul>
<li><em>(boolean)</em> True for DICOM images. (default=True)</li>
</ul>
<p><strong>label_from_table:</strong></p>
<ul>
<li><em>(boolean)</em> True if labels are to extracted from table, False if labels are to be extracted from subfolders names. (default=False)</li>
</ul>
<p><strong>is_csv:</strong></p>
<ul>
<li><em>(boolean)</em> True for csv, False for pandas dataframe.</li>
</ul>
<p><strong>table_source:</strong></p>
<ul>
<li><em>(str or pandas dataframe object)</em> source for labelling data.This is path to csv file or name of pandas dataframe if pandas to be used. (default=None).</li>
</ul>
<p><strong>predefined_datasets</strong></p>
<ul>
<li><em>(dict)</em> dictionary of predefined pandas dataframes for training. This follows the following scheme: {'train': dataframe, 'valid': dataframe, 'test':dataframe }</li>
</ul>
<p><strong>path_col:</strong></p>
<ul>
<li><em>(str)</em>  name of the column with the image path. (default='IMAGE_PATH')</li>
</ul>
<p><strong>label_col:</strong></p>
<ul>
<li><em>(str)</em>  name of the label/class column. (default='IMAGE_LABEL')</li>
</ul>
<p><strong>mode:</strong></p>
<ul>
<li><em>(str)</em>  output mode for DICOM images only where RAW= Raw pixels, HU= Image converted to Hounsefield Units, WIN= 'window' image windowed to certain W and L, MWIN = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together. (default='RAW')</li>
</ul>
<p><strong>wl:</strong></p>
<ul>
<li><em>(list)</em> list of lists of combinations of window level and widths to be used with WIN and MWIN.In the form of : [[Level,Width], [Level,Width],...].  </li>
<li>Only 3 combinations are allowed for MWIN (for now). (default=None)</li>
</ul>
<p><strong>balance_class:</strong></p>
<ul>
<li><em>(boolen)</em> balance classes in train/valid/test subsets. Under the hood, oversampling is done for the classes with fewer number of instances. (default=False)</li>
</ul>
<p><strong>normalize:</strong></p>
<ul>
<li><em>(str)</em> Normalization algorithm applied to data. Options: 'default' normalizes data with mean of 0.5 and standard deviation of 0.5, 'auto' normalizes the data using mean and standard deviation calculated from the datasets, 'False' applies no normalization. (default='default')</li>
</ul>
<p><strong>transformations:</strong></p>
<ul>
<li><em>(pytorch transforms list)</em> pytroch transforms to be performed on the dataset. (default=Convert to tensor)</li>
</ul>
<p><strong>custom_resize:</strong>
- <em>(int)</em> by default, a radtorch pipeline will resize the input images into the default training model input image size as demonstrated in the table shown below. This default size can be changed here if needed.</p>
<p><strong>batch_size:</strong></p>
<ul>
<li><em>(int)</em> batch size of the dataset (default=16)</li>
</ul>
<p><strong>test_percent:</strong></p>
<ul>
<li><em>(float)</em> percentage of dataset to use for testing. Float value between 0 and 1.0. (default=0.2)</li>
</ul>
<p><strong>valid_percent:</strong></p>
<ul>
<li><em>(float)</em> percentage of dataset to use for validation. Float value between 0 and 1.0. (default=0.2)</li>
</ul>
<p><strong>model_arch:</strong></p>
<ul>
<li><em>(str)</em> PyTorch neural network architecture (default='vgg16')</li>
</ul>
<p><strong>pre_trained:</strong></p>
<ul>
<li><em>(boolean)</em> Load the pretrained weights of the neural network. (default=True)</li>
</ul>
<p><strong>unfreeze_weights:</strong></p>
<ul>
<li><em>(boolean)</em> if True, all model weights will be retrained. This note that if no pre_trained weights are applied, this option will be set to True automatically. (default=False)</li>
</ul>
<p><strong>train_epochs:</strong></p>
<ul>
<li><em>(int)</em> Number of training epochs. (default=20)</li>
</ul>
<p><strong>learning_rate:</strong></p>
<ul>
<li><em>(str)</em> training learning rate. (default = 0.0001)</li>
</ul>
<p><strong>loss_function:</strong></p>
<ul>
<li><em>(str)</em> training loss function. (default='CrossEntropyLoss')</li>
</ul>
<p><strong>optimizer:</strong></p>
<ul>
<li><em>(str)</em> Optimizer to be used during training. (default='Adam')</li>
</ul>
<p><strong>device:</strong></p>
<ul>
<li><em>(str)</em> device to be used for training. This can be adjusted to 'cpu' or 'cuda'. If nothing is selected, the pipeline automatically detects if CUDA is available and trains on it.</li>
</ul>
</details>
<!-- ####Methods -->

<details class="quote"><summary>Methods</summary><p><strong>.info()</strong></p>
<ul>
<li>Display table with properties of the Image Classification Pipeline.</li>
</ul>
<p><strong>.dataset_info(plot=True, plot_size=(500,300))</strong></p>
<ul>
<li>
<p>Display Dataset Information.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>plot: <em>(boolean)</em> displays dataset information in graphical representation. (default=True)</li>
<li>plot_size: <em>(tuple)</em> figures size.</li>
</ul>
</li>
</ul>
<p><strong>.sample(fig_size=(10,10), show_labels=True, show_file_name=False)</strong></p>
<ul>
<li>
<p>Display sample of the training dataset.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>fig_size: <em>(tuple)</em> figure size. (default=(10,10))</li>
<li>show_labels: <em>(boolean)</em> show the image label idx. (default=True)</li>
<li>show_file_name: <em>(boolean)</em> show the file name as label. (default=False)</li>
</ul>
</li>
</ul>
<p><strong>.run(verbose=True)</strong></p>
<ul>
<li>
<p>Start the image classification pipeline training.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>verbose: <em>(boolean)</em> Show display progress after each epoch. (default=True)</li>
</ul>
</li>
</ul>
<p><strong>.metrics(fig_size=(500,300))</strong></p>
<ul>
<li>Display the training metrics.</li>
</ul>
<p><strong>.export_model(output_path)</strong></p>
<ul>
<li>
<p>Export the trained model into a target file.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>output_path: <em>(str)</em> path to output file. For example 'foler/folder/model.pth'</li>
</ul>
</li>
</ul>
<p><strong>.export(output_path)</strong></p>
<ul>
<li>
<p>Exports the whole image classification pipeline for future use</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>target_path: <em>(str)</em> target location for export.</li>
</ul>
</li>
</ul>
<p><strong>.set_trained_model(model_path, mode)</strong></p>
<ul>
<li>
<p>Loads a previously trained model into pipeline</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>model_path: <em>(str)</em> path to target model</li>
<li>mode: <em>(str)</em> either 'train' or 'infer'.'train' will load the model to be trained. 'infer' will load the model for inference.</li>
</ul>
</li>
</ul>
<p><strong>.inference(test_img_path, transformations='default',  all_predictions=False)</strong></p>
<ul>
<li>
<p>Performs inference using the trained model on a target image.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>test_img_path: <em>(str)</em> path to target image.</li>
<li>transformations: <em>(pytorch transforms list)</em> list of transforms to be performed on the target image. (default='default' which is the same transforms using for training the pipeline)</li>
<li>all_predictions: <em>(boolean)</em>  if True , shows table of all prediction classes and accuracy percentages. (default=False)</li>
</ul>
</li>
</ul>
<p><strong>.roc(target_data_set='default', figure_size=(600,400))</strong></p>
<ul>
<li>
<p>Display ROC and AUC.</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>target_data_set: <em>(pytorch dataset object)</em> dataset used for predictions to create the ROC. By default, the image classification pipeline uses the test dataset created to calculate the ROC. If no test dataset was created in the pipeline (e.g. test_percent=0), then an external test dataset is required. (default=default')</li>
<li>figure_size: <em>(tuple)</em> figure size. (default=(600,400))</li>
</ul>
</li>
</ul>
<p><strong>.confusion_matrix(target_data_set='default', target_classes='default', figure_size=(7,7), cmap=None)</strong></p>
<ul>
<li>
<p>Display Confusion Matrix</p>
</li>
<li>
<p>Arguments:</p>
<ul>
<li>target_data_set: <em>(pytorch dataset object)</em> dataset used for predictions to create the confusion matrix. By default, the image classification - pipeline uses the test dataset created to calculate the matrix.</li>
<li>target_classes: <em>(list)</em> list of classes. By default, the image classification pipeline uses the training classes.</li>
<li>figure_size: <em>(tuple)</em> figure size. (default=(7,7))</li>
<li>cmap: <em>(str)</em> user specific matplotlib cmap.</li>
</ul>
</li>
</ul>
<p><strong>.misclassfied(target_data_set='default', num_of_images=16, figure_size=(10,10), show_table=False)</strong></p>
<ul>
<li>
<p>Displays sample of misclassfied images from confusion matrix or ROC.</p>
</li>
<li>
<p>Arguments:</p>
</li>
<li>target_data_set: <em>(pytorch dataset object)</em> dataset used for predictions. By default, the image classification pipeline uses the test dataset. If no test dataset was created in the pipeline (e.g. test_percent=0), then an external test dataset is required. (default=default')</li>
<li>num_of_images: <em>(int)</em> number of images to be displayed.</li>
<li>figure_size: <em>(tuple)</em> figure size (default=(10,10))</li>
<li>show_table: <em>(boolean)</em> display table of misclassied images. (default=False)</li>
</ul>
</details>
<!-- ####Examples -->

<details class="quote"><summary>Example</summary><p>Full example for Image Classification Pipeline can be found <a href="https://colab.research.google.com/drive/1O7op_RtuNs12uIs0QVbwoeZdtbyQ4Q9i#scrollTo=njIH9PnCLhHp">HERE</a></p>
</details>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/vendor.36cbf620.min.js"></script>
      <script src="../assets/javascripts/bundle.00c583dd.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents"}</script>
      
      <script>
        app = initialize({
          base: "..",
          features: ["instant"],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.7f7c8775.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
    
  </body>
</html>