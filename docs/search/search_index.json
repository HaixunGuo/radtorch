{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-116382803-2'); RADTorch The Radiology Machine Learning Tool Kit About RADTorch provides a package of higher level functions and classes that significantly decrease the amount of time needed for implementation of different machine and deep learning algorithms on DICOM medical images. RADTorch was developed and is currently maintained by Mohamed Elbanan, MD: a Radiology Resident at Yale New Haven Health System, Clinical Research Affiliate at Yale School of Medicine and a Machine-learning enthusiast. Getting Started Running a state-of-the-art DICOM image classifier can be run using the Image Classification Pipeline using the commands: from radtorch import pipeline classifier = pipeline.Image_Classification(data_directory='path to data') classifier.run() The above 3 lines of code will run an image classifier using VGG16 with pre-trained weights. Playground RADTorch playground for testing is provided on Google Colab . Feature Requests Feature requests are more than welcomed on our discussion board HERE Contributing RadTorch is on GitHub . Bug reports and pull requests are welcome.","title":"Home"},{"location":"#radtorch-the-radiology-machine-learning-tool-kit","text":"","title":"RADTorch   The Radiology Machine Learning Tool Kit "},{"location":"#about","text":"RADTorch provides a package of higher level functions and classes that significantly decrease the amount of time needed for implementation of different machine and deep learning algorithms on DICOM medical images. RADTorch was developed and is currently maintained by Mohamed Elbanan, MD: a Radiology Resident at Yale New Haven Health System, Clinical Research Affiliate at Yale School of Medicine and a Machine-learning enthusiast.","title":"About"},{"location":"#getting-started","text":"Running a state-of-the-art DICOM image classifier can be run using the Image Classification Pipeline using the commands: from radtorch import pipeline classifier = pipeline.Image_Classification(data_directory='path to data') classifier.run() The above 3 lines of code will run an image classifier using VGG16 with pre-trained weights.","title":"Getting Started"},{"location":"#playground","text":"RADTorch playground for testing is provided on Google Colab .","title":"Playground"},{"location":"#feature-requests","text":"Feature requests are more than welcomed on our discussion board HERE","title":"Feature Requests"},{"location":"#contributing","text":"RadTorch is on GitHub . Bug reports and pull requests are welcome.","title":"Contributing"},{"location":"copyright/","text":"Copyrights Copyrights are reserved to authors of all used open source packages and snippets of code. PyTorch https://github.com/pytorch/pytorch/blob/master/LICENSE From PyTorch Copyright \u00a9 2016- Facebook, Inc (Adam Paszke) Copyright \u00a9 2014- Facebook, Inc (Soumith Chintala) Copyright \u00a9 2011-2014 Idiap Research Institute (Ronan Collobert) Copyright \u00a9 2012-2014 Deepmind Technologies (Koray Kavukcuoglu) Copyright \u00a9 2011-2012 NEC Laboratories America (Koray Kavukcuoglu) Copyright \u00a9 2011-2013 NYU (Clement Farabet) Copyright \u00a9 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston) Copyright \u00a9 2006 Idiap Research Institute (Samy Bengio) Copyright \u00a9 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz) From Caffe2: Copyright \u00a9 2016-present, Facebook Inc. All rights reserved. All contributions by Facebook: Copyright \u00a9 2016 Facebook Inc. All contributions by Google: Copyright \u00a9 2015 Google Inc. All rights reserved. All contributions by Yangqing Jia: Copyright \u00a9 2015 Yangqing Jia All rights reserved. All contributions from Caffe: Copyright\u00a9 2013, 2014, 2015, the respective contributors All rights reserved. All other contributions: Copyright\u00a9 2015, 2016 the respective contributors All rights reserved. Caffe2 uses a copyright model similar to Caffe: each contributor holds copyright over their contributions to Caffe2. The project versioning records all such contribution and copyright details. If a contributor wants to further mark their specific copyright on a particular contribution, they should indicate their copyright solely in the commit message of the change when it is committed. Sklearn https://scikit-learn.org/stable/about.html#citing-scikit-learn Buitinck, L., Louppe, G., Blondel, M., Pedregosa, F., Mueller, A., Grisel, O., Niculae, V., Prettenhofer, P., Gramfort, A., Grobler, J. and Layton, R., 2013. API design for machine learning software: experiences from the scikit-learn project. arXiv preprint arXiv:1309.0238. Pydicom https://github.com/pydicom/pydicom/blob/master/LICENSE License file for pydicom, a pure-python DICOM library Copyright \u00a9 2008-2020 Darcy Mason and pydicom contributors Except for portions outlined below, pydicom is released under an MIT license: Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. Portions of pydicom (private dictionary file(s)) were generated from the private dictionary of the GDCM library, released under the following license: Program: GDCM (Grassroots DICOM). A DICOM library Module: http://gdcm.sourceforge.net/Copyright.html Copyright \u00a9 2006-2010 Mathieu Malaterre Copyright \u00a9 1993-2005 CREATIS (CREATIS = Centre de Recherche et d'Applications en Traitement de l'Image) All rights reserved. Matplotlib https://matplotlib.org/users/license.html Matplotlib only uses BSD compatible code, and its license is based on the PSF license. See the Open Source Initiative licenses page for details on individual licenses. Non-BSD compatible licenses (e.g., LGPL) are acceptable in matplotlib toolkits. For a discussion of the motivations behind the licensing choice, see Licenses. Bokeh https://github.com/bokeh/demo.bokeh.org/blob/master/LICENSE.txt Copyright \u00a9 2012 - 2018, Anaconda, Inc., and Bokeh Contributors All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of Anaconda nor the names of any contributors may be used to endorse or promote products derived from this software without specific prior written permission. Confusion Matrix Matplotlib Code snippet adapted with modification from : https://www.kaggle.com/grfiv4/plot-a-confusion-matrix","title":"Copyrights"},{"location":"copyright/#copyrights","text":"Copyrights are reserved to authors of all used open source packages and snippets of code.","title":"Copyrights"},{"location":"copyright/#pytorch","text":"https://github.com/pytorch/pytorch/blob/master/LICENSE From PyTorch Copyright \u00a9 2016- Facebook, Inc (Adam Paszke) Copyright \u00a9 2014- Facebook, Inc (Soumith Chintala) Copyright \u00a9 2011-2014 Idiap Research Institute (Ronan Collobert) Copyright \u00a9 2012-2014 Deepmind Technologies (Koray Kavukcuoglu) Copyright \u00a9 2011-2012 NEC Laboratories America (Koray Kavukcuoglu) Copyright \u00a9 2011-2013 NYU (Clement Farabet) Copyright \u00a9 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston) Copyright \u00a9 2006 Idiap Research Institute (Samy Bengio) Copyright \u00a9 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz) From Caffe2: Copyright \u00a9 2016-present, Facebook Inc. All rights reserved. All contributions by Facebook: Copyright \u00a9 2016 Facebook Inc. All contributions by Google: Copyright \u00a9 2015 Google Inc. All rights reserved. All contributions by Yangqing Jia: Copyright \u00a9 2015 Yangqing Jia All rights reserved. All contributions from Caffe: Copyright\u00a9 2013, 2014, 2015, the respective contributors All rights reserved. All other contributions: Copyright\u00a9 2015, 2016 the respective contributors All rights reserved. Caffe2 uses a copyright model similar to Caffe: each contributor holds copyright over their contributions to Caffe2. The project versioning records all such contribution and copyright details. If a contributor wants to further mark their specific copyright on a particular contribution, they should indicate their copyright solely in the commit message of the change when it is committed.","title":"PyTorch"},{"location":"copyright/#sklearn","text":"https://scikit-learn.org/stable/about.html#citing-scikit-learn Buitinck, L., Louppe, G., Blondel, M., Pedregosa, F., Mueller, A., Grisel, O., Niculae, V., Prettenhofer, P., Gramfort, A., Grobler, J. and Layton, R., 2013. API design for machine learning software: experiences from the scikit-learn project. arXiv preprint arXiv:1309.0238.","title":"Sklearn"},{"location":"copyright/#pydicom","text":"https://github.com/pydicom/pydicom/blob/master/LICENSE License file for pydicom, a pure-python DICOM library Copyright \u00a9 2008-2020 Darcy Mason and pydicom contributors Except for portions outlined below, pydicom is released under an MIT license: Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. Portions of pydicom (private dictionary file(s)) were generated from the private dictionary of the GDCM library, released under the following license: Program: GDCM (Grassroots DICOM). A DICOM library Module: http://gdcm.sourceforge.net/Copyright.html Copyright \u00a9 2006-2010 Mathieu Malaterre Copyright \u00a9 1993-2005 CREATIS (CREATIS = Centre de Recherche et d'Applications en Traitement de l'Image) All rights reserved.","title":"Pydicom"},{"location":"copyright/#matplotlib","text":"https://matplotlib.org/users/license.html Matplotlib only uses BSD compatible code, and its license is based on the PSF license. See the Open Source Initiative licenses page for details on individual licenses. Non-BSD compatible licenses (e.g., LGPL) are acceptable in matplotlib toolkits. For a discussion of the motivations behind the licensing choice, see Licenses.","title":"Matplotlib"},{"location":"copyright/#bokeh","text":"https://github.com/bokeh/demo.bokeh.org/blob/master/LICENSE.txt Copyright \u00a9 2012 - 2018, Anaconda, Inc., and Bokeh Contributors All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of Anaconda nor the names of any contributors may be used to endorse or promote products derived from this software without specific prior written permission.","title":"Bokeh"},{"location":"copyright/#confusion-matrix-matplotlib","text":"Code snippet adapted with modification from : https://www.kaggle.com/grfiv4/plot-a-confusion-matrix","title":"Confusion Matrix Matplotlib"},{"location":"datautils/","text":"Data Module radtorch.datautils Documentation Outdated. Please check again later for update. set_random_seed datautils.set_random_seed(seed) Sets the PyTorch and Numpy randoom seed for consistensy. Arguments seed: (int) target random seed. list_of_files datautils.list_of_files(root) Create a list of file paths from a root folder and its sub directories. Arguments root: (str) path of target folder. Output list of file paths. Example root_path = 'root/' list_of_files(root_path) ['root/folder1/0000.dcm', 'root/folder1/0001.dcm', 'root/folder2/0000.dcm', ...] path_to_class datautils.path_to_class(filepath) Creates a class name from the immediate parent folder of a target file. Arguments filepath: (str) path to target file. Output (str) folder name / class name. Example file_path = 'root/folder1/folder2/0000.dcm' path_to_class(file_path) 'folder2' root_to_class datautils.root_to_class(root) Creates list of classes and dictionary of classes and idx in a given data root. All first level subfolders within the root are converted into classes and given class id. Arguments root: (str) path of target root. Output (tuple) of classes: (list) of generated classes, class_to_idx: (dictionary) of classes and class id numbers Example This example assumes that root folder contains 3 folders (folder1, folder2 and folder3) each contains images of 1 class. root_folder = 'root/' root_to_class(root_folder) ['folder1', 'folder2', 'folder3'], {'folder1':0, 'folder2':1, 'folder3':2} class_to_idx datautils.class_to_idx(classes) Creates a dictionary of classes to classes idx from provided list of classes Arguments classes: (list) list of classes Output Output: (dictionary) dictionary of classes to class idx Example class_list = ['class1','class4', 'class2', 'class3'] class_to_idx(class_list) {'class1':0, 'class2':1, 'class3':2, 'class4':3} dataset_from_folder datautils.dataset_from_folder(data_directory, is_dicom=True, mode='RAW', wl=None, trans=Compose(ToTensor())) Creates a dataset from a root directory using subdirectories as classes/labels. Parameters data_director: (str) target data root directory. is_dicom: (boolean) True for DICOM images, False for regular images.(default=True) mode: (str) output mode for DICOM images only. options: RAW= Raw pixels, HU= Image converted to Hounsefield Units, WIN= 'window' image windowed to certain W and L, MWIN = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together]. wl: (list) list of lists of combinations of window level and widths to be used with WIN and MWIN. In the form of : [[Level,Width], [Level,Width],\u2026]. Only 3 combinations are allowed for MWIN (for now). (default=None) trans: (pytorch transforms) pytroch transforms to be performed on the dataset. (default=Convert to tensor) Methods .class_to_idx() Returns dictionary of dataset classes and corresponding class id. .classes() Returns list of dataset classes .info() Returns detailed information of the dataset. len(dataset) Returns number of items in the dataset. dataset_from_table datautils.dataset_from_table(data_directory, is_csv=True, is_dicom=True, input_source=None, img_path_column='IMAGE_PATH', img_label_column='IMAGE_LABEL', mode='RAW', wl=None, trans=Compose(ToTensor())) Creates a dataset using labels and filepaths from a table which can be either a excel sheet or pandas dataframe. Parameters data_directory: (str) target data directory. is_csv: (boolean) True for csv, False for pandas dataframe. (default=True) is_dicom: (boolean) True for DICOM images, False for regular images.(default=True) input_source: (str or pandas dataframe object) source for labelling data. This is path to csv file or name of pandas dataframe if pandas to be used. img_path_column: (str) name of the image path column in data input. (default = \"IMAGE_PATH\") img_label_column: (str) name of label column in the data input (default = \"IMAGE_LABEL\") mode: (str) output mode for DICOM images only. options: RAW= Raw pixels, HU= Image converted to Hounsefield Units, WIN= 'window' image windowed to certain W and L, MWIN = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together]. wl: (list) list of lists of combinations of window level and widths to be used with WIN and MWIN.In the form of : [[Level,Width], [Level,Width],\u2026]. Only 3 combinations are allowed for MWIN (for now). (default=None) transforms: (pytorch transforms) pytroch transforms to be performed on the dataset. (default=Convert to tensor) Methods .class_to_idx() Returns dictionary of dataset classes and corresponding class id. .classes() Returns list of dataset classes .info() Returns detailed information of the dataset. len(dataset) Returns number of items in the dataset.","title":"Datautils Module"},{"location":"datautils/#data-module-radtorchdatautils","text":"Documentation Outdated. Please check again later for update.","title":"Data Module  radtorch.datautils "},{"location":"datautils/#set_random_seed","text":"datautils.set_random_seed(seed) Sets the PyTorch and Numpy randoom seed for consistensy. Arguments seed: (int) target random seed.","title":"set_random_seed"},{"location":"datautils/#list_of_files","text":"datautils.list_of_files(root) Create a list of file paths from a root folder and its sub directories. Arguments root: (str) path of target folder. Output list of file paths. Example root_path = 'root/' list_of_files(root_path) ['root/folder1/0000.dcm', 'root/folder1/0001.dcm', 'root/folder2/0000.dcm', ...]","title":"list_of_files"},{"location":"datautils/#path_to_class","text":"datautils.path_to_class(filepath) Creates a class name from the immediate parent folder of a target file. Arguments filepath: (str) path to target file. Output (str) folder name / class name. Example file_path = 'root/folder1/folder2/0000.dcm' path_to_class(file_path) 'folder2'","title":"path_to_class"},{"location":"datautils/#root_to_class","text":"datautils.root_to_class(root) Creates list of classes and dictionary of classes and idx in a given data root. All first level subfolders within the root are converted into classes and given class id. Arguments root: (str) path of target root. Output (tuple) of classes: (list) of generated classes, class_to_idx: (dictionary) of classes and class id numbers Example This example assumes that root folder contains 3 folders (folder1, folder2 and folder3) each contains images of 1 class. root_folder = 'root/' root_to_class(root_folder) ['folder1', 'folder2', 'folder3'], {'folder1':0, 'folder2':1, 'folder3':2}","title":"root_to_class"},{"location":"datautils/#class_to_idx","text":"datautils.class_to_idx(classes) Creates a dictionary of classes to classes idx from provided list of classes Arguments classes: (list) list of classes Output Output: (dictionary) dictionary of classes to class idx Example class_list = ['class1','class4', 'class2', 'class3'] class_to_idx(class_list) {'class1':0, 'class2':1, 'class3':2, 'class4':3}","title":"class_to_idx"},{"location":"datautils/#dataset_from_folder","text":"datautils.dataset_from_folder(data_directory, is_dicom=True, mode='RAW', wl=None, trans=Compose(ToTensor())) Creates a dataset from a root directory using subdirectories as classes/labels. Parameters data_director: (str) target data root directory. is_dicom: (boolean) True for DICOM images, False for regular images.(default=True) mode: (str) output mode for DICOM images only. options: RAW= Raw pixels, HU= Image converted to Hounsefield Units, WIN= 'window' image windowed to certain W and L, MWIN = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together]. wl: (list) list of lists of combinations of window level and widths to be used with WIN and MWIN. In the form of : [[Level,Width], [Level,Width],\u2026]. Only 3 combinations are allowed for MWIN (for now). (default=None) trans: (pytorch transforms) pytroch transforms to be performed on the dataset. (default=Convert to tensor) Methods .class_to_idx() Returns dictionary of dataset classes and corresponding class id. .classes() Returns list of dataset classes .info() Returns detailed information of the dataset. len(dataset) Returns number of items in the dataset.","title":"dataset_from_folder"},{"location":"datautils/#dataset_from_table","text":"datautils.dataset_from_table(data_directory, is_csv=True, is_dicom=True, input_source=None, img_path_column='IMAGE_PATH', img_label_column='IMAGE_LABEL', mode='RAW', wl=None, trans=Compose(ToTensor())) Creates a dataset using labels and filepaths from a table which can be either a excel sheet or pandas dataframe. Parameters data_directory: (str) target data directory. is_csv: (boolean) True for csv, False for pandas dataframe. (default=True) is_dicom: (boolean) True for DICOM images, False for regular images.(default=True) input_source: (str or pandas dataframe object) source for labelling data. This is path to csv file or name of pandas dataframe if pandas to be used. img_path_column: (str) name of the image path column in data input. (default = \"IMAGE_PATH\") img_label_column: (str) name of label column in the data input (default = \"IMAGE_LABEL\") mode: (str) output mode for DICOM images only. options: RAW= Raw pixels, HU= Image converted to Hounsefield Units, WIN= 'window' image windowed to certain W and L, MWIN = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together]. wl: (list) list of lists of combinations of window level and widths to be used with WIN and MWIN.In the form of : [[Level,Width], [Level,Width],\u2026]. Only 3 combinations are allowed for MWIN (for now). (default=None) transforms: (pytorch transforms) pytroch transforms to be performed on the dataset. (default=Convert to tensor) Methods .class_to_idx() Returns dictionary of dataset classes and corresponding class id. .classes() Returns list of dataset classes .info() Returns detailed information of the dataset. len(dataset) Returns number of items in the dataset.","title":"dataset_from_table"},{"location":"dicomutils/","text":"DICOM Module radtorch.dicomutils Documentation Outdated. Please check again later for update. Tools and Functions for DICOM images handling and extraction of pixel information. from radtorch import dicomutils window_dicom dicomutils.window_dicom(filepath, level, width) Converts DICOM image to numpy array with certain width and level. Arguments filepath: (str) input DICOM image path. level: (int) target window level. width: (int) target window width. Output Output: (array) windowed image as numpy array. dicom_to_narray dicomutils.dicom_to_narray(filepath, mode='RAW', wl=None) Converts DICOM image to a numpy array with target changes as below. Arguments filepath: (str) input DICOM image path. mode: (str) output mode. (default='RAW') Options: 'RAW' = Raw pixels, 'HU' = Image converted to Hounsefield Units. 'WIN' = 'window' image windowed to certain W and L, 'MWIN' = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together]. wl: (list) list of lists of combinations of window level and widths to be used with WIN and MWIN. (default=None) In the form of : [[Level,Width], [Level,Width],...]. Only 3 combinations are allowed for MWIN (for now). Output Output: (array) array of same shape as input DICOM image with 1 channel. In case of MWIN mode, output has same size by 3 channels. dicom_to_pil dicomutils.dicom_to_pil(filepath) Converts DICOM image to PIL image object. Arguments filepath: (str) input DICOM image path. Output Output: (pillow image object) .","title":"Dicomutils Module"},{"location":"dicomutils/#dicom-module-radtorchdicomutils","text":"Documentation Outdated. Please check again later for update. Tools and Functions for DICOM images handling and extraction of pixel information. from radtorch import dicomutils","title":"DICOM Module  radtorch.dicomutils "},{"location":"dicomutils/#window_dicom","text":"dicomutils.window_dicom(filepath, level, width) Converts DICOM image to numpy array with certain width and level. Arguments filepath: (str) input DICOM image path. level: (int) target window level. width: (int) target window width. Output Output: (array) windowed image as numpy array.","title":"window_dicom"},{"location":"dicomutils/#dicom_to_narray","text":"dicomutils.dicom_to_narray(filepath, mode='RAW', wl=None) Converts DICOM image to a numpy array with target changes as below. Arguments filepath: (str) input DICOM image path. mode: (str) output mode. (default='RAW') Options: 'RAW' = Raw pixels, 'HU' = Image converted to Hounsefield Units. 'WIN' = 'window' image windowed to certain W and L, 'MWIN' = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together]. wl: (list) list of lists of combinations of window level and widths to be used with WIN and MWIN. (default=None) In the form of : [[Level,Width], [Level,Width],...]. Only 3 combinations are allowed for MWIN (for now). Output Output: (array) array of same shape as input DICOM image with 1 channel. In case of MWIN mode, output has same size by 3 channels.","title":"dicom_to_narray"},{"location":"dicomutils/#dicom_to_pil","text":"dicomutils.dicom_to_pil(filepath) Converts DICOM image to PIL image object. Arguments filepath: (str) input DICOM image path. Output Output: (pillow image object) .","title":"dicom_to_pil"},{"location":"install/","text":"Installation RADTorch tool kit and its dependencies can be installed using the following terminal commands: pip3 install https://repo.radtorch.com/archive/v0.1.3-beta.zip To uninstall simply use: pip3 uninstall radtorch Requirements Internet connection is required for installation and model weight download. Python 3.5 or later. Dependencies RADTorch depends on the following packages which are installed automatically during the installation process. torch 1.4.0 torchvision 0.5.0 numpy 1.17.4 pandas 0.25.3 pydicom 1.3.0 matplotlib 3.1.3 pillow 7.0.0 tqdm 4.38.0 sklearn 0.22.1 pathlib Bokeh 2.0.0","title":"Installation"},{"location":"install/#installation","text":"RADTorch tool kit and its dependencies can be installed using the following terminal commands: pip3 install https://repo.radtorch.com/archive/v0.1.3-beta.zip To uninstall simply use: pip3 uninstall radtorch","title":"Installation"},{"location":"install/#requirements","text":"Internet connection is required for installation and model weight download. Python 3.5 or later.","title":"Requirements"},{"location":"install/#dependencies","text":"RADTorch depends on the following packages which are installed automatically during the installation process. torch 1.4.0 torchvision 0.5.0 numpy 1.17.4 pandas 0.25.3 pydicom 1.3.0 matplotlib 3.1.3 pillow 7.0.0 tqdm 4.38.0 sklearn 0.22.1 pathlib Bokeh 2.0.0","title":"Dependencies"},{"location":"license/","text":"GNU Affero General Public License v3.0 License Copyright \u00a9 2020 RADTorch, Mohamed Elbanan M.D. This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program. If not, see https://www.gnu.org/licenses/.","title":"License"},{"location":"license/#gnu-affero-general-public-license-v30-license","text":"Copyright \u00a9 2020 RADTorch, Mohamed Elbanan M.D. This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program. If not, see https://www.gnu.org/licenses/.","title":"GNU Affero General Public License v3.0 License"},{"location":"modelsutils/","text":"Models Module radtorch.modelsutils Documentation Outdated. Please check again later for update. create_model modelsutils.create_model(model_arch, output_classes, mode, pre_trained=True, unfreeze_weights=True) Creates a PyTorch training neural network model with specified network architecture. Input channels and output classes can be specified. Arguments model_arch: (str) The architecture of the model neural network. Examples include 'vgg16', 'resnet50', and 'resnet152'. pre_trained: (boolen) Load the pretrained weights of the neural network. (default=True) unfreeze_weights: (boolen) Unfreeze model weights for training.(default=True) output_classes: (int) Number of output classes for image classification problems. mode: (str) 'train' for training model. 'feature_extraction' for feature extraction model Output Output: (PyTorch neural network object) Example my_model = modelsutils.create_model(model_arch='resnet50', output_classes=2) create_loss_function modelsutils.create_loss_function(type) Creates a PyTorch training loss function object. Arguments type: (str) type of the loss functions required. Output Output: (PyTorch loss function object) Example loss = modelsutils.create_loss_function(type='NLLLoss') create_optimizer modelsutils.create_optimizer(traning_model, optimizer_type, learning_rate) Creates a PyTorch optimizer object. Arguments training_model: (pytorch Model object) target training model. optimizer_type: (str) type of optimizer e.g.'Adam' or 'SGD'. learning_rate: (float) learning rate. Output Output: (PyTorch optimizer object) train_model train_model(model, train_data_loader, valid_data_loader, train_data_set, valid_data_set, loss_criterion, optimizer, epochs, device, verbose) Training loop for pytorch model object. Arguments model: (PyTorch neural network object) Model to be trained. train_data_loader: (PyTorch dataloader object) training data dataloader. valid_data_loader: (PyTorch dataloader object) validation data dataloader. train_data_loader: (PyTorch dataset object) training data dataset. valid_data_loader: (PyTorch dataset object) validation data dataset. loss_criterion: (PyTorch nn object) Loss function to be used during training. optimizer: (PyTorch optimizer object) Optimizer to be used during training. epochs: (int) training epochs. device: (str) device to be used for training. This can be 'cpu' or 'cuda'. verbose: (boolen) True to display training messages. Output model: (PyTorch neural network object) trained model. train_metrics: (list) list of np arrays of training loss and accuracy. Example trained_model, training_metrics = modelsutils.train_model(model=my_model, train_data_loader=train_dl, valid_data_loader=valid_dl, train_data_set=train_ds, valid_data_set=valid_ds, loss_criterion=my_loss, optimizer=my_optim, epochs=100, device='cuda', verbose=True) model_inference modelsutils.model_inference(model, input_image_path, inference_transformations=transforms.Compose([transforms.ToTensor()])) Performs Inference/Predictions on a target image using a trained model. Arguments model: (PyTorch Model) Trained neural network. input_image_path: (str) path to target image inference_transformations: (pytorch transforms list) pytroch transforms to be performed on the dataset. Output Output: (tupe) tuple of prediction class id and prediction accuracy percentage. supported modelsutils.supported() Returns a list of the currently supported network architectures and loss functions.","title":"Modelsutils Module"},{"location":"modelsutils/#models-module-radtorchmodelsutils","text":"Documentation Outdated. Please check again later for update.","title":"Models Module  radtorch.modelsutils "},{"location":"modelsutils/#create_model","text":"modelsutils.create_model(model_arch, output_classes, mode, pre_trained=True, unfreeze_weights=True) Creates a PyTorch training neural network model with specified network architecture. Input channels and output classes can be specified. Arguments model_arch: (str) The architecture of the model neural network. Examples include 'vgg16', 'resnet50', and 'resnet152'. pre_trained: (boolen) Load the pretrained weights of the neural network. (default=True) unfreeze_weights: (boolen) Unfreeze model weights for training.(default=True) output_classes: (int) Number of output classes for image classification problems. mode: (str) 'train' for training model. 'feature_extraction' for feature extraction model Output Output: (PyTorch neural network object) Example my_model = modelsutils.create_model(model_arch='resnet50', output_classes=2)","title":"create_model"},{"location":"modelsutils/#create_loss_function","text":"modelsutils.create_loss_function(type) Creates a PyTorch training loss function object. Arguments type: (str) type of the loss functions required. Output Output: (PyTorch loss function object) Example loss = modelsutils.create_loss_function(type='NLLLoss')","title":"create_loss_function"},{"location":"modelsutils/#create_optimizer","text":"modelsutils.create_optimizer(traning_model, optimizer_type, learning_rate) Creates a PyTorch optimizer object. Arguments training_model: (pytorch Model object) target training model. optimizer_type: (str) type of optimizer e.g.'Adam' or 'SGD'. learning_rate: (float) learning rate. Output Output: (PyTorch optimizer object)","title":"create_optimizer"},{"location":"modelsutils/#train_model","text":"train_model(model, train_data_loader, valid_data_loader, train_data_set, valid_data_set, loss_criterion, optimizer, epochs, device, verbose) Training loop for pytorch model object. Arguments model: (PyTorch neural network object) Model to be trained. train_data_loader: (PyTorch dataloader object) training data dataloader. valid_data_loader: (PyTorch dataloader object) validation data dataloader. train_data_loader: (PyTorch dataset object) training data dataset. valid_data_loader: (PyTorch dataset object) validation data dataset. loss_criterion: (PyTorch nn object) Loss function to be used during training. optimizer: (PyTorch optimizer object) Optimizer to be used during training. epochs: (int) training epochs. device: (str) device to be used for training. This can be 'cpu' or 'cuda'. verbose: (boolen) True to display training messages. Output model: (PyTorch neural network object) trained model. train_metrics: (list) list of np arrays of training loss and accuracy. Example trained_model, training_metrics = modelsutils.train_model(model=my_model, train_data_loader=train_dl, valid_data_loader=valid_dl, train_data_set=train_ds, valid_data_set=valid_ds, loss_criterion=my_loss, optimizer=my_optim, epochs=100, device='cuda', verbose=True)","title":"train_model"},{"location":"modelsutils/#model_inference","text":"modelsutils.model_inference(model, input_image_path, inference_transformations=transforms.Compose([transforms.ToTensor()])) Performs Inference/Predictions on a target image using a trained model. Arguments model: (PyTorch Model) Trained neural network. input_image_path: (str) path to target image inference_transformations: (pytorch transforms list) pytroch transforms to be performed on the dataset. Output Output: (tupe) tuple of prediction class id and prediction accuracy percentage.","title":"model_inference"},{"location":"modelsutils/#supported","text":"modelsutils.supported() Returns a list of the currently supported network architectures and loss functions.","title":"supported"},{"location":"pipeline/","text":"Pipeline Module radtorch.pipeline Pipelines are probably the most exciting feature of RADTorch tool kit. With few lines of code, the pipeline module allows you to run state-of-the-art image classification algorithms and much more. from radtorch import pipeline Image_Classification pipeline.Image_Classification(data_directory, name = None, transformations='default',custom_resize = 'default', device='default', optimizer='Adam', is_dicom=True, label_from_table=False, is_csv=None, table_source=None, path_col = 'IMAGE_PATH', label_col = 'IMAGE_LABEL', balance_class = False, predefined_datasets = False, mode='RAW', wl=None, normalize='default', batch_size=16, test_percent = 0.2, valid_percent = 0.2, model_arch='vgg16', pre_trained=True, unfreeze_weights=False,train_epochs=20, learning_rate=0.0001, loss_function='CrossEntropyLoss') Description The Image Classification pipeline simplifies the process of binary and multi-class image classification into a single line of code. Under the hood, the following happens: The pipeline creates a master dataset from the provided data directory and source of labels/classes either from folder structre or pandas/csv table. Master dataset is subdivided into train, valid and test subsets using the percentages defined by user. The following transformations are applied on the dataset images: Resize to the default image size allowed by the model architecture. Window/Level adjustment according to values specified by user. Single channel grayscale DICOM images are converted into 3 channel grayscale images to fit into the model. Selected Model architecture, optimizer, and loss function are downloaded/created. Model is trained. Training metrics are saved as training progresses and can be displayed after training is done. Confusion Matrix and ROC (for binary classification) can be displayed as well (by default, the test subset is used to calculate the confusion matrix and the ROC) Misclassifed samples can be displayed. Trained model can be exported to outside file for future use. Parameters data_directory: (str) target data directory. (Required) name: (str) preferred name to be given to classifier. is_dicom: (boolean) True for DICOM images. (default=True) label_from_table: (boolean) True if labels are to extracted from table, False if labels are to be extracted from subfolders names. (default=False) is_csv: (boolean) True for csv, False for pandas dataframe. table_source: (str or pandas dataframe object) source for labelling data.This is path to csv file or name of pandas dataframe if pandas to be used. (default=None). predefined_datasets (dict) dictionary of predefined pandas dataframes for training. This follows the following scheme: {'train': dataframe, 'valid': dataframe, 'test':dataframe } path_col: (str) name of the column with the image path. (default='IMAGE_PATH') label_col: (str) name of the label/class column. (default='IMAGE_LABEL') mode: (str) output mode for DICOM images only where RAW= Raw pixels, HU= Image converted to Hounsefield Units, WIN= 'window' image windowed to certain W and L, MWIN = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together. (default='RAW') wl: (list) list of lists of combinations of window level and widths to be used with WIN and MWIN.In the form of : [[Level,Width], [Level,Width],...]. Only 3 combinations are allowed for MWIN (for now). (default=None) balance_class: (boolen) balance classes in train/valid/test subsets. Under the hood, oversampling is done for the classes with fewer number of instances. (default=False) normalize: (str) Normalization algorithm applied to data. Options: 'default' normalizes data with mean of 0.5 and standard deviation of 0.5, 'auto' normalizes the data using mean and standard deviation calculated from the datasets, 'False' applies no normalization. (default='default') transformations: (pytorch transforms list) pytroch transforms to be performed on the dataset. (default=Convert to tensor) custom_resize: - (int) by default, a radtorch pipeline will resize the input images into the default training model input image size as demonstrated in the table shown below. This default size can be changed here if needed. batch_size: (int) batch size of the dataset (default=16) test_percent: (float) percentage of dataset to use for testing. Float value between 0 and 1.0. (default=0.2) valid_percent: (float) percentage of dataset to use for validation. Float value between 0 and 1.0. (default=0.2) model_arch: (str) PyTorch neural network architecture (default='vgg16') pre_trained: (boolean) Load the pretrained weights of the neural network. (default=True) unfreeze_weights: (boolean) if True, all model weights will be retrained. This note that if no pre_trained weights are applied, this option will be set to True automatically. (default=False) train_epochs: (int) Number of training epochs. (default=20) learning_rate: (str) training learning rate. (default = 0.0001) loss_function: (str) training loss function. (default='CrossEntropyLoss') optimizer: (str) Optimizer to be used during training. (default='Adam') device: (str) device to be used for training. This can be adjusted to 'cpu' or 'cuda'. If nothing is selected, the pipeline automatically detects if CUDA is available and trains on it. Methods .info() Display table with properties of the Image Classification Pipeline. .dataset_info(plot=True, plot_size=(500,300)) Display Dataset Information. Arguments: plot: (boolean) displays dataset information in graphical representation. (default=True) plot_size: (tuple) figures size. .sample(fig_size=(10,10), show_labels=True, show_file_name=False) Display sample of the training dataset. Arguments: fig_size: (tuple) figure size. (default=(10,10)) show_labels: (boolean) show the image label idx. (default=True) show_file_name: (boolean) show the file name as label. (default=False) .run(verbose=True) Start the image classification pipeline training. Arguments: verbose: (boolean) Show display progress after each epoch. (default=True) .metrics(fig_size=(500,300)) Display the training metrics. .export_model(output_path) Export the trained model into a target file. Arguments: output_path: (str) path to output file. For example 'foler/folder/model.pth' .export(output_path) Exports the whole image classification pipeline for future use Arguments: target_path: (str) target location for export. .set_trained_model(model_path, mode) Loads a previously trained model into pipeline Arguments: model_path: (str) path to target model mode: (str) either 'train' or 'infer'.'train' will load the model to be trained. 'infer' will load the model for inference. .inference(test_img_path, transformations='default', all_predictions=False) Performs inference using the trained model on a target image. Arguments: test_img_path: (str) path to target image. transformations: (pytorch transforms list) list of transforms to be performed on the target image. (default='default' which is the same transforms using for training the pipeline) all_predictions: (boolean) if True , shows table of all prediction classes and accuracy percentages. (default=False) .roc(target_data_set='default', figure_size=(600,400)) Display ROC and AUC. Arguments: target_data_set: (pytorch dataset object) dataset used for predictions to create the ROC. By default, the image classification pipeline uses the test dataset created to calculate the ROC. If no test dataset was created in the pipeline (e.g. test_percent=0), then an external test dataset is required. (default=default') figure_size: (tuple) figure size. (default=(600,400)) .confusion_matrix(target_data_set='default', target_classes='default', figure_size=(7,7), cmap=None) Display Confusion Matrix Arguments: target_data_set: (pytorch dataset object) dataset used for predictions to create the confusion matrix. By default, the image classification - pipeline uses the test dataset created to calculate the matrix. target_classes: (list) list of classes. By default, the image classification pipeline uses the training classes. figure_size: (tuple) figure size. (default=(7,7)) cmap: (str) user specific matplotlib cmap. .misclassfied(target_data_set='default', num_of_images=16, figure_size=(10,10), show_table=False) Displays sample of misclassfied images from confusion matrix or ROC. Arguments: target_data_set: (pytorch dataset object) dataset used for predictions. By default, the image classification pipeline uses the test dataset. If no test dataset was created in the pipeline (e.g. test_percent=0), then an external test dataset is required. (default=default') num_of_images: (int) number of images to be displayed. figure_size: (tuple) figure size (default=(10,10)) show_table: (boolean) display table of misclassied images. (default=False) Example Full example for Image Classification Pipeline can be found HERE Feature_Extraction pipeline.Feature_Extraction(data_directory, transformations='default', custom_resize = 'default', is_dicom=True,label_from_table=False, is_csv=None,table_source=None, device='default', path_col = 'IMAGE_PATH', label_col = 'IMAGE_LABEL', mode='RAW', wl=None, model_arch='vgg16', pre_trained=True, unfreeze_weights=False, shuffle=True) Description The feature extraction pipeline utilizes a pre-trained model to extract a set of features that can be used in another machine learning algorithms e.g. Adaboost or KNN. The trained model by default can one of the supported model architectures trained with default weights trained on the ImageNet dataset. (The ability to use a model that has been trained and exported using the image classification pipeline will be added later.) The output is a pandas dataframe that has feature columns, label column and file path column. Under the hood, the pipeline removes the last FC layer of the pretrained models to output the features. The number of extracted features depends on the model architecture selected: Model Architecture Default Input Image Size Output Features vgg11 224 x 224 4096 vgg13 224 x 224 4096 vgg16 224 x 224 4096 vgg19 224 x 224 4096 vgg11_bn 224 x 224 4096 vgg13_bn 224 x 224 4096 vgg16_bn 224 x 224 4096 vgg19_bn 224 x 224 4096 resnet18 224 x 224 512 resnet34 224 x 224 512 resnet50 224 x 224 2048 resnet101 224 x 224 2048 resnet152 224 x 224 2048 wide_resnet50_2 224 x 224 2048 wide_resnet101_2 224 x 224 2048 alexnet 256 x 256 4096 Parameters data_directory: (str) target data directory. (Required) is_dicom: (boolean) True for DICOM images, False for regular images.(default=True) label_from_table: (boolean) True if labels are to extracted from table, False if labels are to be extracted from subfolders. (default=False) is_csv: (boolean) True for csv, False for pandas dataframe. table_source: (str or pandas dataframe object) source for labelling data. (default=None). This is path to csv file or name of pandas dataframe if pandas to be used. path_col: (str) name of the column with the image path. (default='IMAGE_PATH') label_col: (str) name of the label/class column. (default='IMAGE_LABEL') shuffle - (boolean) shuffles items in dataset.(default=True) mode: (str) output mode for DICOM images only. Options: RAW= Raw pixels, HU= Image converted to Hounsefield Units, WIN= 'window' image windowed to certain W and L, MWIN = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together]. (default='RAW') wl: (list) list of lists of combinations of window level and widths to be used with WIN and MWIN. In the form of : [[Level,Width], [Level,Width],...]. Only 3 combinations are allowed for MWIN (for now).(default=None) transformations: (pytorch transforms) pytroch transforms to be performed on the dataset. (default=Convert to tensor) custom_resize: (int) by default, a radtorch pipeline will resize the input images into the default training model input image size as demosntrated in the table shown in radtorch home page. This default size can be changed here if needed. model_arch: [str] PyTorch neural network architecture (default='vgg16') pre_trained: (boolean) Load the pretrained weights of the neural network. If False, the last layer is only retrained = Transfer Learning. (default=True) device: (str) device to be used for training. This can be adjusted to 'cpu' or 'cuda'. If nothing is selected, the pipeline automatically detects if cuda is available and trains on it. Methods .info() Display Pandas Dataframe with properties of the Pipeline. .dataset_info(plot=True) Display Dataset Information. Arguments: plot: (boolean) displays dataset information in graphical representation. (default=True) .sample(fig_size=(10,10), show_labels=True, show_file_name=False) Display sample of the training dataset. Arguments: fig_size: (tuple) figure size. (default=(10,10)) show_labels: (boolean) show the image label idx. (default=True) show_file_name: (boolean) show the image name as label. (default=False) .num_features() Displays number of features to be extracted. .run(verbose=True) Extracts features from dataset. Arguments: verbose: (boolean) Show the feature table. (default=True) .export_features(csv_path) Exports the features to csv. Arguments: csv_path: (str) Path to output csv file. .export(target_path) Exports the whole image classification pipeline for future use Arguments: target_path: (str) target location for export. .plot_extracted_features(feature_table=None, feature_names=None, num_features=100, num_images=100,image_path_col='img_path', image_label_col='label_idx') Plots a graphical representation of extracted features. Arguments: num_features: (int) number of features to be displayed (default=100) num_images: (int) number of images to display features for (default=100) image_path_col: (str) name of column containing image paths in the feature table. (default='img_path') image_label_col: (str) name of column containing image label idx in the feature table. (default='label_idx') Examples Full example for Feature Extraction Pipeline can be found HERE Compare_Image_Classifier pipeline.Compare_Image_Classifier(data_directory,transformations='default', custom_resize = 'default', device='default', optimizer='Adam', is_dicom=True, label_from_table=False, is_csv=None, table_source=None, path_col = 'IMAGE_PATH', label_col = 'IMAGE_LABEL', balance_class =[False], multi_label = False, mode='RAW', wl=None, normalize=['default'], batch_size=[8], test_percent = [0.2], valid_percent = [0.2], model_arch=['vgg16'], pre_trained=[True], unfreeze_weights=False, train_epochs=[10], learning_rate=[0.0001],loss_function='CrossEntropyLoss') Description The Compare Image Classifier class performs analysis and comparison of different image classification pipelines. This is particularly useful when comparing different model architectures and/or different training parameters. Important Please note that this pipeline performs training from scratch on the selected model architectures. The ability to compared outside trained models will be added in a future release. Supported Parameters The currently supported parameters that can be compared include: balance_class normalize batch_size test_percent valid_percent train_epochs learning_rate model_arch self.pre_trained Use of supported parameters Please note that the supported parameters are supplied as List e.g. model_arch=['renet50'] or train_epochs=[10,20]. Parameters This pipeline follows the same parameters used for the image classification as above. Please take note of the warning on how to use the supported parameters above. Methods .info() Display Pandas Dataframe with properties of the Pipeline. .grid() Display table with all generated image classifier objects that will be used for comparison. .parameters() Displays a list of supported comparison parameters. .dataset_info(plot=True) Display Dataset Information. Arguments: plot: (boolean) displays dataset information in graphical representation. (default=True) .sample(fig_size=(10,10), show_labels=True, show_file_name=False) Display sample of the training dataset. Arguments: fig_size: (tuple) figure size. (default=(10,10)) show_labels: (boolean) show the image label idx. (default=True) show_file_name: (boolean) show the image name as label. (default=False) .run(verbose=True) Runs the pipeline. .metrics(fig_size=(650,400))) Display the training metrics. .roc(fig_size=(700,400)) Displays comparison between ROC curves of different classifiers with AUC. .best(path=None, export_classifier=False, export_model=False)) Displays the best classifier based on AUC. Arguments: path: (str) exporting path. export_classifier: (boolen) export the best classifier. export_model: (boolen) export the best model. Examples Full example for Compare_Image_Classifier can be found HERE load_pipeline pipeline.load_pipeline(target_path) Description Loads a previously saved pipeline for future use. Arguments target_path: (str) target path of the target pipeline. Examples my_classifier = load_pipeline('/path/to/pipeline.dump')","title":"Pipeline Module"},{"location":"pipeline/#pipeline-module-radtorchpipeline","text":"Pipelines are probably the most exciting feature of RADTorch tool kit. With few lines of code, the pipeline module allows you to run state-of-the-art image classification algorithms and much more. from radtorch import pipeline","title":"Pipeline Module  radtorch.pipeline "},{"location":"pipeline/#image_classification","text":"pipeline.Image_Classification(data_directory, name = None, transformations='default',custom_resize = 'default', device='default', optimizer='Adam', is_dicom=True, label_from_table=False, is_csv=None, table_source=None, path_col = 'IMAGE_PATH', label_col = 'IMAGE_LABEL', balance_class = False, predefined_datasets = False, mode='RAW', wl=None, normalize='default', batch_size=16, test_percent = 0.2, valid_percent = 0.2, model_arch='vgg16', pre_trained=True, unfreeze_weights=False,train_epochs=20, learning_rate=0.0001, loss_function='CrossEntropyLoss') Description The Image Classification pipeline simplifies the process of binary and multi-class image classification into a single line of code. Under the hood, the following happens: The pipeline creates a master dataset from the provided data directory and source of labels/classes either from folder structre or pandas/csv table. Master dataset is subdivided into train, valid and test subsets using the percentages defined by user. The following transformations are applied on the dataset images: Resize to the default image size allowed by the model architecture. Window/Level adjustment according to values specified by user. Single channel grayscale DICOM images are converted into 3 channel grayscale images to fit into the model. Selected Model architecture, optimizer, and loss function are downloaded/created. Model is trained. Training metrics are saved as training progresses and can be displayed after training is done. Confusion Matrix and ROC (for binary classification) can be displayed as well (by default, the test subset is used to calculate the confusion matrix and the ROC) Misclassifed samples can be displayed. Trained model can be exported to outside file for future use. Parameters data_directory: (str) target data directory. (Required) name: (str) preferred name to be given to classifier. is_dicom: (boolean) True for DICOM images. (default=True) label_from_table: (boolean) True if labels are to extracted from table, False if labels are to be extracted from subfolders names. (default=False) is_csv: (boolean) True for csv, False for pandas dataframe. table_source: (str or pandas dataframe object) source for labelling data.This is path to csv file or name of pandas dataframe if pandas to be used. (default=None). predefined_datasets (dict) dictionary of predefined pandas dataframes for training. This follows the following scheme: {'train': dataframe, 'valid': dataframe, 'test':dataframe } path_col: (str) name of the column with the image path. (default='IMAGE_PATH') label_col: (str) name of the label/class column. (default='IMAGE_LABEL') mode: (str) output mode for DICOM images only where RAW= Raw pixels, HU= Image converted to Hounsefield Units, WIN= 'window' image windowed to certain W and L, MWIN = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together. (default='RAW') wl: (list) list of lists of combinations of window level and widths to be used with WIN and MWIN.In the form of : [[Level,Width], [Level,Width],...]. Only 3 combinations are allowed for MWIN (for now). (default=None) balance_class: (boolen) balance classes in train/valid/test subsets. Under the hood, oversampling is done for the classes with fewer number of instances. (default=False) normalize: (str) Normalization algorithm applied to data. Options: 'default' normalizes data with mean of 0.5 and standard deviation of 0.5, 'auto' normalizes the data using mean and standard deviation calculated from the datasets, 'False' applies no normalization. (default='default') transformations: (pytorch transforms list) pytroch transforms to be performed on the dataset. (default=Convert to tensor) custom_resize: - (int) by default, a radtorch pipeline will resize the input images into the default training model input image size as demonstrated in the table shown below. This default size can be changed here if needed. batch_size: (int) batch size of the dataset (default=16) test_percent: (float) percentage of dataset to use for testing. Float value between 0 and 1.0. (default=0.2) valid_percent: (float) percentage of dataset to use for validation. Float value between 0 and 1.0. (default=0.2) model_arch: (str) PyTorch neural network architecture (default='vgg16') pre_trained: (boolean) Load the pretrained weights of the neural network. (default=True) unfreeze_weights: (boolean) if True, all model weights will be retrained. This note that if no pre_trained weights are applied, this option will be set to True automatically. (default=False) train_epochs: (int) Number of training epochs. (default=20) learning_rate: (str) training learning rate. (default = 0.0001) loss_function: (str) training loss function. (default='CrossEntropyLoss') optimizer: (str) Optimizer to be used during training. (default='Adam') device: (str) device to be used for training. This can be adjusted to 'cpu' or 'cuda'. If nothing is selected, the pipeline automatically detects if CUDA is available and trains on it. Methods .info() Display table with properties of the Image Classification Pipeline. .dataset_info(plot=True, plot_size=(500,300)) Display Dataset Information. Arguments: plot: (boolean) displays dataset information in graphical representation. (default=True) plot_size: (tuple) figures size. .sample(fig_size=(10,10), show_labels=True, show_file_name=False) Display sample of the training dataset. Arguments: fig_size: (tuple) figure size. (default=(10,10)) show_labels: (boolean) show the image label idx. (default=True) show_file_name: (boolean) show the file name as label. (default=False) .run(verbose=True) Start the image classification pipeline training. Arguments: verbose: (boolean) Show display progress after each epoch. (default=True) .metrics(fig_size=(500,300)) Display the training metrics. .export_model(output_path) Export the trained model into a target file. Arguments: output_path: (str) path to output file. For example 'foler/folder/model.pth' .export(output_path) Exports the whole image classification pipeline for future use Arguments: target_path: (str) target location for export. .set_trained_model(model_path, mode) Loads a previously trained model into pipeline Arguments: model_path: (str) path to target model mode: (str) either 'train' or 'infer'.'train' will load the model to be trained. 'infer' will load the model for inference. .inference(test_img_path, transformations='default', all_predictions=False) Performs inference using the trained model on a target image. Arguments: test_img_path: (str) path to target image. transformations: (pytorch transforms list) list of transforms to be performed on the target image. (default='default' which is the same transforms using for training the pipeline) all_predictions: (boolean) if True , shows table of all prediction classes and accuracy percentages. (default=False) .roc(target_data_set='default', figure_size=(600,400)) Display ROC and AUC. Arguments: target_data_set: (pytorch dataset object) dataset used for predictions to create the ROC. By default, the image classification pipeline uses the test dataset created to calculate the ROC. If no test dataset was created in the pipeline (e.g. test_percent=0), then an external test dataset is required. (default=default') figure_size: (tuple) figure size. (default=(600,400)) .confusion_matrix(target_data_set='default', target_classes='default', figure_size=(7,7), cmap=None) Display Confusion Matrix Arguments: target_data_set: (pytorch dataset object) dataset used for predictions to create the confusion matrix. By default, the image classification - pipeline uses the test dataset created to calculate the matrix. target_classes: (list) list of classes. By default, the image classification pipeline uses the training classes. figure_size: (tuple) figure size. (default=(7,7)) cmap: (str) user specific matplotlib cmap. .misclassfied(target_data_set='default', num_of_images=16, figure_size=(10,10), show_table=False) Displays sample of misclassfied images from confusion matrix or ROC. Arguments: target_data_set: (pytorch dataset object) dataset used for predictions. By default, the image classification pipeline uses the test dataset. If no test dataset was created in the pipeline (e.g. test_percent=0), then an external test dataset is required. (default=default') num_of_images: (int) number of images to be displayed. figure_size: (tuple) figure size (default=(10,10)) show_table: (boolean) display table of misclassied images. (default=False) Example Full example for Image Classification Pipeline can be found HERE","title":"Image_Classification"},{"location":"pipeline/#feature_extraction","text":"pipeline.Feature_Extraction(data_directory, transformations='default', custom_resize = 'default', is_dicom=True,label_from_table=False, is_csv=None,table_source=None, device='default', path_col = 'IMAGE_PATH', label_col = 'IMAGE_LABEL', mode='RAW', wl=None, model_arch='vgg16', pre_trained=True, unfreeze_weights=False, shuffle=True) Description The feature extraction pipeline utilizes a pre-trained model to extract a set of features that can be used in another machine learning algorithms e.g. Adaboost or KNN. The trained model by default can one of the supported model architectures trained with default weights trained on the ImageNet dataset. (The ability to use a model that has been trained and exported using the image classification pipeline will be added later.) The output is a pandas dataframe that has feature columns, label column and file path column. Under the hood, the pipeline removes the last FC layer of the pretrained models to output the features. The number of extracted features depends on the model architecture selected: Model Architecture Default Input Image Size Output Features vgg11 224 x 224 4096 vgg13 224 x 224 4096 vgg16 224 x 224 4096 vgg19 224 x 224 4096 vgg11_bn 224 x 224 4096 vgg13_bn 224 x 224 4096 vgg16_bn 224 x 224 4096 vgg19_bn 224 x 224 4096 resnet18 224 x 224 512 resnet34 224 x 224 512 resnet50 224 x 224 2048 resnet101 224 x 224 2048 resnet152 224 x 224 2048 wide_resnet50_2 224 x 224 2048 wide_resnet101_2 224 x 224 2048 alexnet 256 x 256 4096 Parameters data_directory: (str) target data directory. (Required) is_dicom: (boolean) True for DICOM images, False for regular images.(default=True) label_from_table: (boolean) True if labels are to extracted from table, False if labels are to be extracted from subfolders. (default=False) is_csv: (boolean) True for csv, False for pandas dataframe. table_source: (str or pandas dataframe object) source for labelling data. (default=None). This is path to csv file or name of pandas dataframe if pandas to be used. path_col: (str) name of the column with the image path. (default='IMAGE_PATH') label_col: (str) name of the label/class column. (default='IMAGE_LABEL') shuffle - (boolean) shuffles items in dataset.(default=True) mode: (str) output mode for DICOM images only. Options: RAW= Raw pixels, HU= Image converted to Hounsefield Units, WIN= 'window' image windowed to certain W and L, MWIN = 'multi-window' converts image to 3 windowed images of different W and L (specified in wl argument) stacked together]. (default='RAW') wl: (list) list of lists of combinations of window level and widths to be used with WIN and MWIN. In the form of : [[Level,Width], [Level,Width],...]. Only 3 combinations are allowed for MWIN (for now).(default=None) transformations: (pytorch transforms) pytroch transforms to be performed on the dataset. (default=Convert to tensor) custom_resize: (int) by default, a radtorch pipeline will resize the input images into the default training model input image size as demosntrated in the table shown in radtorch home page. This default size can be changed here if needed. model_arch: [str] PyTorch neural network architecture (default='vgg16') pre_trained: (boolean) Load the pretrained weights of the neural network. If False, the last layer is only retrained = Transfer Learning. (default=True) device: (str) device to be used for training. This can be adjusted to 'cpu' or 'cuda'. If nothing is selected, the pipeline automatically detects if cuda is available and trains on it. Methods .info() Display Pandas Dataframe with properties of the Pipeline. .dataset_info(plot=True) Display Dataset Information. Arguments: plot: (boolean) displays dataset information in graphical representation. (default=True) .sample(fig_size=(10,10), show_labels=True, show_file_name=False) Display sample of the training dataset. Arguments: fig_size: (tuple) figure size. (default=(10,10)) show_labels: (boolean) show the image label idx. (default=True) show_file_name: (boolean) show the image name as label. (default=False) .num_features() Displays number of features to be extracted. .run(verbose=True) Extracts features from dataset. Arguments: verbose: (boolean) Show the feature table. (default=True) .export_features(csv_path) Exports the features to csv. Arguments: csv_path: (str) Path to output csv file. .export(target_path) Exports the whole image classification pipeline for future use Arguments: target_path: (str) target location for export. .plot_extracted_features(feature_table=None, feature_names=None, num_features=100, num_images=100,image_path_col='img_path', image_label_col='label_idx') Plots a graphical representation of extracted features. Arguments: num_features: (int) number of features to be displayed (default=100) num_images: (int) number of images to display features for (default=100) image_path_col: (str) name of column containing image paths in the feature table. (default='img_path') image_label_col: (str) name of column containing image label idx in the feature table. (default='label_idx') Examples Full example for Feature Extraction Pipeline can be found HERE","title":"Feature_Extraction"},{"location":"pipeline/#compare_image_classifier","text":"pipeline.Compare_Image_Classifier(data_directory,transformations='default', custom_resize = 'default', device='default', optimizer='Adam', is_dicom=True, label_from_table=False, is_csv=None, table_source=None, path_col = 'IMAGE_PATH', label_col = 'IMAGE_LABEL', balance_class =[False], multi_label = False, mode='RAW', wl=None, normalize=['default'], batch_size=[8], test_percent = [0.2], valid_percent = [0.2], model_arch=['vgg16'], pre_trained=[True], unfreeze_weights=False, train_epochs=[10], learning_rate=[0.0001],loss_function='CrossEntropyLoss') Description The Compare Image Classifier class performs analysis and comparison of different image classification pipelines. This is particularly useful when comparing different model architectures and/or different training parameters. Important Please note that this pipeline performs training from scratch on the selected model architectures. The ability to compared outside trained models will be added in a future release. Supported Parameters The currently supported parameters that can be compared include: balance_class normalize batch_size test_percent valid_percent train_epochs learning_rate model_arch self.pre_trained Use of supported parameters Please note that the supported parameters are supplied as List e.g. model_arch=['renet50'] or train_epochs=[10,20]. Parameters This pipeline follows the same parameters used for the image classification as above. Please take note of the warning on how to use the supported parameters above. Methods .info() Display Pandas Dataframe with properties of the Pipeline. .grid() Display table with all generated image classifier objects that will be used for comparison. .parameters() Displays a list of supported comparison parameters. .dataset_info(plot=True) Display Dataset Information. Arguments: plot: (boolean) displays dataset information in graphical representation. (default=True) .sample(fig_size=(10,10), show_labels=True, show_file_name=False) Display sample of the training dataset. Arguments: fig_size: (tuple) figure size. (default=(10,10)) show_labels: (boolean) show the image label idx. (default=True) show_file_name: (boolean) show the image name as label. (default=False) .run(verbose=True) Runs the pipeline. .metrics(fig_size=(650,400))) Display the training metrics. .roc(fig_size=(700,400)) Displays comparison between ROC curves of different classifiers with AUC. .best(path=None, export_classifier=False, export_model=False)) Displays the best classifier based on AUC. Arguments: path: (str) exporting path. export_classifier: (boolen) export the best classifier. export_model: (boolen) export the best model. Examples Full example for Compare_Image_Classifier can be found HERE","title":"Compare_Image_Classifier"},{"location":"pipeline/#load_pipeline","text":"pipeline.load_pipeline(target_path) Description Loads a previously saved pipeline for future use. Arguments target_path: (str) target path of the target pipeline. Examples my_classifier = load_pipeline('/path/to/pipeline.dump')","title":"load_pipeline"},{"location":"template/","text":"<!-- Function/Class Code Here --> Arguments : : : Output : Example <!-- Example code here -->","title":"Template"},{"location":"template/#_1","text":"<!-- Function/Class Code Here --> Arguments : : : Output : Example <!-- Example code here -->","title":""},{"location":"version/","text":"Releases/Versions Official Releases v.0.1.3-beta - Release Date: 3-30-2020 - New Features - New Pipeline for Comparison of Image Classifiers. - Allow user to solve class imbalance problem in training subsets through oversampling. - Allow data normalization for training. - Add titles to samples displayed. - Add titles including true label/prediction/accuracy% on misclassified images. _ New Model architecture are now supported. Check pipeline documentation for comprehensive list. v.0.1.2-beta - Release Date: 3-17-2020 - Fixes - Restructure how dataset information is processed. Internally a dataframe input_data is created to expedite calling dataset_info. - Set same random seed during training. - Change Image_Classification.train() to Image_Classification.run() - Update number of workers for pipeline dataloader to default = 4 - New Features - Allow user to set/change pytorch and numpy random seed. - Graphs have been updated to Bokeh (more interactive). - Graphically display pipeline dataset information - Misclassified items during testing in the image classification pipeline can now be viewed. - Extracted Imaging Features can now be graphically viewed per class. - Figure size of training metrics is now changeable. - When doing inference using a trained model, prediction percentages for all classes can now be viewed as table. v.0.1.1-beta - Release Date: 3-3-2020 - Feature Extraction Pipeline now uses batches not single images into GPU = improved speed. - Inference done during confusion matrix and roc creation for Image Classification Pipeline now uses batches not single images into GPU = improved speed. - Fix error with Image Classification Pipeline when using external test dataset. - Allow omitting creating an internal test subset for the Image Classification Pipeline by setting test_percent = 0. - Change line color in roc to default blue. - Allow export and import of pipeline structures. - Add shuffle parameter to Feature Extraction Pipeline. - Restructure exported csv file with feature extraction pipeline. - Updated documentation. v.0.1.0-beta - Release Date: 3-1-2020 - First BETA release.","title":"Releases/Versions"},{"location":"version/#releasesversions","text":"","title":"Releases/Versions"},{"location":"version/#official-releases","text":"","title":"Official Releases"},{"location":"version/#v013-beta","text":"- Release Date: 3-30-2020 - New Features - New Pipeline for Comparison of Image Classifiers. - Allow user to solve class imbalance problem in training subsets through oversampling. - Allow data normalization for training. - Add titles to samples displayed. - Add titles including true label/prediction/accuracy% on misclassified images. _ New Model architecture are now supported. Check pipeline documentation for comprehensive list.","title":"v.0.1.3-beta"},{"location":"version/#v012-beta","text":"- Release Date: 3-17-2020 - Fixes - Restructure how dataset information is processed. Internally a dataframe input_data is created to expedite calling dataset_info. - Set same random seed during training. - Change Image_Classification.train() to Image_Classification.run() - Update number of workers for pipeline dataloader to default = 4 - New Features - Allow user to set/change pytorch and numpy random seed. - Graphs have been updated to Bokeh (more interactive). - Graphically display pipeline dataset information - Misclassified items during testing in the image classification pipeline can now be viewed. - Extracted Imaging Features can now be graphically viewed per class. - Figure size of training metrics is now changeable. - When doing inference using a trained model, prediction percentages for all classes can now be viewed as table.","title":"v.0.1.2-beta"},{"location":"version/#v011-beta","text":"- Release Date: 3-3-2020 - Feature Extraction Pipeline now uses batches not single images into GPU = improved speed. - Inference done during confusion matrix and roc creation for Image Classification Pipeline now uses batches not single images into GPU = improved speed. - Fix error with Image Classification Pipeline when using external test dataset. - Allow omitting creating an internal test subset for the Image Classification Pipeline by setting test_percent = 0. - Change line color in roc to default blue. - Allow export and import of pipeline structures. - Add shuffle parameter to Feature Extraction Pipeline. - Restructure exported csv file with feature extraction pipeline. - Updated documentation.","title":"v.0.1.1-beta"},{"location":"version/#v010-beta","text":"- Release Date: 3-1-2020 - First BETA release.","title":"v.0.1.0-beta"},{"location":"visutils/","text":"Visualization Module radtorch.visutils Documentation Outdated. Please check again later for update. Different tools and utilities for data visualization. Based upon Matplotlib and Bokeh. from radtorch import visutils show_dataloader_sample visutils.show_dataloader_sample(dataloader, num_of_images_per_row=10, figsize=(10,10), show_labels=False) Displays sample of a dataloader with corresponding class idx Arguments dataloader: (dataloader object) selected pytorch dataloader. num_of_images_per_row: (int) number of images per row. (default=10) figsize: (tuple) size of displayed figure. (default = (10,10)) show_labels: (boolen) display class idx of the sample displayed .(default=False) show_dataset_info visutils.show_dataset_info(dataset) Displays a Pandas DataFrame summary of the pytorch dataset information. Arguments dataset: (pytorch dataset object) target dataset to inspect. plot_pipline_dataset_info visutils.plot_pipline_dataset_info(dataframe, test_percent) Displays a graphical representation of the dataset information dataframe generated by visutils.show_dataset_info Arguments dataframe: (Pandas DataFrame) dataframe containing dataset information test_percent: (float) the percent of testing subset. show_metrics visutils.show_metrics(metric_source, metric='all', show_points = False, fig_size = (600,400)) Displays metrics created by model training loop. Arguments metric_source: (list) the metrics generated during the training process as by modelsutils.train_model() metric: (str) the metric to display. Options include 'all', 'accuracy' or 'loss'. (default='all') show_points: (boolean) display single points on graph (default = False) fig_size: (tuple) size of the displayed figure. (default=400,600) show_dicom_sample visutils.how_dicom_sample(dataloader, figsize=(30,10)) Displays a sample image from a DICOM dataloader. Returns a single image in case of one window and 3 images in case of multiple window. Arguments dataloader: (dataloader object) selected pytorch dataloader. figsize: (tuple) size of the displayed figure. (default=30,10) show_roc visutils.show_roc(true_labels, predictions, figure_size=(550,400), title='ROC Curve') Displays ROC curve and AUC using true and predicted label lists. Arguments true_labels: (list) list of true labels. predictions: (list) list of predicted labels. figure_size: (tuple) size of the displayed figure. (default=550,400) title: (str) title displayed on top of the output figure. (default='ROC Curve') show_nn_roc visutils.show_nn_roc(model, target_data_set, device, figure_size=(600,400)) Displays the ROC and AUC of a certain trained model on a target(for example test) dataset. Arguments model: (pytorch model object) target model. target_data_set: (pytorch dataset object) target dataset. device: (str) 'cpu' or 'cuda'. figure_size: (tuple) size of the displayed figure. (default=(600,400)) show_confusion_matrix visutils.show_confusion_matrix(cm,target_names, title='Confusion Matrix', cmap=None,normalize=False,figure_size=(8,6)) Given a sklearn confusion matrix (cm), make a nice plot. Code adapted from : https://www.kaggle.com/grfiv4/plot-a-confusion-matrix . Arguments cm: (numpy array) confusion matrix from sklearn.metrics.confusion_matrix. target_names: (list) list of class names. title: (str) title displayed on top of the output figure. (default='Confusion Matrix') cmap: (str) The gradient of the values displayed from matplotlib.pyplot.cm . See http://matplotlib.org/examples/color/colormaps_reference.html . (default=None which is plt.get_cmap('jet') or plt.cm.Blues) normalize: (boolean) If False, plot the raw numbers. If True, plot the proportions. (default=False) figure_size: (tuple) size of the displayed figure. (default=8,6) show_nn_confusion_matrix visutils.show_nn_confusion_matrix(model, target_data_set, target_classes, figure_size=(8,6), cmap=None) Displays Confusion Matrix for Image Classifier Model. Arguments model: (pytorch model object) target model. target_data_set: (pytorch dataset object) target dataset. target_classes: (list) list of class names. figure_size: (tuple) size of the displayed figure. (default=8,6) cmap: (str) the colormap of the generated figure (default=None, which is Blues) misclassified visutils.misclassified(true_labels_list, predicted_labels_list, img_path_list) Returns Dictionary of image path, true label and predicted label for instances not correctly classified by image classification model. Arguments true_labels_list: (list) list of true label idx predicted_labels_list: (list) list of predicted label idx img_path_list: (list) list of image paths Output Dictionary of image path, true label and predicted label for instances not correctly classified by image classification model. show_misclassified visutils.show_misclassified(misclassified_dictionary, is_dicom = True, num_of_images = 16, figure_size = (5,5)) Displays sample images from misclassified instances. Arguments misclassified_dictionary: (dictionary) dictionary of image path, true labels and predicted labels generated by visutils.misclassified is_dicom: (boolean) True for DICOM images. num_of_images: (int) number of imgaes to display (default=16) figure_size: (tuple) size of displayed figure (default = (5,5)) show_nn_misclassified visutils.show_nn_misclassified(model, target_data_set, num_of_images, device, is_dicom = True, figure_size=(5,5)) Displays sample of misclassified images of an image classification pipeline. Arguments model: (pytorch model object) target model. target_data_set: (pytorch dataset object) target dataset. num_of_images: (int) number of images to display. device: (str) 'cpu' or 'cuda'. is_dicom: (boolean) True for DICOM images. (default=True) figure_size: (tuple) size of displayed figure (default = (5,5)) plot_features visutils.plot_features(feature_table, feature_names, num_features, num_images, image_path_col, image_label_col) Displays a graphical representation of extracted imaging features from a Feature Extraction Pipeline. Features can be displayed overall or per class. Arguments feature_table: (Pandas DataFrame) feature table produced by feature extraction pipeline. feature_names: (list) list of imaging features produced by feature extraction pipeline. num_features: (int) number of imaging features produced by feature extraction pipeline. num_images: (int) number of images to show features for. image_path_col: (str) the name of the column which contains image path in the feature table. image_label_col: (str) the name of the column which contains image label idx in the feature table.","title":"Visutils Module"},{"location":"visutils/#visualization-module-radtorchvisutils","text":"Documentation Outdated. Please check again later for update. Different tools and utilities for data visualization. Based upon Matplotlib and Bokeh. from radtorch import visutils","title":"Visualization Module  radtorch.visutils "},{"location":"visutils/#show_dataloader_sample","text":"visutils.show_dataloader_sample(dataloader, num_of_images_per_row=10, figsize=(10,10), show_labels=False) Displays sample of a dataloader with corresponding class idx Arguments dataloader: (dataloader object) selected pytorch dataloader. num_of_images_per_row: (int) number of images per row. (default=10) figsize: (tuple) size of displayed figure. (default = (10,10)) show_labels: (boolen) display class idx of the sample displayed .(default=False)","title":"show_dataloader_sample"},{"location":"visutils/#show_dataset_info","text":"visutils.show_dataset_info(dataset) Displays a Pandas DataFrame summary of the pytorch dataset information. Arguments dataset: (pytorch dataset object) target dataset to inspect.","title":"show_dataset_info"},{"location":"visutils/#plot_pipline_dataset_info","text":"visutils.plot_pipline_dataset_info(dataframe, test_percent) Displays a graphical representation of the dataset information dataframe generated by visutils.show_dataset_info Arguments dataframe: (Pandas DataFrame) dataframe containing dataset information test_percent: (float) the percent of testing subset.","title":"plot_pipline_dataset_info"},{"location":"visutils/#show_metrics","text":"visutils.show_metrics(metric_source, metric='all', show_points = False, fig_size = (600,400)) Displays metrics created by model training loop. Arguments metric_source: (list) the metrics generated during the training process as by modelsutils.train_model() metric: (str) the metric to display. Options include 'all', 'accuracy' or 'loss'. (default='all') show_points: (boolean) display single points on graph (default = False) fig_size: (tuple) size of the displayed figure. (default=400,600)","title":"show_metrics"},{"location":"visutils/#show_dicom_sample","text":"visutils.how_dicom_sample(dataloader, figsize=(30,10)) Displays a sample image from a DICOM dataloader. Returns a single image in case of one window and 3 images in case of multiple window. Arguments dataloader: (dataloader object) selected pytorch dataloader. figsize: (tuple) size of the displayed figure. (default=30,10)","title":"show_dicom_sample"},{"location":"visutils/#show_roc","text":"visutils.show_roc(true_labels, predictions, figure_size=(550,400), title='ROC Curve') Displays ROC curve and AUC using true and predicted label lists. Arguments true_labels: (list) list of true labels. predictions: (list) list of predicted labels. figure_size: (tuple) size of the displayed figure. (default=550,400) title: (str) title displayed on top of the output figure. (default='ROC Curve')","title":"show_roc"},{"location":"visutils/#show_nn_roc","text":"visutils.show_nn_roc(model, target_data_set, device, figure_size=(600,400)) Displays the ROC and AUC of a certain trained model on a target(for example test) dataset. Arguments model: (pytorch model object) target model. target_data_set: (pytorch dataset object) target dataset. device: (str) 'cpu' or 'cuda'. figure_size: (tuple) size of the displayed figure. (default=(600,400))","title":"show_nn_roc"},{"location":"visutils/#show_confusion_matrix","text":"visutils.show_confusion_matrix(cm,target_names, title='Confusion Matrix', cmap=None,normalize=False,figure_size=(8,6)) Given a sklearn confusion matrix (cm), make a nice plot. Code adapted from : https://www.kaggle.com/grfiv4/plot-a-confusion-matrix . Arguments cm: (numpy array) confusion matrix from sklearn.metrics.confusion_matrix. target_names: (list) list of class names. title: (str) title displayed on top of the output figure. (default='Confusion Matrix') cmap: (str) The gradient of the values displayed from matplotlib.pyplot.cm . See http://matplotlib.org/examples/color/colormaps_reference.html . (default=None which is plt.get_cmap('jet') or plt.cm.Blues) normalize: (boolean) If False, plot the raw numbers. If True, plot the proportions. (default=False) figure_size: (tuple) size of the displayed figure. (default=8,6)","title":"show_confusion_matrix"},{"location":"visutils/#show_nn_confusion_matrix","text":"visutils.show_nn_confusion_matrix(model, target_data_set, target_classes, figure_size=(8,6), cmap=None) Displays Confusion Matrix for Image Classifier Model. Arguments model: (pytorch model object) target model. target_data_set: (pytorch dataset object) target dataset. target_classes: (list) list of class names. figure_size: (tuple) size of the displayed figure. (default=8,6) cmap: (str) the colormap of the generated figure (default=None, which is Blues)","title":"show_nn_confusion_matrix"},{"location":"visutils/#misclassified","text":"visutils.misclassified(true_labels_list, predicted_labels_list, img_path_list) Returns Dictionary of image path, true label and predicted label for instances not correctly classified by image classification model. Arguments true_labels_list: (list) list of true label idx predicted_labels_list: (list) list of predicted label idx img_path_list: (list) list of image paths Output Dictionary of image path, true label and predicted label for instances not correctly classified by image classification model.","title":"misclassified"},{"location":"visutils/#show_misclassified","text":"visutils.show_misclassified(misclassified_dictionary, is_dicom = True, num_of_images = 16, figure_size = (5,5)) Displays sample images from misclassified instances. Arguments misclassified_dictionary: (dictionary) dictionary of image path, true labels and predicted labels generated by visutils.misclassified is_dicom: (boolean) True for DICOM images. num_of_images: (int) number of imgaes to display (default=16) figure_size: (tuple) size of displayed figure (default = (5,5))","title":"show_misclassified"},{"location":"visutils/#show_nn_misclassified","text":"visutils.show_nn_misclassified(model, target_data_set, num_of_images, device, is_dicom = True, figure_size=(5,5)) Displays sample of misclassified images of an image classification pipeline. Arguments model: (pytorch model object) target model. target_data_set: (pytorch dataset object) target dataset. num_of_images: (int) number of images to display. device: (str) 'cpu' or 'cuda'. is_dicom: (boolean) True for DICOM images. (default=True) figure_size: (tuple) size of displayed figure (default = (5,5))","title":"show_nn_misclassified"},{"location":"visutils/#plot_features","text":"visutils.plot_features(feature_table, feature_names, num_features, num_images, image_path_col, image_label_col) Displays a graphical representation of extracted imaging features from a Feature Extraction Pipeline. Features can be displayed overall or per class. Arguments feature_table: (Pandas DataFrame) feature table produced by feature extraction pipeline. feature_names: (list) list of imaging features produced by feature extraction pipeline. num_features: (int) number of imaging features produced by feature extraction pipeline. num_images: (int) number of images to show features for. image_path_col: (str) the name of the column which contains image path in the feature table. image_label_col: (str) the name of the column which contains image label idx in the feature table.","title":"plot_features"}]}